{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.metrics import accuracy_score as acs_score\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0.1: Preprocessing of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_RAW_DATA_LOCATION = \"data/raw/\"\n",
    "\n",
    "def get_raw_data():\n",
    "    \"\"\"\n",
    "    Utility function to load the raw data from the data/raw folder.\n",
    "\n",
    "    Returns:\n",
    "        train_a (pd.DataFrame): The training targets for the A dataset.\n",
    "        train_b (pd.DataFrame): The training targets for the B dataset.\n",
    "        train_c (pd.DataFrame): The training targets for the C dataset.\n",
    "        X_train_estimated_a (pd.DataFrame): The estimated training features for the A dataset.\n",
    "        X_train_estimated_b (pd.DataFrame): The estimated training features for the B dataset.\n",
    "        X_train_estimated_c (pd.DataFrame): The estimated training features for the C dataset.\n",
    "        X_train_observed_a (pd.DataFrame): The observed training features for the A dataset.\n",
    "        X_train_observed_b (pd.DataFrame): The observed training features for the B dataset.\n",
    "        X_train_observed_c (pd.DataFrame): The observed training features for the C dataset.\n",
    "        X_test_estimated_a (pd.DataFrame): The estimated test features for the A dataset.\n",
    "        X_test_estimated_b (pd.DataFrame): The estimated test features for the B dataset.\n",
    "        X_test_estimated_c (pd.DataFrame): The estimated test features for the C dataset.\n",
    "    \"\"\"\n",
    "    train_a = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}A/train_targets.parquet')\n",
    "    train_b = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}B/train_targets.parquet')\n",
    "    train_c = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}C/train_targets.parquet')\n",
    "    X_train_estimated_a = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}A/X_train_estimated.parquet')\n",
    "    X_train_estimated_b = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}B/X_train_estimated.parquet')\n",
    "    X_train_estimated_c = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}C/X_train_estimated.parquet')\n",
    "    X_train_observed_a = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}A/X_train_observed.parquet')\n",
    "    X_train_observed_b = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}B/X_train_observed.parquet')\n",
    "    X_train_observed_c = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}C/X_train_observed.parquet')\n",
    "    X_test_estimated_a = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}A/X_test_estimated.parquet')\n",
    "    X_test_estimated_b = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}B/X_test_estimated.parquet')\n",
    "    X_test_estimated_c = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}C/X_test_estimated.parquet')\n",
    "\n",
    "    return train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import skew\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "import math\n",
    "\n",
    "\n",
    "def prepare_data(\n",
    "    train_observed: pd.DataFrame,\n",
    "    train_estimated: pd.DataFrame,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    drop_features: bool = True,\n",
    ") -> Tuple[\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.Series,\n",
    "    pd.Series,\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.Series,\n",
    "    pd.Series,\n",
    "]:\n",
    "    \"\"\"\n",
    "    Prepares the data for modeling by handling missing values and splitting the data.\n",
    "\n",
    "    Args:\n",
    "    train_observed (pd.DataFrame): The aligned training DataFrame with observed features.\n",
    "    train_estimated (pd.DataFrame): The aligned training DataFrame with estimated features.\n",
    "    test_size (float): The proportion of the dataset to include in the test split.\n",
    "    random_state (int): Controls the shuffling applied to the data before applying the split.\n",
    "\n",
    "    Returns:\n",
    "    X_train_obs (pd.DataFrame): The training features with observed data.\n",
    "    X_val_obs (pd.DataFrame): The validation features with observed data.\n",
    "    y_train_obs (pd.Series): The training target with observed data.\n",
    "    y_val_obs (pd.Series): The validation target with observed data.\n",
    "    X_train_est (pd.DataFrame): The training features with estimated data.\n",
    "    X_val_est (pd.DataFrame): The validation features with estimated data.\n",
    "    y_train_est (pd.Series): The training target with estimated data.\n",
    "    y_val_est (pd.Series): The validation target with estimated data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove missing features\n",
    "    train_observed = remove_missing_features(train_observed)\n",
    "    train_estimated = remove_missing_features(train_estimated)\n",
    "\n",
    "    # Handle missing values (e.g., imputation, removal)\n",
    "    train_observed_clean = train_observed.dropna(\n",
    "        subset=[\"visibility:m\", \"pv_measurement\"]\n",
    "    )\n",
    "    train_estimated_clean = train_estimated.dropna(\n",
    "        subset=[\"visibility:m\", \"pv_measurement\"]\n",
    "    )\n",
    "\n",
    "    # Remove discrepancies\n",
    "    train_observed_clean = remove_discrepancies(train_observed_clean)\n",
    "    train_estimated_clean = remove_discrepancies(train_estimated_clean)\n",
    "\n",
    "    # Feature engineer\n",
    "    train_observed_clean = feature_engineer(train_observed_clean)\n",
    "    train_estimated_clean = feature_engineer(train_estimated_clean)\n",
    "\n",
    "    # Split the data into features (X) and target (y)\n",
    "\n",
    "    y_obs = train_observed_clean[\"pv_measurement\"]\n",
    "\n",
    "    if drop_features:\n",
    "        X_obs = train_observed_clean.drop(\n",
    "            columns=[\"time\", \"pv_measurement\", \"date_forecast\", \"date_calc\"],\n",
    "            errors=\"ignore\",\n",
    "        )\n",
    "    else:\n",
    "        X_obs = train_observed_clean\n",
    "\n",
    "    if drop_features:\n",
    "        X_est = train_estimated_clean.drop(\n",
    "            columns=[\"time\", \"pv_measurement\", \"date_calc\", \"date_forecast\"],\n",
    "            errors=\"ignore\",\n",
    "        )\n",
    "    else:\n",
    "        X_est = train_estimated_clean\n",
    "\n",
    "    y_est = train_estimated_clean[\"pv_measurement\"]\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train_obs, X_val_obs, y_train_obs, y_val_obs = train_test_split(\n",
    "        X_obs, y_obs, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    X_train_est, X_val_est, y_train_est, y_val_est = train_test_split(\n",
    "        X_est, y_est, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        X_train_obs,\n",
    "        X_val_obs,\n",
    "        y_train_obs,\n",
    "        y_val_obs,\n",
    "        X_train_est,\n",
    "        X_val_est,\n",
    "        y_train_est,\n",
    "        y_val_est,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_location_datasets(\n",
    "    df: pd.DataFrame,\n",
    ") -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    locations = [\"location_a\", \"location_b\", \"location_c\"]\n",
    "    x_a = df[df[\"location_a\"] == 1]\n",
    "    x_a = x_a.drop(locations, axis=1)\n",
    "    y_a = x_a[\"pv_measurement\"]\n",
    "    if \"pv_measurement\" in x_a.columns:\n",
    "        x_a = x_a.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "    x_b = df[df[\"location_b\"] == 1]\n",
    "    x_b = x_b.drop(locations, axis=1)\n",
    "    y_b = x_b[\"pv_measurement\"]\n",
    "    if \"pv_measurement\" in x_b.columns:\n",
    "        x_b = x_b.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "    x_c = df[df[\"location_c\"] == 1]\n",
    "    x_c = x_c.drop(locations, axis=1)\n",
    "    y_c = x_c[\"pv_measurement\"]\n",
    "    if \"pv_measurement\" in x_b.columns:\n",
    "        x_b = x_b.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "    return (x_a, x_b, x_c, y_a, y_b, y_c)\n",
    "\n",
    "\n",
    "def remove_missing_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.drop(\"snow_density:kgm3\", axis=1)\n",
    "    df = df.drop(\"ceiling_height_agl:m\", axis=1)\n",
    "    df[\"cloud_base_agl:m\"] = df[\"cloud_base_agl:m\"].fillna(0)\n",
    "    df = df.drop(\"elevation:m\", axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_discrepancies(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = remove_positive_pv_in_night(df)\n",
    "    df = remove_night_light_discrepancies(df)\n",
    "    df = remove_zero_value_discrepancies(df)\n",
    "    df = remove_faulty_zero_measurements_for_direct_sun_light(df)\n",
    "    # df = remove_outliers(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_positive_pv_in_night(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove positive pv measurements when is_day is 0 and pv_measurement is positive and pv_measurement is the same next timestep\n",
    "    \"\"\"\n",
    "    # Remove positive pv measurements when is_day is 0 and pv_measurement is positive and pv_measurement is the same next timestep\n",
    "    df = df.drop(\n",
    "        df[\n",
    "            (df[\"is_day:idx\"] == 0)\n",
    "            & (df[\"pv_measurement\"] > 0)\n",
    "            & (df[\"pv_measurement\"] == df[\"pv_measurement\"].shift(1))\n",
    "        ].index\n",
    "    )\n",
    "\n",
    "    # Remove positive pv measurements when sun_elevation is negative\n",
    "    threshold = -10\n",
    "    df = df.drop(\n",
    "        df[(df[\"sun_elevation:d\"] < threshold) & (df[\"pv_measurement\"] > 0)].index\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_outliers(\n",
    "    df: pd.DataFrame, lower_bound: float = 0.1, upper_bound: float = 0.9\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removing outliers using IQR method\n",
    "    \"\"\"\n",
    "\n",
    "    columns_to_check = [col for col in df.columns if col != \"pv_measurement\"]\n",
    "    for col in columns_to_check:\n",
    "        # Calculate IQR\n",
    "        Q1 = df[col].quantile(lower_bound)\n",
    "        Q3 = df[col].quantile(upper_bound)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Define outlier bounds\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Filter the data\n",
    "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_night_light_discrepancies(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Remove all rows where pv_measurement has the same value for 6 timesteps and not is 0 remove them\n",
    "\n",
    "    # Step 1: Identify runs of equal, non-zero values\n",
    "    df[\"group\"] = (\n",
    "        (df[\"pv_measurement\"] != df[\"pv_measurement\"].shift())\n",
    "        | (df[\"pv_measurement\"] == 0)\n",
    "    ).cumsum()\n",
    "\n",
    "    # Step 2: Count occurrences in each run\n",
    "    counts = df.groupby(\"group\")[\"pv_measurement\"].transform(\"count\")\n",
    "\n",
    "    # Step 3: Identify groups to remove\n",
    "    to_remove = (counts >= 6) & (df[\"pv_measurement\"] != 0)\n",
    "\n",
    "    # Step 4: Remove those rows\n",
    "    df_cleaned = df[~to_remove].drop(columns=[\"group\"])\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "def remove_zero_value_discrepancies(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Remove all rows where pv_measurement has the same value for 100 timesteps and is 0 remove them\n",
    "\n",
    "    # Didn't do anything lol\n",
    "\n",
    "    # Step 1: Identify runs of equal, non-zero values\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def remove_faulty_zero_measurements_for_direct_sun_light(\n",
    "    df: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\" \"\"\"\n",
    "    mask = ((df[\"diffuse_rad:W\"] + df[\"direct_rad:W\"]) >= 30) & (\n",
    "        df[\"pv_measurement\"] == 0\n",
    "    )\n",
    "    df = df[~mask]\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_engineer(data_frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    data_frame = create_time_features_from_date(data_frame)\n",
    "    # data_frame = create_expected_pv_based_on_previous_years_same_day(data_frame)\n",
    "    data_frame[\"sun_product\"] = data_frame[\"diffuse_rad:W\"] * data_frame[\"direct_rad:W\"]\n",
    "\n",
    "    data_frame[\"modified_solar_elevation\"] = np.where(\n",
    "        data_frame[\"sun_elevation:d\"] <= 0,\n",
    "        0,\n",
    "        np.sin(np.radians(data_frame[\"sun_elevation:d\"])),\n",
    "    )\n",
    "    data_frame = data_frame.drop(\"sun_elevation:d\", axis=1)\n",
    "\n",
    "    data_frame[\"effective_radiation\"] = np.where(\n",
    "        data_frame[\"clear_sky_energy_1h:J\"] == 0,\n",
    "        0,  # or your specified value\n",
    "        data_frame[\"direct_rad_1h:J\"] / data_frame[\"clear_sky_energy_1h:J\"],\n",
    "    )\n",
    "\n",
    "    # # Check for the existence of date_calc column\n",
    "    # if \"date_calc\" not in data_frame.columns:\n",
    "    #     data_frame[\"time_since_prediction\"] = 0\n",
    "    # else:\n",
    "    #     data_frame[\"time_since_prediction\"] = 1\n",
    "\n",
    "    # data_frame[\"time_since_prediction\"] =\n",
    "\n",
    "    data_frame[\"residual_radiation\"] = (\n",
    "        data_frame[\"clear_sky_rad:W\"]\n",
    "        - data_frame[\"direct_rad:W\"]\n",
    "        - data_frame[\"diffuse_rad:W\"]\n",
    "    )\n",
    "\n",
    "    # WAS WORSE\n",
    "    # data_frame[\"effective_radiation2\"] = np.where(\n",
    "    #     data_frame[\"clear_sky_rad:W\"] == 0,\n",
    "    #     0,  # or your specified value\n",
    "    #     data_frame[\"direct_rad:W\"] / data_frame[\"clear_sky_rad:W\"],\n",
    "    # )\n",
    "\n",
    "    data_frame[\"cloud_ratio\"] = np.where(\n",
    "        data_frame[\"total_cloud_cover:p\"] == 0,\n",
    "        0,  # or your specified value\n",
    "        data_frame[\"effective_cloud_cover:p\"] / data_frame[\"total_cloud_cover:p\"],\n",
    "    )\n",
    "\n",
    "    data_frame[\"diffuse_cloud_conditional_interaction\"] = data_frame[\n",
    "        \"diffuse_rad:W\"\n",
    "    ].where(data_frame[\"effective_cloud_cover:p\"] < 0.3, 0)\n",
    "\n",
    "    data_frame[\"cloud_cover_over_30%\"] = np.where(\n",
    "        data_frame[\"effective_cloud_cover:p\"] > 30, 1, 0\n",
    "    )\n",
    "\n",
    "    data_frame[\"sun_addition\"] = (\n",
    "        data_frame[\"diffuse_rad:W\"] + data_frame[\"direct_rad:W\"]\n",
    "    )\n",
    "\n",
    "    data_frame[\"direct_rad_cloud_interaction\"] = data_frame[\"direct_rad:W\"] * (\n",
    "        100 - data_frame[\"effective_cloud_cover:p\"]\n",
    "    )\n",
    "\n",
    "    data_frame[\"modified_solar_elevation_squared\"] = (\n",
    "        data_frame[\"modified_solar_elevation\"] ** 0.5\n",
    "    )\n",
    "    \n",
    "    # data_frame[\"sun_addition_squared\"] = data_frame[\"sun_addition\"] ** 2\n",
    "\n",
    "    snow_columns = [\n",
    "        \"snow_depth:cm\",\n",
    "        \"fresh_snow_12h:cm\",\n",
    "        \"fresh_snow_1h:cm\",\n",
    "        \"fresh_snow_24h:cm\",\n",
    "        \"fresh_snow_3h:cm\",\n",
    "        \"fresh_snow_6h:cm\",\n",
    "    ]\n",
    "\n",
    "    data_frame[\"is_freezing\"] = (data_frame[\"t_1000hPa:K\"] < 273).astype(int)\n",
    "\n",
    "    data_frame[\"is_snow\"] = (data_frame[snow_columns] > 0).any(axis=1).astype(int)\n",
    "    data_frame[\"is_rain\"] = (data_frame[\"precip_5min:mm\"] > 0).astype(int)\n",
    "\n",
    "    data_frame = data_frame.drop(\"snow_drift:idx\", axis=1)\n",
    "\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "def ratio(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    # Sample DataFrame (replace this with your actual DataFrame)\n",
    "\n",
    "    # Calculate total irradiance\n",
    "    df[\"total_radiance\"] = df[\"direct_radiance\"] + df[\"diffuse_radiance\"]\n",
    "\n",
    "    # Set up the linear regression problem\n",
    "    X = df[[\"total_radiance\", \"ambient_temp\"]]\n",
    "    y = df[\"pv_measurement\"]\n",
    "\n",
    "    # Train a linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Extract coefficients\n",
    "    a = model.coef_[0]  # Coefficient for total_radiance\n",
    "    b = model.coef_[1]  # Coefficient for ambient_temp\n",
    "    c = model.intercept_\n",
    "\n",
    "    # Estimate the parameters\n",
    "    T_ref = 25  # Reference temperature, you can set this based on your understanding\n",
    "    eta = a  # Assuming that a corresponds to efficiency\n",
    "    beta = (\n",
    "        -b / a\n",
    "    )  # Assuming linear relationship between power drop and temperature increase\n",
    "\n",
    "    # Estimate panel temperature\n",
    "    df[\"T_panel\"] = df[\"ambient_temp\"] + (df[\"total_radiance\"] / a) * (1 - eta)\n",
    "\n",
    "    # Calculate the desired ratio\n",
    "    df[\"ratio\"] = 1 - beta * (df[\"T_panel\"] - T_ref)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_surface_temp(row):\n",
    "    # Constants\n",
    "    R = 287  # Specific gas constant for dry air, J kg^-1 K^-1\n",
    "    g = 9.81  # Acceleration due to gravity, m s^-2\n",
    "    lapse_rate = 0.0065  # Average lapse rate, K m^-1\n",
    "\n",
    "    P1 = 1000  # Initial pressure level, hPa\n",
    "    P2 = row[\"sfc_pressure:hPa\"]  # Final pressure level, hPa\n",
    "    T1 = row[\"t_1000hPa:K\"]  # Temperature at P1, K\n",
    "\n",
    "    # Altitude difference using barometric formula (approximation)\n",
    "    delta_h = (R * T1 / g) * np.log(P1 / P2)\n",
    "\n",
    "    # Temperature difference using constant lapse rate\n",
    "    delta_T = lapse_rate * delta_h\n",
    "\n",
    "    # Temperature at P2, converting from K to C\n",
    "    T2_C = T1 - delta_T - 273.15\n",
    "\n",
    "    return T2_C\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_time_features_from_date(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a new data frame with new features from date_forecast column.\n",
    "    This will create temporal features from date_forecast that are easier to learn by the model.\n",
    "    It creates the following features: month, season, year, day_of_year, day_segment.\n",
    "    All of the new features are int type.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Data frame with date_forecast column.\n",
    "    Returns:\n",
    "        pd.DataFrame: Data frame copy with new features.\n",
    "\n",
    "    \"\"\"\n",
    "    # df[\"days_from_peak\"] = df[\"date_forecast\"].apply(nearest_june_21st)\n",
    "    df[\"sin_day_of_year\"] = df[\"date_forecast\"].apply(get_sin_day)\n",
    "    df[\"cos_day_of_year\"] = df[\"date_forecast\"].apply(get_cos_day)\n",
    "    df[\"sin_hour\"] = df[\"date_forecast\"].apply(get_sin_hour)\n",
    "    df[\"cos_hour\"] = df[\"date_forecast\"].apply(get_cos_hour)\n",
    "    return df\n",
    "\n",
    "\n",
    "def nearest_june_21st(date):\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    # Find June 21st of the same year and the next year\n",
    "    this_year = datetime(date.year, 6, 21)\n",
    "    next_year = datetime(date.year + 1, 6, 21)\n",
    "    last_year = datetime(date.year - 1, 6, 21)\n",
    "\n",
    "    # Calculate the absolute difference in days\n",
    "    days_this_year = abs((date - this_year).days)\n",
    "    days_next_year = abs((date - next_year).days)\n",
    "    days_last_year = abs((date - last_year).days)\n",
    "\n",
    "    # Return the smallest difference\n",
    "    return min(days_this_year, days_next_year, days_last_year)\n",
    "\n",
    "\n",
    "def get_sin_hour(date: datetime) -> float:\n",
    "    HOURS_OF_DAY = 24\n",
    "    return math.sin(2 * math.pi * (date.hour) / HOURS_OF_DAY)\n",
    "\n",
    "\n",
    "def get_cos_hour(date: datetime) -> float:\n",
    "    HOURS_OF_DAY = 24\n",
    "    return math.cos(2 * math.pi * (date.hour) / HOURS_OF_DAY)\n",
    "\n",
    "\n",
    "def get_sin_day(date: datetime) -> float:\n",
    "    DAY_OF_YEAR = 365.25  # Add 0.25 to account for leap years\n",
    "    return math.sin(2 * math.pi * (date.timetuple().tm_yday - 1) / DAY_OF_YEAR)\n",
    "\n",
    "\n",
    "def get_cos_day(date: datetime) -> float:\n",
    "    DAY_OF_YEAR = 365.25  # Add 0.25 to account for leap years\n",
    "    return math.cos(2 * math.pi * (date.timetuple().tm_yday - 1) / DAY_OF_YEAR)\n",
    "\n",
    "\n",
    "def create_polynomial_features(\n",
    "    df, columns, degree=2, include_bias=False, interaction_only=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Create polynomial features for specified columns in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "    - columns (list): List of column names for which to create polynomial features.\n",
    "    - degree (int): The degree of the polynomial features. Default is 2.\n",
    "    - include_bias (bool): Whether to include a bias column in the output. Default is False.\n",
    "    - interaction_only (bool): Whether to include only interaction features. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    - df_poly (pd.DataFrame): DataFrame with original and new polynomial features.\n",
    "    \"\"\"\n",
    "    poly = PolynomialFeatures(\n",
    "        degree=degree, include_bias=include_bias, interaction_only=interaction_only\n",
    "    )\n",
    "    poly_features = poly.fit_transform(df[columns])\n",
    "    feature_names = poly.get_feature_names(input_features=columns)\n",
    "\n",
    "    df_poly = pd.DataFrame(poly_features, columns=feature_names, index=df.index)\n",
    "    df_poly = df_poly.drop(\n",
    "        columns=columns\n",
    "    )  # Drop original columns as they are included in the polynomial features\n",
    "\n",
    "    # Concatenate the original DataFrame with the new polynomial features DataFrame\n",
    "    df_poly = pd.concat([df.drop(columns=columns), df_poly], axis=1)\n",
    "\n",
    "    return df_poly\n",
    "\n",
    "\n",
    "def log_transform(df: pd.DataFrame, columns: List[str]):\n",
    "    df_transformed = df.copy()\n",
    "    for column in columns:\n",
    "        df_transformed[column] = np.log1p(df_transformed[column])\n",
    "    return df_transformed\n",
    "\n",
    "\n",
    "def scale_features(df: pd.DataFrame, columns: List[str], method=\"standard\"):\n",
    "    df_scaled = df.copy()\n",
    "    scaler = StandardScaler() if method == \"standard\" else MinMaxScaler()\n",
    "    df_scaled[columns] = scaler.fit_transform(df_scaled[columns])\n",
    "    return df_scaled\n",
    "\n",
    "\n",
    "def trig_transform(df: pd.DataFrame, column: str, period: int):\n",
    "    df_trig = df.copy()\n",
    "    df_trig[f\"{column}_sin\"] = np.sin(2 * math.pi * df_trig[column] / period)\n",
    "    df_trig[f\"{column}_cos\"] = np.cos(2 * math.pi * df_trig[column] / period)\n",
    "    return df_trig\n",
    "\n",
    "\n",
    "def create_expected_pv_based_on_previous_years_same_day(\n",
    "    df: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a mean pv_measurement for each data point based on the previous years same day and hour\n",
    "    Add this as a feature to the data frame\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing at least columns:\n",
    "                           location_a, location_b, location_c, date_forecast,\n",
    "                           pv_measurement, sin_day_of_year, cos_day_of_year, sin_hour, cos_hour\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with additional feature of mean pv_measurement based on historical data\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # When the data does not contain needed columns, return the original df.\n",
    "    if not all(\n",
    "        col in df.columns\n",
    "        for col in [\n",
    "            \"location_a\",\n",
    "            \"location_b\",\n",
    "            \"location_c\",\n",
    "            \"pv_measurement\",\n",
    "            \"sin_day_of_year\",\n",
    "            \"cos_day_of_year\",\n",
    "            \"sin_hour\",\n",
    "            \"cos_hour\",\n",
    "        ]\n",
    "    ):\n",
    "        return df\n",
    "    # Identify the location from the binary flags\n",
    "    df[\"location\"] = df[[\"location_a\", \"location_b\", \"location_c\"]].idxmax(axis=1)\n",
    "\n",
    "    # Calculate mean pv_measurement for each location, sin_day_of_year, cos_day_of_year, sin_hour, and cos_hour\n",
    "    mean_pv = (\n",
    "        df.groupby(\n",
    "            [\"location\", \"sin_day_of_year\", \"cos_day_of_year\", \"sin_hour\", \"cos_hour\"]\n",
    "        )[\"pv_measurement\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    mean_pv.rename(columns={\"pv_measurement\": \"mean_pv_measurement\"}, inplace=True)\n",
    "\n",
    "    # Merge mean_pv_measurement back to the original DataFrame\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        mean_pv,\n",
    "        on=[\"location\", \"sin_day_of_year\", \"cos_day_of_year\", \"sin_hour\", \"cos_hour\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    df.drop(columns=[\"location\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_simple_rolling_mean(\n",
    "    df: pd.DataFrame, column: str, window: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a simple rolling mean feature for a given column in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing your time-series data.\n",
    "        column: The name of the column for which you want to create lagged features.\n",
    "        window: The size of the window for calculating the rolling mean.\n",
    "                For example, if window=10, it will take the previous 10 days.\n",
    "    \"\"\"\n",
    "    # Ensure the DataFrame is sorted by date\n",
    "    df = df.sort_values(by=\"date_forecast\")\n",
    "\n",
    "    # Ensure 'date_forecast' is in datetime format\n",
    "    df[\"date_forecast\"] = pd.to_datetime(df[\"date_forecast\"])\n",
    "\n",
    "    # Calculate the rolling mean\n",
    "    df[\"rolling_mean_of_\" + column] = df[column].rolling(window=window).mean()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Define a function to identify positively skewed numerical features in a DataFrame\n",
    "def identify_skewed_features(df, skew_threshold=1):\n",
    "    # Select numerical features\n",
    "    num_features = df.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n",
    "\n",
    "    # Calculate skewness for each numerical feature and filter those that are positively skewed\n",
    "    skewed_features = [\n",
    "        feature for feature in num_features if skew(df[feature]) > skew_threshold\n",
    "    ]\n",
    "\n",
    "    return skewed_features\n",
    "\n",
    "\n",
    "def create_domain_specific_features(df):\n",
    "    \"\"\"\n",
    "    Create domain-specific features for solar energy production.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - df_domain (pd.DataFrame): DataFrame with original and new domain-specific features.\n",
    "    \"\"\"\n",
    "    df_domain = df.copy()\n",
    "\n",
    "    # Create a binary feature representing whether the sky is clear\n",
    "    df_domain[\"is_clear_sky\"] = (df_domain[\"clear_sky_energy_1h:J\"] > 0).astype(int)\n",
    "\n",
    "    # Create a feature representing total sun exposure\n",
    "    df_domain[\"total_sun_exposure\"] = (\n",
    "        df_domain[\"direct_rad:W\"] + df_domain[\"diffuse_rad:W\"]\n",
    "    )\n",
    "\n",
    "    # Create a binary feature representing whether it is raining\n",
    "    df_domain[\"is_raining\"] = (df_domain[\"precip_5min:mm\"] > 0).astype(int)\n",
    "\n",
    "    # Create a binary feature representing whether there is snow cover\n",
    "    df_domain[\"is_snow_cover\"] = (df_domain[\"snow_depth:cm\"] > 0).astype(int)\n",
    "\n",
    "    return df_domain\n",
    "\n",
    "\n",
    "def clean_data(data_frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean data frame by removing outliers and NaN values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Data frame with date_forecast column.\n",
    "    Returns:\n",
    "        pd.DataFrame: Data frame copy without outliers and NaN values.\n",
    "\n",
    "    \"\"\"\n",
    "    df = data_frame.copy()\n",
    "    df = create_time_features_from_date(df)\n",
    "    df = df[df[\"target\"] > 0]\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_location(data_frame: pd.DataFrame, location: str):\n",
    "    if location.lower() == \"a\":\n",
    "        data_frame[\"location_a\"] = 1\n",
    "    else:\n",
    "        data_frame[\"location_a\"] = 0\n",
    "\n",
    "    if location.lower() == \"b\":\n",
    "        data_frame[\"location_b\"] = 1\n",
    "    else:\n",
    "        data_frame[\"location_b\"] = 0\n",
    "\n",
    "    if location.lower() == \"c\":\n",
    "        data_frame[\"location_c\"] = 1\n",
    "    else:\n",
    "        data_frame[\"location_c\"] = 0\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "# Define a function to align the temporal resolution of the datasets\n",
    "def temporal_alignment(\n",
    "    train: pd.DataFrame, observed: pd.DataFrame, estimated: pd.DataFrame\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Aligns the temporal resolution of the datasets by aggregating the 15-min interval weather data to hourly intervals.\n",
    "\n",
    "    Args:\n",
    "        train (pd.DataFrame): The training targets DataFrame.\n",
    "        observed (pd.DataFrame): The observed training features DataFrame.\n",
    "        estimated (pd.DataFrame): The estimated training features DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        train_observed (pd.DataFrame): The aligned training DataFrame with observed features.\n",
    "        train_estimated (pd.DataFrame): The aligned training DataFrame with estimated features.\n",
    "    \"\"\"\n",
    "    # Convert the time columns to datetime objects\n",
    "    train[\"time\"] = pd.to_datetime(train[\"time\"])\n",
    "    observed[\"date_forecast\"] = pd.to_datetime(observed[\"date_forecast\"])\n",
    "    estimated[\"date_forecast\"] = pd.to_datetime(estimated[\"date_forecast\"])\n",
    "\n",
    "    # Set the date_forecast column as index for resampling\n",
    "    observed.set_index(\"date_forecast\", inplace=True)\n",
    "    estimated.set_index(\"date_forecast\", inplace=True)\n",
    "\n",
    "    # Resample the weather data to hourly intervals and aggregate the values by mean\n",
    "    observed_resampled = observed.resample(\"1H\").mean()\n",
    "    estimated_resampled = estimated.resample(\"1H\").mean()\n",
    "\n",
    "    # Reset the index after resampling\n",
    "    observed_resampled.reset_index(inplace=True)\n",
    "    estimated_resampled.reset_index(inplace=True)\n",
    "\n",
    "    # Merge the aggregated weather data with the solar production data based on the timestamp\n",
    "    train_observed = pd.merge(\n",
    "        train, observed_resampled, how=\"left\", left_on=\"time\", right_on=\"date_forecast\"\n",
    "    )\n",
    "    train_estimated = pd.merge(\n",
    "        train, estimated_resampled, how=\"left\", left_on=\"time\", right_on=\"date_forecast\"\n",
    "    )\n",
    "\n",
    "    return train_observed, train_estimated\n",
    "\n",
    "\n",
    "def temporal_alignment_tests(test: pd.DataFrame) -> Tuple[pd.DataFrame]:\n",
    "    return aggregate_rows(test)\n",
    "\n",
    "\n",
    "def aggregate_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Create a 'group' column to group every 4 rows together\n",
    "    df[\"group\"] = df.index // 4\n",
    "\n",
    "    # Define the aggregation functions\n",
    "    aggregation = {col: \"mean\" for col in df.columns if col != \"date_forecast\"}\n",
    "    aggregation[\"date_forecast\"] = \"first\"\n",
    "\n",
    "    # Group by the 'group' column and aggregate\n",
    "    df_agg = df.groupby(\"group\").agg(aggregation).reset_index(drop=True)\n",
    "\n",
    "    # Drop the 'group' column from the original dataframe\n",
    "    df_agg.drop(\"group\", axis=1, inplace=True)\n",
    "\n",
    "    return df_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_preprocessed_data(drop_features: bool = True) -> (\n",
    "    Tuple[\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "    ]\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch the preprocessed data for training and validation.\n",
    "\n",
    "    Returns:\n",
    "        X_train_obs_combined: The observed data for training\n",
    "        X_val_obs_combined: The observed data for validation\n",
    "        y_train_obs_combined: The observed labels for training\n",
    "        y_val_obs_combined: The observed labels for validation\n",
    "        X_train_est_combined: The estimated data for training\n",
    "        X_val_est_combined: The estimated data for validation\n",
    "        y_train_est_combined: The estimated labels for training\n",
    "        y_val_est_combined: The estimated labels for validation\n",
    "    \"\"\"\n",
    "    (\n",
    "        train_a,\n",
    "        train_b,\n",
    "        train_c,\n",
    "        X_train_estimated_a,\n",
    "        X_train_estimated_b,\n",
    "        X_train_estimated_c,\n",
    "        X_train_observed_a,\n",
    "        X_train_observed_b,\n",
    "        X_train_observed_c,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "    ) = get_raw_data()\n",
    "\n",
    "    # Temporally align the data from all three locations to the same time.\n",
    "    train_observed_a, train_estimated_a = temporal_alignment(\n",
    "        train_a, X_train_observed_a, X_train_estimated_a\n",
    "    )\n",
    "    train_observed_b, train_estimated_b = temporal_alignment(\n",
    "        train_b, X_train_observed_b, X_train_estimated_b\n",
    "    )\n",
    "    train_observed_c, train_estimated_c = temporal_alignment(\n",
    "        train_c, X_train_observed_c, X_train_estimated_c\n",
    "    )\n",
    "\n",
    "    # Add location data\n",
    "    train_observed_a = add_location(train_observed_a, \"a\")\n",
    "    train_estimated_a = add_location(train_estimated_a, \"a\")\n",
    "\n",
    "    train_observed_b = add_location(train_observed_b, \"b\")\n",
    "    train_estimated_b = add_location(train_estimated_b, \"b\")\n",
    "\n",
    "    train_observed_c = add_location(train_observed_c, \"c\")\n",
    "    train_estimated_c = add_location(train_estimated_c, \"c\")\n",
    "\n",
    "    # Combine the temporally aligned datasets from all three locations\n",
    "    train_observed_combined = pd.concat(\n",
    "        [train_observed_a, train_observed_b, train_observed_c], ignore_index=True\n",
    "    )\n",
    "    train_estimated_combined = pd.concat(\n",
    "        [train_estimated_a, train_estimated_b, train_estimated_c], ignore_index=True\n",
    "    )\n",
    "\n",
    "\n",
    "    # # Add boolean flag for estimated vs observed\n",
    "    # train_observed_combined[\"estimated_flag\"] = 0\n",
    "    # train_estimated_combined[\"estimated_flag\"] = 1\n",
    "\n",
    "    # Prepare the combined dataset by handling missing values and splitting the data\n",
    "    (\n",
    "        X_train_obs_combined,\n",
    "        X_val_obs_combined,\n",
    "        y_train_obs_combined,\n",
    "        y_val_obs_combined,\n",
    "        X_train_est_combined,\n",
    "        X_val_est_combined,\n",
    "        y_train_est_combined,\n",
    "        y_val_est_combined,\n",
    "    ) = prepare_data(train_observed_combined, train_estimated_combined, drop_features=drop_features)\n",
    "\n",
    "    return (\n",
    "        X_train_obs_combined,\n",
    "        X_val_obs_combined,\n",
    "        y_train_obs_combined,\n",
    "        y_val_obs_combined,\n",
    "        X_train_est_combined,\n",
    "        X_val_est_combined,\n",
    "        y_train_est_combined,\n",
    "        y_val_est_combined,\n",
    "    )\n",
    "\n",
    "def get_preprocessed_test_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the preprocessed test data without the 'date_forecast' column.\n",
    "    \"\"\"\n",
    "    (\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        X_test_estimated_a,\n",
    "        X_test_estimated_b,\n",
    "        X_test_estimated_c,\n",
    "    ) = get_raw_data()\n",
    "\n",
    "    # Align the test data to the same time as the training data\n",
    "    X_test_estimated_a = temporal_alignment_tests(X_test_estimated_a)\n",
    "    X_test_estimated_b = temporal_alignment_tests(X_test_estimated_b)\n",
    "    X_test_estimated_c = temporal_alignment_tests(X_test_estimated_c)\n",
    "    print(\"After temporal alignment\")\n",
    "    print(f\"X_test_estimated_a.shape = {X_test_estimated_a.shape}, X_test_estimated_b.shape = {X_test_estimated_b.shape}, X_test_estimated_c.shape = {X_test_estimated_c.shape}\")\n",
    "\n",
    "    X_test_estimated_a = remove_missing_features(X_test_estimated_a)\n",
    "    X_test_estimated_b = remove_missing_features(X_test_estimated_b)\n",
    "    X_test_estimated_c = remove_missing_features(X_test_estimated_c)\n",
    "\n",
    "    # Add location data\n",
    "    X_test_estimated_a = add_location(X_test_estimated_a, \"a\")\n",
    "    X_test_estimated_b = add_location(X_test_estimated_b, \"b\")\n",
    "    X_test_estimated_c = add_location(X_test_estimated_c, \"c\")\n",
    "\n",
    "    X_test_a_correct_features = feature_engineer(X_test_estimated_a)\n",
    "    X_test_b_correct_features = feature_engineer(X_test_estimated_b)\n",
    "    X_test_c_correct_features = feature_engineer(X_test_estimated_c)\n",
    "\n",
    "    # X_train_obs_combined, X_val_obs_combined, y_train_obs_combined, y_val_obs_combined, X_train_est_combined, X_val_est_combined, y_train_est_combined, y_val_est_combined = fetch_preprocessed_data()\n",
    "    \n",
    "    # # Add historical data so that the model can use it for prediction\n",
    "    # # Add mean_pv_measurement with same day and hour from previous years\n",
    "    # X_test_estimated_a_with_historical_data = add_expected_pv_to_test_data(X_test_a_correct_features, X_train_obs_combined)\n",
    "    # X_test_estimated_b_with_historical_data = add_expected_pv_to_test_data(X_test_b_correct_features, X_train_obs_combined)\n",
    "    # X_test_estimated_c_with_historical_data = add_expected_pv_to_test_data(X_test_c_correct_features, X_train_obs_combined)\n",
    "\n",
    "    # Drop the 'date_calc' and 'date_forecast' columns from the test data\n",
    "    X_test_estimated_a_processed = X_test_a_correct_features.drop(\n",
    "        columns=[\"date_calc\", \"date_forecast\"], errors='ignore'\n",
    "    )\n",
    "    X_test_estimated_b_processed = X_test_b_correct_features.drop(\n",
    "        columns=[\"date_calc\", \"date_forecast\"], errors='ignore'\n",
    "    )\n",
    "    X_test_estimated_c_processed = X_test_c_correct_features.drop(\n",
    "        columns=[\"date_calc\", \"date_forecast\"], errors='ignore'\n",
    "    )\n",
    "\n",
    "    # # # Handle NaN values in the test data by filling them with the mean value of the respective column from the training data\n",
    "    # X_test_estimated_a_processed.dropna()\n",
    "    # X_test_estimated_b_processed.dropna()\n",
    "    # X_test_estimated_c_processed.dropna()\n",
    "    print(f\"X_test_estimated_a_processed.shape = {X_test_estimated_a_processed.shape}, X_test_estimated_b_processed.shape = {X_test_estimated_b_processed.shape}, X_test_estimated_c_processed.shape = {X_test_estimated_c_processed.shape}\")\n",
    "    tests = pd.concat([X_test_estimated_a_processed, X_test_estimated_b_processed, X_test_estimated_c_processed], ignore_index=True)\n",
    "\n",
    "    # Add boolean flag for estimated vs observed data\n",
    "    # tests[\"estimated_flag\"] = 1\n",
    "    return tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After temporal alignment\n",
      "X_test_estimated_a.shape = (720, 47), X_test_estimated_b.shape = (720, 47), X_test_estimated_c.shape = (720, 47)\n",
      "X_test_estimated_a_processed.shape = (720, 60), X_test_estimated_b_processed.shape = (720, 60), X_test_estimated_c_processed.shape = (720, 60)\n"
     ]
    }
   ],
   "source": [
    "X_train_obs_combined, X_val_obs_combined, y_train_obs_combined, y_val_obs_combined, X_train_est_combined, X_val_est_combined, y_train_est_combined, y_val_est_combined = fetch_preprocessed_data()\n",
    "x_test_whole = get_preprocessed_test_data()\n",
    "\n",
    "#estimated_flag\n",
    "X_train_obs_combined[\"estimated_flag\"] = 0\n",
    "X_val_obs_combined[\"estimated_flag\"] = 0\n",
    "X_train_est_combined[\"estimated_flag\"] = 1\n",
    "X_val_est_combined[\"estimated_flag\"] = 1\n",
    "x_test_whole[\"estimated_flag\"] = 1\n",
    "\n",
    "\n",
    "x_whole = pd.concat([X_train_obs_combined, X_val_obs_combined, X_train_est_combined, X_val_est_combined])\n",
    "y_whole = pd.concat([y_train_obs_combined, y_val_obs_combined, y_train_est_combined, y_val_est_combined])\n",
    "x_whole.reset_index(drop=True, inplace=True)\n",
    "y_whole.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole[\"pv_measurement\"] = y_whole\n",
    "df_shuffled = x_whole.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "x_whole_a = df_shuffled[df_shuffled['location_a'] == 1]\n",
    "x_whole_b = df_shuffled[df_shuffled['location_b'] == 1]\n",
    "x_whole_c = df_shuffled[df_shuffled['location_c'] == 1]\n",
    "\n",
    "y_whole_a = x_whole_a[\"pv_measurement\"]\n",
    "x_whole_a = x_whole_a.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_a', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_b', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_c', axis = 1)\n",
    "\n",
    "y_whole_b = x_whole_b[\"pv_measurement\"]\n",
    "x_whole_b = x_whole_b.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_a', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_b', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_c', axis = 1)\n",
    "\n",
    "y_whole_c = x_whole_c[\"pv_measurement\"]\n",
    "x_whole_c = x_whole_c.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_a', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_b', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_c', axis = 1)\n",
    "cat_features = [\"estimated_flag\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "\n",
    "# Initialize the TabularPredictor\n",
    "best_model_a = TabularPredictor(label='pv_measurement', eval_metric='mean_absolute_error', problem_type=\"regression\").fit(\n",
    "    train_data=x_whole_a,\n",
    "    presets='best_quality',\n",
    "    time_limit=3600\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.08, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=9, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=2000, n_jobs=None, num_parallel_tree=None,\n",
       "             objective=&#x27;reg:pseudohubererror&#x27;, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.08, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=9, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=2000, n_jobs=None, num_parallel_tree=None,\n",
       "             objective=&#x27;reg:pseudohubererror&#x27;, predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.08, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=9, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=2000, n_jobs=None, num_parallel_tree=None,\n",
       "             objective='reg:pseudohubererror', predictor=None, ...)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# Initialize the TabularPredictor\n",
    "best_model_b = TabularPredictor(label='pv_measurement', eval_metric='mean_absolute_error', problem_type=\"regression\").fit(\n",
    "    train_data=x_whole_b,\n",
    "    presets='best_quality',\n",
    "    time_limit=3600\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.08, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=9, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=2000, n_jobs=None, num_parallel_tree=None,\n",
       "             objective=&#x27;reg:pseudohubererror&#x27;, predictor=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.08, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=9, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=2000, n_jobs=None, num_parallel_tree=None,\n",
       "             objective=&#x27;reg:pseudohubererror&#x27;, predictor=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.08, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=9, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=2000, n_jobs=None, num_parallel_tree=None,\n",
       "             objective='reg:pseudohubererror', predictor=None, ...)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "\n",
    "# Initialize the TabularPredictor\n",
    "best_model_c = TabularPredictor(label='pv_measurement', eval_metric='mean_absolute_error', problem_type=\"regression\").fit(\n",
    "    train_data=x_whole_c,\n",
    "    presets='best_quality',\n",
    "    time_limit=3600\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importances = best_model_a.get_feature_importance()\n",
    "\n",
    "# # Get feature names\n",
    "# feature_names = x_whole_a.columns\n",
    "\n",
    "# # Create a pandas DataFrame to display importances\n",
    "# import pandas as pd\n",
    "\n",
    "# feature_importance_df = pd.DataFrame({\n",
    "#     'Feature': feature_names,\n",
    "#     'Importance': feature_importances\n",
    "# })\n",
    "# # Display the feature importances\n",
    "# print(feature_importance_df)\n",
    "# # Sort the DataFrame to show the most important features at the top\n",
    "# feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# # If you want to plot feature importances, you can use a bar chart:\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.title('Feature Importances')\n",
    "# plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "# plt.gca().invert_yaxis()  # To display the highest importance at the top\n",
    "# plt.xlabel('Relative Importance')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importances = best_model_b.get_feature_importance()\n",
    "\n",
    "# # Get feature names\n",
    "# feature_names = x_whole_b.columns\n",
    "\n",
    "# # Create a pandas DataFrame to display importances\n",
    "# import pandas as pd\n",
    "\n",
    "# feature_importance_df = pd.DataFrame({\n",
    "#     'Feature': feature_names,\n",
    "#     'Importance': feature_importances\n",
    "# })\n",
    "\n",
    "# # Sort the DataFrame to show the most important features at the top\n",
    "# feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# # Display the feature importances\n",
    "# print(feature_importance_df)\n",
    "\n",
    "# # If you want to plot feature importances, you can use a bar chart:\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.title('Feature Importances')\n",
    "# plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "# plt.gca().invert_yaxis()  # To display the highest importance at the top\n",
    "# plt.xlabel('Relative Importance')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_importances = best_model_c.get_feature_importance()\n",
    "\n",
    "# # Get feature names\n",
    "# feature_names = x_whole_c.columns\n",
    "\n",
    "# # Create a pandas DataFrame to display importances\n",
    "# import pandas as pd\n",
    "\n",
    "# feature_importance_df = pd.DataFrame({\n",
    "#     'Feature': feature_names,\n",
    "#     'Importance': feature_importances\n",
    "# })\n",
    "\n",
    "# # Sort the DataFrame to show the most important features at the top\n",
    "# feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# # Display the feature importances\n",
    "# print(feature_importance_df)\n",
    "\n",
    "# # If you want to plot feature importances, you can use a bar chart:\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# plt.title('Feature Importances')\n",
    "# plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "# plt.gca().invert_yaxis()  # To display the highest importance at the top\n",
    "# plt.xlabel('Relative Importance')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from src.features.preprocess_data import fetch_preprocessed_data\n",
    "\n",
    "find_time_sin = lambda hour: math.sin(2 * math.pi * (hour) / 24)\n",
    "find_time_cos = lambda hour: math.cos(2 * math.pi * (hour) / 24)\n",
    "\n",
    "def postprocess_data(x_test: pd.DataFrame, y_pred: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Postprocess the data to set the predicted values to 0 at the correct times.\"\"\"\n",
    "    \n",
    "    # Cap the min and max values for each location for each hour\n",
    "    y_pred = cap_min_max_values(x_test, y_pred)\n",
    "\n",
    "    # Set the predicted values to 0 at the correct times\n",
    "    y_pred = set_0_pv_at_times(x_test, y_pred, \"a\", [22, 23, 0])\n",
    "    y_pred = set_0_pv_at_times(x_test, y_pred, \"b\", [22, 23, 0])\n",
    "    y_pred = set_0_pv_at_times(x_test, y_pred, \"c\", [22, 23, 0])\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def cap_min_max_values(x_test: pd.DataFrame, y_pred: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Cap the min and max values for each location for each hour.\"\"\"\n",
    "    for hour in range(24):\n",
    "        # Get the min and max values for each location for each hour\n",
    "        min_value_a, max_value_a = get_min_max_values_for_location_at_hour(\"a\", hour)\n",
    "        min_value_b, max_value_b = get_min_max_values_for_location_at_hour(\"b\", hour)\n",
    "        min_value_c, max_value_c = get_min_max_values_for_location_at_hour(\"c\", hour)\n",
    "        print(f\"hour: {hour}, min_value_a: {min_value_a}, max_value_a: {max_value_a}, min_value_b: {min_value_b}, max_value_b: {max_value_b}, min_value_c: {min_value_c}, max_value_c: {max_value_c}\")\n",
    "        # Cap the values between min_value and max_value\n",
    "        y_pred = cap_min_max_values_for_hour(x_test, y_pred, \"a\", hour, min_value_a, max_value_a)\n",
    "        y_pred = cap_min_max_values_for_hour(x_test, y_pred, \"b\", hour, min_value_b, max_value_b)\n",
    "        y_pred = cap_min_max_values_for_hour(x_test, y_pred, \"c\", hour, min_value_c, max_value_c)\n",
    "    return y_pred\n",
    "\n",
    "X_train_obs_combined, X_val_obs_combined, y_train_obs_combined, y_val_obs_combined, X_train_est_combined, X_val_est_combined, y_train_est_combined, y_val_est_combined = fetch_preprocessed_data(drop_features=False)\n",
    "x_whole_with_time = pd.concat([X_train_obs_combined, X_val_obs_combined, X_train_est_combined, X_val_est_combined])\n",
    "\n",
    "def get_min_max_values_for_location_at_hour(location: str, hour: int) -> tuple[float, float]:\n",
    "    \"\"\"Get the min and max values for a specific location at a specific hour.\"\"\"\n",
    "    # Get the x and y for the given hour and location\n",
    "    hour_sin = find_time_sin(hour)\n",
    "    hour_cos = find_time_cos(hour)\n",
    "    # find the min and max values for the given hour and location\n",
    "    min_value = x_whole_with_time[(x_whole_with_time[\"location_\" + location] == 1) & (x_whole_with_time[\"sin_hour\"] == hour_sin) & (x_whole_with_time[\"cos_hour\"] == hour_cos)][\"pv_measurement\"].min()\n",
    "    max_value = x_whole_with_time[(x_whole_with_time[\"location_\" + location] == 1) & (x_whole_with_time[\"sin_hour\"] == hour_sin) & (x_whole_with_time[\"cos_hour\"] == hour_cos)][\"pv_measurement\"].max()\n",
    "    \n",
    "    return (min_value, max_value)\n",
    "\n",
    "def cap_min_max_values_for_hour(x_test: pd.DataFrame, y_pred: pd.DataFrame, location: str, hour: int, min_value: float, max_value: float) -> pd.DataFrame:\n",
    "    \"\"\"Cap the min and max values for a specific hour.\"\"\"\n",
    "    \n",
    "    # Calculate sin and cos values for the given hour\n",
    "    hour_sin = find_time_sin(hour)\n",
    "    hour_cos = find_time_cos(hour)\n",
    "    \n",
    "    # Find indices corresponding to the given hour at the given location\n",
    "    indices = x_test[(x_test[\"location_\" + location] == 1) & (x_test[\"sin_hour\"] == hour_sin) & (x_test[\"cos_hour\"] == hour_cos)].index\n",
    "    \n",
    "    # Cap the values between min_value and max_value\n",
    "    y_pred.loc[indices] = y_pred.loc[indices].clip(min_value, max_value)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def set_0_pv_at_times(x_test: pd.DataFrame, y_pred: pd.DataFrame, location: str, hours: list[int]) -> pd.DataFrame:\n",
    "    \"\"\"Find the correct predicted values at the given times and locaiton and set them to 0.\"\"\"\n",
    "    hours_to_set_0_sin = [find_time_sin(hour) for hour in hours]\n",
    "    hours_to_set_0_cos = [find_time_cos(hour) for hour in hours]\n",
    "\n",
    "\n",
    "    indices = x_test[(x_test[\"location_\" + location] == 1) & (x_test[\"sin_hour\"].isin(hours_to_set_0_sin) & (x_test[\"cos_hour\"].isin(hours_to_set_0_cos)))].index\n",
    "    for index in indices:\n",
    "        y_pred.loc[index] = 0\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole_a = x_test_whole[x_test_whole['location_a'] == 1]\n",
    "x_whole_b = x_test_whole[x_test_whole['location_b'] == 1]\n",
    "x_whole_c = x_test_whole[x_test_whole['location_c'] == 1]\n",
    "\n",
    "x_whole_a = x_whole_a.drop('location_a', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_b', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_c', axis = 1)\n",
    "\n",
    "x_whole_b = x_whole_b.drop('location_a', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_b', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_c', axis = 1)\n",
    "\n",
    "x_whole_c = x_whole_c.drop('location_a', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_b', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_c', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "RES_PATH = 'results/output/'\n",
    "\n",
    "\n",
    "def save_predictions(test: pd.DataFrame, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the 'id' and 'prediction' columns of the test DataFrame to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        test (pd.DataFrame): A 1D DataFrame containing only the predictions.\n",
    "        filename (str): The name of the file where the predictions will be saved.\n",
    "    \"\"\"\n",
    "    model = pd.DataFrame()\n",
    "    \n",
    "    model[\"prediction\"] = test\n",
    "    model['id'] = model.index\n",
    "\n",
    "    model['prediction'] = model['prediction'].apply(lambda x: max(0, x))\n",
    "    \n",
    "    # Reorder the columns to ensure 'id' comes before 'prediction'\n",
    "    model = model[['id', 'prediction']]\n",
    "    \n",
    "\n",
    "    # Save the resulting DataFrame to a CSV file\n",
    "    model.to_csv(f'{RES_PATH}{filename}.csv', index=False)\n",
    "    \n",
    "    # Display the first few rows of the saved DataFrame\n",
    "    print(model.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hour: 0, min_value_a: 0.0, max_value_a: 3.3, min_value_b: -0.0, max_value_b: -0.0, min_value_c: 0.0, max_value_c: 0.0\n",
      "hour: 1, min_value_a: 0.0, max_value_a: 53.68, min_value_b: -0.0, max_value_b: 12.075, min_value_c: 0.0, max_value_c: 9.8\n",
      "hour: 2, min_value_a: 0.0, max_value_a: 233.64000000000001, min_value_b: -0.0, max_value_b: 68.1375, min_value_c: 0.0, max_value_c: 39.2\n",
      "hour: 3, min_value_a: 0.0, max_value_a: 439.12, min_value_b: -0.0, max_value_b: 138.0, min_value_c: 0.0, max_value_c: 88.2\n",
      "hour: 4, min_value_a: 0.0, max_value_a: 1046.98, min_value_b: -0.0, max_value_b: 307.05, min_value_c: 0.0, max_value_c: 176.4\n",
      "hour: 5, min_value_a: 0.0, max_value_a: 2049.08, min_value_b: -0.0, max_value_b: 452.8125, min_value_c: 0.0, max_value_c: 264.6\n",
      "hour: 6, min_value_a: 0.0, max_value_a: 3244.78, min_value_b: -0.0, max_value_b: 681.375, min_value_c: 0.0, max_value_c: 499.8\n",
      "hour: 7, min_value_a: 0.0, max_value_a: 4266.46, min_value_b: -0.0, max_value_b: 865.0875, min_value_c: 0.0, max_value_c: 705.6\n",
      "hour: 8, min_value_a: 0.0, max_value_a: 5048.780000000001, min_value_b: -0.0, max_value_b: 997.9125, min_value_c: 0.0, max_value_c: 784.0\n",
      "hour: 9, min_value_a: 0.0, max_value_a: 5477.339999999999, min_value_b: -0.0, max_value_b: 1091.925, min_value_c: 0.0, max_value_c: 882.0000000000001\n",
      "hour: 10, min_value_a: 0.0, max_value_a: 5733.42, min_value_b: -0.0, max_value_b: 1146.2625, min_value_c: 0.0, max_value_c: 980.0000000000001\n",
      "hour: 11, min_value_a: 0.0, max_value_a: 5651.8, min_value_b: -0.0, max_value_b: 1120.3875, min_value_c: 0.0, max_value_c: 999.6\n",
      "hour: 12, min_value_a: 0.0, max_value_a: 5344.46, min_value_b: 0.0, max_value_b: 1152.3, min_value_c: 0.0, max_value_c: 950.6\n",
      "hour: 13, min_value_a: 0.0, max_value_a: 5058.900000000001, min_value_b: -0.0, max_value_b: 1053.9750000000001, min_value_c: 0.0, max_value_c: 911.4000000000001\n",
      "hour: 14, min_value_a: 0.0, max_value_a: 4491.3, min_value_b: -0.0, max_value_b: 823.6875, min_value_c: 0.0, max_value_c: 803.6\n",
      "hour: 15, min_value_a: 0.0, max_value_a: 3633.96, min_value_b: -0.0, max_value_b: 723.6375, min_value_c: 0.0, max_value_c: 597.8000000000001\n",
      "hour: 16, min_value_a: 0.0, max_value_a: 2710.4, min_value_b: -0.0, max_value_b: 601.1625, min_value_c: 0.0, max_value_c: 494.90000000000003\n",
      "hour: 17, min_value_a: 0.0, max_value_a: 1606.0, min_value_b: -0.0, max_value_b: 363.1125, min_value_c: 0.0, max_value_c: 303.8\n",
      "hour: 18, min_value_a: 0.0, max_value_a: 849.64, min_value_b: -0.0, max_value_b: 242.3625, min_value_c: 0.0, max_value_c: 176.4\n",
      "hour: 19, min_value_a: 0.0, max_value_a: 554.4, min_value_b: -0.0, max_value_b: 145.7625, min_value_c: 0.0, max_value_c: 107.80000000000001\n",
      "hour: 20, min_value_a: 0.0, max_value_a: 197.12, min_value_b: -0.0, max_value_b: 33.637499999999996, min_value_c: 0.0, max_value_c: 29.400000000000002\n",
      "hour: 21, min_value_a: 0.0, max_value_a: 19.139999999999997, min_value_b: -0.0, max_value_b: 13.8, min_value_c: 0.0, max_value_c: 9.8\n",
      "hour: 22, min_value_a: 0.0, max_value_a: 0.66, min_value_b: -0.0, max_value_b: -0.0, min_value_c: -0.0, max_value_c: -0.0\n",
      "hour: 23, min_value_a: 0.0, max_value_a: 0.0, min_value_b: -0.0, max_value_b: -0.0, min_value_c: -0.0, max_value_c: -0.0\n",
      "   id  prediction\n",
      "0   0    0.013495\n",
      "1   1    0.013495\n",
      "2   2    0.026042\n",
      "3   3   80.950157\n",
      "4   4  337.983063\n"
     ]
    }
   ],
   "source": [
    "y_predictions_a = best_model_a.predict(x_whole_a)\n",
    "y_predictions_b = best_model_b.predict(x_whole_b)\n",
    "y_predictions_c = best_model_c.predict(x_whole_c)\n",
    "y_predictions = pd.concat([pd.Series(y_predictions_a), pd.Series(y_predictions_b), pd.Series(y_predictions_c)])\n",
    "y_predictions = y_predictions.reset_index(drop=True)\n",
    "# Save the model\n",
    "y_predictions_catboost_1 = postprocess_data(x_test_whole, pd.DataFrame(y_predictions))\n",
    "save_predictions(y_predictions, 'autogluon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second catboost model with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole = x_whole.drop(\"modified_solar_elevation_squared\", axis=1)\n",
    "x_test_whole = x_test_whole.drop(\"modified_solar_elevation_squared\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole[\"pv_measurement\"] = y_whole\n",
    "df_shuffled = x_whole.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "x_whole_a = df_shuffled[df_shuffled['location_a'] == 1]\n",
    "x_whole_b = df_shuffled[df_shuffled['location_b'] == 1]\n",
    "x_whole_c = df_shuffled[df_shuffled['location_c'] == 1]\n",
    "\n",
    "y_whole_a = x_whole_a[\"pv_measurement\"]\n",
    "x_whole_a = x_whole_a.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_a', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_b', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_c', axis = 1)\n",
    "\n",
    "y_whole_b = x_whole_b[\"pv_measurement\"]\n",
    "x_whole_b = x_whole_b.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_a', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_b', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_c', axis = 1)\n",
    "\n",
    "y_whole_c = x_whole_c[\"pv_measurement\"]\n",
    "x_whole_c = x_whole_c.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_a', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_b', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_c', axis = 1)\n",
    "cat_features = [\"estimated_flag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 616.9363093\ttotal: 53.9ms\tremaining: 53.9s\n",
      "100:\tlearn: 199.9656401\ttotal: 4.37s\tremaining: 38.9s\n",
      "200:\tlearn: 185.3356686\ttotal: 8.56s\tremaining: 34s\n",
      "300:\tlearn: 180.5070612\ttotal: 12.5s\tremaining: 29s\n",
      "400:\tlearn: 176.7196537\ttotal: 16.5s\tremaining: 24.7s\n",
      "500:\tlearn: 172.6197441\ttotal: 20.8s\tremaining: 20.7s\n",
      "600:\tlearn: 163.7581476\ttotal: 25s\tremaining: 16.6s\n",
      "700:\tlearn: 153.8238135\ttotal: 29.1s\tremaining: 12.4s\n",
      "800:\tlearn: 147.6203702\ttotal: 33.2s\tremaining: 8.25s\n",
      "900:\tlearn: 141.6572310\ttotal: 37.2s\tremaining: 4.09s\n",
      "999:\tlearn: 136.7908527\ttotal: 41.3s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x261e5499060>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_a = CatBoostRegressor(\n",
    "    max_depth=9,\n",
    "    cat_features=cat_features,\n",
    "    loss_function=\"MAE\",\n",
    "    verbose = 100\n",
    ")\n",
    "best_model_a.fit(x_whole_a, y_whole_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 94.1836286\ttotal: 42.4ms\tremaining: 42.3s\n",
      "100:\tlearn: 25.3949714\ttotal: 4.11s\tremaining: 36.6s\n",
      "200:\tlearn: 22.1209997\ttotal: 8.12s\tremaining: 32.3s\n",
      "300:\tlearn: 20.7702607\ttotal: 12.1s\tremaining: 28s\n",
      "400:\tlearn: 19.4111036\ttotal: 15.9s\tremaining: 23.8s\n",
      "500:\tlearn: 18.1584056\ttotal: 19.8s\tremaining: 19.8s\n",
      "600:\tlearn: 17.4038913\ttotal: 23.8s\tremaining: 15.8s\n",
      "700:\tlearn: 16.7939747\ttotal: 27.6s\tremaining: 11.8s\n",
      "800:\tlearn: 16.1720278\ttotal: 31.5s\tremaining: 7.84s\n",
      "900:\tlearn: 15.6136213\ttotal: 35.4s\tremaining: 3.89s\n",
      "999:\tlearn: 15.1423810\ttotal: 39.3s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x26187bf8070>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_b = CatBoostRegressor(\n",
    "    max_depth=9,\n",
    "    cat_features=cat_features,\n",
    "    loss_function = \"MAE\",\n",
    "    verbose = 100\n",
    ")\n",
    "best_model_b.fit(x_whole_b, y_whole_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 78.7273693\ttotal: 41.7ms\tremaining: 41.6s\n",
      "100:\tlearn: 21.3078061\ttotal: 4.09s\tremaining: 36.4s\n",
      "200:\tlearn: 18.2258783\ttotal: 8.01s\tremaining: 31.8s\n",
      "300:\tlearn: 17.0883950\ttotal: 11.8s\tremaining: 27.5s\n",
      "400:\tlearn: 16.1611729\ttotal: 15.8s\tremaining: 23.5s\n",
      "500:\tlearn: 15.2436549\ttotal: 19.6s\tremaining: 19.5s\n",
      "600:\tlearn: 14.4853484\ttotal: 23.4s\tremaining: 15.5s\n",
      "700:\tlearn: 13.7033024\ttotal: 27.2s\tremaining: 11.6s\n",
      "800:\tlearn: 13.1848967\ttotal: 31.1s\tremaining: 7.73s\n",
      "900:\tlearn: 12.7181220\ttotal: 34.9s\tremaining: 3.84s\n",
      "999:\tlearn: 12.3918690\ttotal: 38.7s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x261e5498910>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_c = CatBoostRegressor(\n",
    "    max_depth=9,\n",
    "    cat_features=cat_features,\n",
    "    loss_function = \"MAE\",\n",
    "    verbose = 100\n",
    ")\n",
    "best_model_c.fit(x_whole_c, y_whole_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole_a = x_test_whole[x_test_whole['location_a'] == 1]\n",
    "x_whole_b = x_test_whole[x_test_whole['location_b'] == 1]\n",
    "x_whole_c = x_test_whole[x_test_whole['location_c'] == 1]\n",
    "\n",
    "x_whole_a = x_whole_a.drop('location_a', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_b', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_c', axis = 1)\n",
    "\n",
    "x_whole_b = x_whole_b.drop('location_a', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_b', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_c', axis = 1)\n",
    "\n",
    "x_whole_c = x_whole_c.drop('location_a', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_b', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_c', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hour: 0, min_value_a: 0.0, max_value_a: 3.3, min_value_b: -0.0, max_value_b: -0.0, min_value_c: 0.0, max_value_c: 0.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hour: 1, min_value_a: 0.0, max_value_a: 53.68, min_value_b: -0.0, max_value_b: 12.075, min_value_c: 0.0, max_value_c: 9.8\n",
      "hour: 2, min_value_a: 0.0, max_value_a: 233.64000000000001, min_value_b: -0.0, max_value_b: 68.1375, min_value_c: 0.0, max_value_c: 39.2\n",
      "hour: 3, min_value_a: 0.0, max_value_a: 439.12, min_value_b: -0.0, max_value_b: 138.0, min_value_c: 0.0, max_value_c: 88.2\n",
      "hour: 4, min_value_a: 0.0, max_value_a: 1046.98, min_value_b: -0.0, max_value_b: 307.05, min_value_c: 0.0, max_value_c: 176.4\n",
      "hour: 5, min_value_a: 0.0, max_value_a: 2049.08, min_value_b: -0.0, max_value_b: 452.8125, min_value_c: 0.0, max_value_c: 264.6\n",
      "hour: 6, min_value_a: 0.0, max_value_a: 3244.78, min_value_b: -0.0, max_value_b: 681.375, min_value_c: 0.0, max_value_c: 499.8\n",
      "hour: 7, min_value_a: 0.0, max_value_a: 4266.46, min_value_b: -0.0, max_value_b: 865.0875, min_value_c: 0.0, max_value_c: 705.6\n",
      "hour: 8, min_value_a: 0.0, max_value_a: 5048.780000000001, min_value_b: -0.0, max_value_b: 997.9125, min_value_c: 0.0, max_value_c: 784.0\n",
      "hour: 9, min_value_a: 0.0, max_value_a: 5477.339999999999, min_value_b: -0.0, max_value_b: 1091.925, min_value_c: 0.0, max_value_c: 882.0000000000001\n",
      "hour: 10, min_value_a: 0.0, max_value_a: 5733.42, min_value_b: -0.0, max_value_b: 1146.2625, min_value_c: 0.0, max_value_c: 980.0000000000001\n",
      "hour: 11, min_value_a: 0.0, max_value_a: 5651.8, min_value_b: -0.0, max_value_b: 1120.3875, min_value_c: 0.0, max_value_c: 999.6\n",
      "hour: 12, min_value_a: 0.0, max_value_a: 5344.46, min_value_b: 0.0, max_value_b: 1152.3, min_value_c: 0.0, max_value_c: 950.6\n",
      "hour: 13, min_value_a: 0.0, max_value_a: 5058.900000000001, min_value_b: -0.0, max_value_b: 1053.9750000000001, min_value_c: 0.0, max_value_c: 911.4000000000001\n",
      "hour: 14, min_value_a: 0.0, max_value_a: 4491.3, min_value_b: -0.0, max_value_b: 823.6875, min_value_c: 0.0, max_value_c: 803.6\n",
      "hour: 15, min_value_a: 0.0, max_value_a: 3633.96, min_value_b: -0.0, max_value_b: 723.6375, min_value_c: 0.0, max_value_c: 597.8000000000001\n",
      "hour: 16, min_value_a: 0.0, max_value_a: 2710.4, min_value_b: -0.0, max_value_b: 601.1625, min_value_c: 0.0, max_value_c: 494.90000000000003\n",
      "hour: 17, min_value_a: 0.0, max_value_a: 1606.0, min_value_b: -0.0, max_value_b: 363.1125, min_value_c: 0.0, max_value_c: 303.8\n",
      "hour: 18, min_value_a: 0.0, max_value_a: 849.64, min_value_b: -0.0, max_value_b: 242.3625, min_value_c: 0.0, max_value_c: 176.4\n",
      "hour: 19, min_value_a: 0.0, max_value_a: 554.4, min_value_b: -0.0, max_value_b: 145.7625, min_value_c: 0.0, max_value_c: 107.80000000000001\n",
      "hour: 20, min_value_a: 0.0, max_value_a: 197.12, min_value_b: -0.0, max_value_b: 33.637499999999996, min_value_c: 0.0, max_value_c: 29.400000000000002\n",
      "hour: 21, min_value_a: 0.0, max_value_a: 19.139999999999997, min_value_b: -0.0, max_value_b: 13.8, min_value_c: 0.0, max_value_c: 9.8\n",
      "hour: 22, min_value_a: 0.0, max_value_a: 0.66, min_value_b: -0.0, max_value_b: -0.0, min_value_c: -0.0, max_value_c: -0.0\n",
      "hour: 23, min_value_a: 0.0, max_value_a: 0.0, min_value_b: -0.0, max_value_b: -0.0, min_value_c: -0.0, max_value_c: -0.0\n",
      "   id    prediction\n",
      "0   0  0.000000e+00\n",
      "1   1  5.292587e-07\n",
      "2   2  5.308785e-07\n",
      "3   3  4.747112e+01\n",
      "4   4  3.363666e+02\n"
     ]
    }
   ],
   "source": [
    "y_predictions_a = best_model_a.predict(x_whole_a)\n",
    "y_predictions_b = best_model_b.predict(x_whole_b)\n",
    "y_predictions_c = best_model_c.predict(x_whole_c)\n",
    "y_predictions = pd.concat([pd.Series(y_predictions_a), pd.Series(y_predictions_b), pd.Series(y_predictions_c)])\n",
    "y_predictions = y_predictions.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Save the model\n",
    "y_predictions_catboost_2 = postprocess_data(x_test_whole, pd.DataFrame(y_predictions))\n",
    "save_predictions(y_predictions, 'catboost without modified solar elevation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id  prediction\n",
      "0   0    0.000000\n",
      "1   1    0.006748\n",
      "2   2    0.013021\n",
      "3   3   64.210640\n",
      "4   4  337.174826\n"
     ]
    }
   ],
   "source": [
    "average_prediction = (y_predictions_catboost_1 + y_predictions_catboost_2) / 2\n",
    "save_predictions(average_prediction, \"catboost final\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
