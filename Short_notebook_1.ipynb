{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team name: Group 6\n",
    "* Full name: Sverre Nystad, StudentNr: 56882,  Kandidatnr: 10003, Kaggle Name: Sverre Nystad\n",
    "* Full name: Gunnar Nystad, StudentNr: 527760, Kandidatnr: 10344, Kaggle Name: Gunnar Nystad\n",
    "* Full name: Peter Skoland, StudentNr: 528091, Kandidatnr 10307,  Kaggle Name: Peter Skoland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost stack\n",
    "This stack is this csv in on kaggle: \"average cat cloud interaction 144.9 cat mod elevation squared 144.8.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import skew\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0.1: Preprocessing of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_RAW_DATA_LOCATION = \"data/raw/\"\n",
    "\n",
    "def get_raw_data():\n",
    "    \"\"\"\n",
    "    Utility function to load the raw data from the data/raw folder.\n",
    "\n",
    "    Returns:\n",
    "        train_a (pd.DataFrame): The training targets for the A dataset.\n",
    "        train_b (pd.DataFrame): The training targets for the B dataset.\n",
    "        train_c (pd.DataFrame): The training targets for the C dataset.\n",
    "        X_train_estimated_a (pd.DataFrame): The estimated training features for the A dataset.\n",
    "        X_train_estimated_b (pd.DataFrame): The estimated training features for the B dataset.\n",
    "        X_train_estimated_c (pd.DataFrame): The estimated training features for the C dataset.\n",
    "        X_train_observed_a (pd.DataFrame): The observed training features for the A dataset.\n",
    "        X_train_observed_b (pd.DataFrame): The observed training features for the B dataset.\n",
    "        X_train_observed_c (pd.DataFrame): The observed training features for the C dataset.\n",
    "        X_test_estimated_a (pd.DataFrame): The estimated test features for the A dataset.\n",
    "        X_test_estimated_b (pd.DataFrame): The estimated test features for the B dataset.\n",
    "        X_test_estimated_c (pd.DataFrame): The estimated test features for the C dataset.\n",
    "    \"\"\"\n",
    "    train_a = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}A/train_targets.parquet')\n",
    "    train_b = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}B/train_targets.parquet')\n",
    "    train_c = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}C/train_targets.parquet')\n",
    "    X_train_estimated_a = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}A/X_train_estimated.parquet')\n",
    "    X_train_estimated_b = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}B/X_train_estimated.parquet')\n",
    "    X_train_estimated_c = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}C/X_train_estimated.parquet')\n",
    "    X_train_observed_a = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}A/X_train_observed.parquet')\n",
    "    X_train_observed_b = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}B/X_train_observed.parquet')\n",
    "    X_train_observed_c = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}C/X_train_observed.parquet')\n",
    "    X_test_estimated_a = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}A/X_test_estimated.parquet')\n",
    "    X_test_estimated_b = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}B/X_test_estimated.parquet')\n",
    "    X_test_estimated_c = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}C/X_test_estimated.parquet')\n",
    "\n",
    "    return train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(\n",
    "    train_observed: pd.DataFrame,\n",
    "    train_estimated: pd.DataFrame,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    drop_features: bool = True,\n",
    ") -> Tuple[\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.Series,\n",
    "    pd.Series,\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.Series,\n",
    "    pd.Series,\n",
    "]:\n",
    "    \"\"\"\n",
    "    Prepares the data for modeling by handling missing values and splitting the data.\n",
    "\n",
    "    Args:\n",
    "    train_observed (pd.DataFrame): The aligned training DataFrame with observed features.\n",
    "    train_estimated (pd.DataFrame): The aligned training DataFrame with estimated features.\n",
    "    test_size (float): The proportion of the dataset to include in the test split.\n",
    "    random_state (int): Controls the shuffling applied to the data before applying the split.\n",
    "\n",
    "    Returns:\n",
    "    X_train_obs (pd.DataFrame): The training features with observed data.\n",
    "    X_val_obs (pd.DataFrame): The validation features with observed data.\n",
    "    y_train_obs (pd.Series): The training target with observed data.\n",
    "    y_val_obs (pd.Series): The validation target with observed data.\n",
    "    X_train_est (pd.DataFrame): The training features with estimated data.\n",
    "    X_val_est (pd.DataFrame): The validation features with estimated data.\n",
    "    y_train_est (pd.Series): The training target with estimated data.\n",
    "    y_val_est (pd.Series): The validation target with estimated data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove missing features\n",
    "    train_observed = remove_missing_features(train_observed)\n",
    "    train_estimated = remove_missing_features(train_estimated)\n",
    "\n",
    "    # Handle missing values (e.g., imputation, removal)\n",
    "    train_observed_clean = train_observed.dropna(\n",
    "        subset=[\"visibility:m\", \"pv_measurement\"]\n",
    "    )\n",
    "    train_estimated_clean = train_estimated.dropna(\n",
    "        subset=[\"visibility:m\", \"pv_measurement\"]\n",
    "    )\n",
    "\n",
    "    # Remove discrepancies\n",
    "    train_observed_clean = clean_pv_data(train_observed_clean)\n",
    "    train_estimated_clean = clean_pv_data(train_estimated_clean)\n",
    "\n",
    "    # Feature engineer\n",
    "    train_observed_clean = feature_engineer(train_observed_clean)\n",
    "    train_estimated_clean = feature_engineer(train_estimated_clean)\n",
    "\n",
    "    # Split the data into features (X) and target (y)\n",
    "    y_obs = train_observed_clean[\"pv_measurement\"]\n",
    "\n",
    "    if drop_features:\n",
    "        X_obs = train_observed_clean.drop(\n",
    "            columns=[\"time\", \"pv_measurement\", \"date_forecast\", \"date_calc\"],\n",
    "            errors=\"ignore\",\n",
    "        )\n",
    "    else:\n",
    "        X_obs = train_observed_clean\n",
    "\n",
    "    if drop_features:\n",
    "        X_est = train_estimated_clean.drop(\n",
    "            columns=[\"time\", \"pv_measurement\", \"date_calc\", \"date_forecast\"],\n",
    "            errors=\"ignore\",\n",
    "        )\n",
    "    else:\n",
    "        X_est = train_estimated_clean\n",
    "\n",
    "    y_est = train_estimated_clean[\"pv_measurement\"]\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train_obs, X_val_obs, y_train_obs, y_val_obs = train_test_split(\n",
    "        X_obs, y_obs, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    X_train_est, X_val_est, y_train_est, y_val_est = train_test_split(\n",
    "        X_est, y_est, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        X_train_obs,\n",
    "        X_val_obs,\n",
    "        y_train_obs,\n",
    "        y_val_obs,\n",
    "        X_train_est,\n",
    "        X_val_est,\n",
    "        y_train_est,\n",
    "        y_val_est,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_location_datasets(\n",
    "    df: pd.DataFrame,\n",
    ") -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    locations = [\"location_a\", \"location_b\", \"location_c\"]\n",
    "    x_a = df[df[\"location_a\"] == 1]\n",
    "    x_a = x_a.drop(locations, axis=1)\n",
    "    y_a = x_a[\"pv_measurement\"]\n",
    "    if \"pv_measurement\" in x_a.columns:\n",
    "        x_a = x_a.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "    x_b = df[df[\"location_b\"] == 1]\n",
    "    x_b = x_b.drop(locations, axis=1)\n",
    "    y_b = x_b[\"pv_measurement\"]\n",
    "    if \"pv_measurement\" in x_b.columns:\n",
    "        x_b = x_b.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "    x_c = df[df[\"location_c\"] == 1]\n",
    "    x_c = x_c.drop(locations, axis=1)\n",
    "    y_c = x_c[\"pv_measurement\"]\n",
    "    if \"pv_measurement\" in x_b.columns:\n",
    "        x_b = x_b.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "    return (x_a, x_b, x_c, y_a, y_b, y_c)\n",
    "\n",
    "\n",
    "def remove_missing_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Remove features with more than 50% missing values or Constant features\n",
    "    df = df.drop(\"snow_density:kgm3\", axis=1)\n",
    "    df = df.drop(\"ceiling_height_agl:m\", axis=1)\n",
    "    df = df.drop(\"elevation:m\", axis=1)\n",
    "    df[\"cloud_base_agl:m\"] = df[\"cloud_base_agl:m\"].fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_pv_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply a series of filters to clean PV data in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame containing PV measurement data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Cleaned DataFrame after applying all filters.\n",
    "    \"\"\"\n",
    "    df = filter_pv_measurements_at_night(df)\n",
    "    df = filter_constant_pv_measurements(df)\n",
    "    df = filter_zero_pv_measurements(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_pv_measurements_at_night(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter out positive PV measurements at night based on specific conditions. As there are no PV measurements at night, these are likely to be measurement errors.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame containing PV measurement and time-of-day data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with unrealistic positive PV measurements at night removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Condition: Positive PV measurement when it's not daytime and the measurement is the same as the previous timestep\n",
    "    night_time_with_positive_pv = (df[\"is_day:idx\"] == 0) & (df[\"pv_measurement\"] > 0)\n",
    "    same_as_previous_step = df[\"pv_measurement\"] == df[\"pv_measurement\"].shift(1)\n",
    "    condition1 = night_time_with_positive_pv & same_as_previous_step\n",
    "\n",
    "    # Condition: Positive PV measurement when sun elevation is below a certain threshold\n",
    "    sun_elevation_threshold = -10\n",
    "    low_sun_elevation_with_positive_pv = (df[\"sun_elevation:d\"] < sun_elevation_threshold) & (df[\"pv_measurement\"] > 0)\n",
    "    \n",
    "    # Combined condition to filter\n",
    "    conditions_to_remove = condition1 | low_sun_elevation_with_positive_pv\n",
    "    df = df.drop(df[conditions_to_remove].index)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_constant_pv_measurements(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter out rows where PV measurement is constant and non-zero for 6 or more consecutive timesteps.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame containing PV measurement data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with specified discrepancies removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Mark changes in pv_measurement and zero values\n",
    "    measurement_change_or_zero = (df[\"pv_measurement\"] != df[\"pv_measurement\"].shift()) | (df[\"pv_measurement\"] == 0)\n",
    "    \n",
    "    # Step 2: Create groups for consecutive measurements\n",
    "    df[\"temp_group\"] = measurement_change_or_zero.cumsum()\n",
    "\n",
    "    # Step 3: Count entries in each group\n",
    "    group_counts = df.groupby(\"temp_group\")[\"pv_measurement\"].transform(\"count\")\n",
    "\n",
    "    # Step 4: Determine rows to remove (constant non-zero measurements for 6+ timesteps)\n",
    "    rows_to_remove = (group_counts >= 6) & (df[\"pv_measurement\"] != 0)\n",
    "\n",
    "    # Step 5: Remove specified rows and the temporary grouping column\n",
    "    df_filtered = df[~rows_to_remove].drop(columns=[\"temp_group\"])\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "\n",
    "def filter_zero_pv_measurements(\n",
    "    un_filtered_df: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove entries where PV measurements are zero despite significant radiation.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame containing radiation and PV measurement data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Filtered DataFrame.\n",
    "    \"\"\"\n",
    "    # Trail and error on total_radiation_threshold, tried 0.5, 5 and 30\n",
    "    total_radiation_threshold = 30\n",
    "    is_significant_radiation = (un_filtered_df[\"diffuse_rad:W\"] + un_filtered_df[\"direct_rad:W\"]) >= total_radiation_threshold\n",
    "    is_zero_pv_measurement = un_filtered_df[\"pv_measurement\"] == 0\n",
    "    filtered_df = un_filtered_df[~(is_significant_radiation & is_zero_pv_measurement)]\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def feature_engineer(data_frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    data_frame = create_time_features_from_date(data_frame)\n",
    "    data_frame[\"solar_radiation_interaction\"] = data_frame[\"diffuse_rad:W\"] * data_frame[\"direct_rad:W\"]\n",
    "\n",
    "    data_frame[\"effective_solar_elevation\"] = np.where(\n",
    "        data_frame[\"sun_elevation:d\"] <= 0,\n",
    "        0,\n",
    "        np.sin(np.radians(data_frame[\"sun_elevation:d\"])),\n",
    "    )\n",
    "    data_frame = data_frame.drop(\"sun_elevation:d\", axis=1)\n",
    "\n",
    "    data_frame[\"effective_radiation\"] = np.where(\n",
    "        data_frame[\"clear_sky_energy_1h:J\"] == 0,\n",
    "        0,  # or your specified value\n",
    "        data_frame[\"direct_rad_1h:J\"] / data_frame[\"clear_sky_energy_1h:J\"],\n",
    "    )\n",
    "\n",
    "    data_frame[\"net_clear_sky_residual\"] = (\n",
    "        data_frame[\"clear_sky_rad:W\"]\n",
    "        - data_frame[\"direct_rad:W\"]\n",
    "        - data_frame[\"diffuse_rad:W\"]\n",
    "    )\n",
    "\n",
    "    data_frame[\"cloud_ratio\"] = np.where(\n",
    "        data_frame[\"total_cloud_cover:p\"] == 0,\n",
    "        0,  # or your specified value\n",
    "        data_frame[\"effective_cloud_cover:p\"] / data_frame[\"total_cloud_cover:p\"],\n",
    "    )\n",
    "\n",
    "    data_frame[\"low_cloud_diffuse_rad\"] = data_frame[\n",
    "        \"diffuse_rad:W\"\n",
    "    ].where(data_frame[\"effective_cloud_cover:p\"] < 0.3, 0)\n",
    "\n",
    "    data_frame[\"cloud_cover_over_30%\"] = np.where(\n",
    "        data_frame[\"effective_cloud_cover:p\"] > 30, 1, 0\n",
    "    )\n",
    "\n",
    "    data_frame[\"global_horizontal_irradiation\"] = (\n",
    "        data_frame[\"diffuse_rad:W\"] + data_frame[\"direct_rad:W\"]\n",
    "    )\n",
    "\n",
    "    data_frame[\"direct_rad_cloud_adjustment\"] = data_frame[\"direct_rad:W\"] * (\n",
    "        100 - data_frame[\"effective_cloud_cover:p\"]\n",
    "    )\n",
    "\n",
    "    data_frame[\"effective_solar_elevation_squared\"] = (\n",
    "        data_frame[\"effective_solar_elevation\"] ** 0.5\n",
    "    )\n",
    "    \n",
    "    snow_columns = [\n",
    "        \"snow_depth:cm\",\n",
    "        \"fresh_snow_12h:cm\",\n",
    "        \"fresh_snow_1h:cm\",\n",
    "        \"fresh_snow_24h:cm\",\n",
    "        \"fresh_snow_3h:cm\",\n",
    "        \"fresh_snow_6h:cm\",\n",
    "    ]\n",
    "\n",
    "    data_frame[\"is_freezing\"] = (data_frame[\"t_1000hPa:K\"] < 273).astype(int)\n",
    "\n",
    "    data_frame[\"is_snow\"] = (data_frame[snow_columns] > 0).any(axis=1).astype(int)\n",
    "    data_frame[\"is_rain\"] = (data_frame[\"precip_5min:mm\"] > 0).astype(int)\n",
    "\n",
    "    data_frame = data_frame.drop(\"snow_drift:idx\", axis=1)\n",
    "\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "def create_time_features_from_date(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a new data frame with new features from date_forecast column.\n",
    "    This will create temporal features from date_forecast that are easier to learn by the model.\n",
    "    It creates the following features: month, season, year, day_of_year, day_segment.\n",
    "    All of the new features are int type.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Data frame with date_forecast column.\n",
    "    Returns:\n",
    "        pd.DataFrame: Data frame copy with new features.\n",
    "\n",
    "    \"\"\"\n",
    "    df[\"sin_day_of_year\"] = df[\"date_forecast\"].apply(get_sin_day)\n",
    "    df[\"cos_day_of_year\"] = df[\"date_forecast\"].apply(get_cos_day)\n",
    "    df[\"sin_hour\"] = df[\"date_forecast\"].apply(get_sin_hour)\n",
    "    df[\"cos_hour\"] = df[\"date_forecast\"].apply(get_cos_hour)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_sin_hour(date: datetime) -> float:\n",
    "    HOURS_OF_DAY = 24\n",
    "    return math.sin(2 * math.pi * (date.hour) / HOURS_OF_DAY)\n",
    "\n",
    "\n",
    "def get_cos_hour(date: datetime) -> float:\n",
    "    HOURS_OF_DAY = 24\n",
    "    return math.cos(2 * math.pi * (date.hour) / HOURS_OF_DAY)\n",
    "\n",
    "\n",
    "def get_sin_day(date: datetime) -> float:\n",
    "    DAY_OF_YEAR = 365.25  # Add 0.25 to account for leap years\n",
    "    return math.sin(2 * math.pi * (date.timetuple().tm_yday - 1) / DAY_OF_YEAR)\n",
    "\n",
    "\n",
    "def get_cos_day(date: datetime) -> float:\n",
    "    DAY_OF_YEAR = 365.25  # Add 0.25 to account for leap years\n",
    "    return math.cos(2 * math.pi * (date.timetuple().tm_yday - 1) / DAY_OF_YEAR)\n",
    "\n",
    "\n",
    "def add_location(data_frame: pd.DataFrame, location: str):\n",
    "    if location.lower() == \"a\":\n",
    "        data_frame[\"location_a\"] = 1\n",
    "    else:\n",
    "        data_frame[\"location_a\"] = 0\n",
    "\n",
    "    if location.lower() == \"b\":\n",
    "        data_frame[\"location_b\"] = 1\n",
    "    else:\n",
    "        data_frame[\"location_b\"] = 0\n",
    "\n",
    "    if location.lower() == \"c\":\n",
    "        data_frame[\"location_c\"] = 1\n",
    "    else:\n",
    "        data_frame[\"location_c\"] = 0\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "# Define a function to align the temporal resolution of the datasets\n",
    "def temporal_alignment(\n",
    "    train: pd.DataFrame, observed: pd.DataFrame, estimated: pd.DataFrame\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Aligns the temporal resolution of the datasets by aggregating the 15-min interval weather data to hourly intervals.\n",
    "\n",
    "    Args:\n",
    "        train (pd.DataFrame): The training targets DataFrame.\n",
    "        observed (pd.DataFrame): The observed training features DataFrame.\n",
    "        estimated (pd.DataFrame): The estimated training features DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        train_observed (pd.DataFrame): The aligned training DataFrame with observed features.\n",
    "        train_estimated (pd.DataFrame): The aligned training DataFrame with estimated features.\n",
    "    \"\"\"\n",
    "    # Convert the time columns to datetime objects\n",
    "    train[\"time\"] = pd.to_datetime(train[\"time\"])\n",
    "    observed[\"date_forecast\"] = pd.to_datetime(observed[\"date_forecast\"])\n",
    "    estimated[\"date_forecast\"] = pd.to_datetime(estimated[\"date_forecast\"])\n",
    "\n",
    "    # Set the date_forecast column as index for resampling\n",
    "    observed.set_index(\"date_forecast\", inplace=True)\n",
    "    estimated.set_index(\"date_forecast\", inplace=True)\n",
    "\n",
    "    # Resample the weather data to hourly intervals and aggregate the values by mean\n",
    "    observed_resampled = observed.resample(\"1H\").mean()\n",
    "    estimated_resampled = estimated.resample(\"1H\").mean()\n",
    "\n",
    "    # Reset the index after resampling\n",
    "    observed_resampled.reset_index(inplace=True)\n",
    "    estimated_resampled.reset_index(inplace=True)\n",
    "\n",
    "    # Merge the aggregated weather data with the solar production data based on the timestamp\n",
    "    train_observed = pd.merge(\n",
    "        train, observed_resampled, how=\"left\", left_on=\"time\", right_on=\"date_forecast\"\n",
    "    )\n",
    "    train_estimated = pd.merge(\n",
    "        train, estimated_resampled, how=\"left\", left_on=\"time\", right_on=\"date_forecast\"\n",
    "    )\n",
    "\n",
    "    return train_observed, train_estimated\n",
    "\n",
    "\n",
    "def temporal_alignment_tests(test: pd.DataFrame) -> Tuple[pd.DataFrame]:\n",
    "    return aggregate_rows(test)\n",
    "\n",
    "\n",
    "def aggregate_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Create a 'group' column to group every 4 rows together\n",
    "    df[\"group\"] = df.index // 4\n",
    "\n",
    "    # Define the aggregation functions\n",
    "    aggregation = {col: \"mean\" for col in df.columns if col != \"date_forecast\"}\n",
    "    aggregation[\"date_forecast\"] = \"first\"\n",
    "\n",
    "    # Group by the 'group' column and aggregate\n",
    "    df_agg = df.groupby(\"group\").agg(aggregation).reset_index(drop=True)\n",
    "\n",
    "    # Drop the 'group' column from the original dataframe\n",
    "    df_agg.drop(\"group\", axis=1, inplace=True)\n",
    "\n",
    "    return df_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_preprocessed_data(drop_features: bool = True) -> (\n",
    "    Tuple[\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "    ]\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch the preprocessed data for training and validation.\n",
    "\n",
    "    Returns:\n",
    "        X_train_obs_combined: The observed data for training\n",
    "        X_val_obs_combined: The observed data for validation\n",
    "        y_train_obs_combined: The observed labels for training\n",
    "        y_val_obs_combined: The observed labels for validation\n",
    "        X_train_est_combined: The estimated data for training\n",
    "        X_val_est_combined: The estimated data for validation\n",
    "        y_train_est_combined: The estimated labels for training\n",
    "        y_val_est_combined: The estimated labels for validation\n",
    "    \"\"\"\n",
    "    (\n",
    "        train_a,\n",
    "        train_b,\n",
    "        train_c,\n",
    "        X_train_estimated_a,\n",
    "        X_train_estimated_b,\n",
    "        X_train_estimated_c,\n",
    "        X_train_observed_a,\n",
    "        X_train_observed_b,\n",
    "        X_train_observed_c,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "    ) = get_raw_data()\n",
    "\n",
    "    # Temporally align the data from all three locations to the same time.\n",
    "    train_observed_a, train_estimated_a = temporal_alignment(\n",
    "        train_a, X_train_observed_a, X_train_estimated_a\n",
    "    )\n",
    "    train_observed_b, train_estimated_b = temporal_alignment(\n",
    "        train_b, X_train_observed_b, X_train_estimated_b\n",
    "    )\n",
    "    train_observed_c, train_estimated_c = temporal_alignment(\n",
    "        train_c, X_train_observed_c, X_train_estimated_c\n",
    "    )\n",
    "\n",
    "    # Add location data\n",
    "    train_observed_a = add_location(train_observed_a, \"a\")\n",
    "    train_estimated_a = add_location(train_estimated_a, \"a\")\n",
    "\n",
    "    train_observed_b = add_location(train_observed_b, \"b\")\n",
    "    train_estimated_b = add_location(train_estimated_b, \"b\")\n",
    "\n",
    "    train_observed_c = add_location(train_observed_c, \"c\")\n",
    "    train_estimated_c = add_location(train_estimated_c, \"c\")\n",
    "\n",
    "    # Combine the temporally aligned datasets from all three locations\n",
    "    train_observed_combined = pd.concat(\n",
    "        [train_observed_a, train_observed_b, train_observed_c], ignore_index=True\n",
    "    )\n",
    "    train_estimated_combined = pd.concat(\n",
    "        [train_estimated_a, train_estimated_b, train_estimated_c], ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Prepare the combined dataset by handling missing values and splitting the data\n",
    "    (\n",
    "        X_train_obs_combined,\n",
    "        X_val_obs_combined,\n",
    "        y_train_obs_combined,\n",
    "        y_val_obs_combined,\n",
    "        X_train_est_combined,\n",
    "        X_val_est_combined,\n",
    "        y_train_est_combined,\n",
    "        y_val_est_combined,\n",
    "    ) = prepare_data(train_observed_combined, train_estimated_combined, drop_features=drop_features)\n",
    "\n",
    "    return (\n",
    "        X_train_obs_combined,\n",
    "        X_val_obs_combined,\n",
    "        y_train_obs_combined,\n",
    "        y_val_obs_combined,\n",
    "        X_train_est_combined,\n",
    "        X_val_est_combined,\n",
    "        y_train_est_combined,\n",
    "        y_val_est_combined,\n",
    "    )\n",
    "\n",
    "def get_preprocessed_test_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the preprocessed test data without the 'date_forecast' column.\n",
    "    \"\"\"\n",
    "    (\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        X_test_estimated_a,\n",
    "        X_test_estimated_b,\n",
    "        X_test_estimated_c,\n",
    "    ) = get_raw_data()\n",
    "\n",
    "    # Align the test data to the same time as the training data\n",
    "    X_test_estimated_a = temporal_alignment_tests(X_test_estimated_a)\n",
    "    X_test_estimated_b = temporal_alignment_tests(X_test_estimated_b)\n",
    "    X_test_estimated_c = temporal_alignment_tests(X_test_estimated_c)\n",
    "\n",
    "    X_test_estimated_a = remove_missing_features(X_test_estimated_a)\n",
    "    X_test_estimated_b = remove_missing_features(X_test_estimated_b)\n",
    "    X_test_estimated_c = remove_missing_features(X_test_estimated_c)\n",
    "\n",
    "    # Add location data\n",
    "    X_test_estimated_a = add_location(X_test_estimated_a, \"a\")\n",
    "    X_test_estimated_b = add_location(X_test_estimated_b, \"b\")\n",
    "    X_test_estimated_c = add_location(X_test_estimated_c, \"c\")\n",
    "\n",
    "    X_test_a_correct_features = feature_engineer(X_test_estimated_a)\n",
    "    X_test_b_correct_features = feature_engineer(X_test_estimated_b)\n",
    "    X_test_c_correct_features = feature_engineer(X_test_estimated_c)\n",
    "\n",
    "    # Drop the 'date_calc' and 'date_forecast' columns from the test data\n",
    "    X_test_estimated_a_processed = X_test_a_correct_features.drop(\n",
    "        columns=[\"date_calc\", \"date_forecast\"], errors='ignore'\n",
    "    )\n",
    "    X_test_estimated_b_processed = X_test_b_correct_features.drop(\n",
    "        columns=[\"date_calc\", \"date_forecast\"], errors='ignore'\n",
    "    )\n",
    "    X_test_estimated_c_processed = X_test_c_correct_features.drop(\n",
    "        columns=[\"date_calc\", \"date_forecast\"], errors='ignore'\n",
    "    )\n",
    "\n",
    "    tests = pd.concat([X_test_estimated_a_processed, X_test_estimated_b_processed, X_test_estimated_c_processed], ignore_index=True)\n",
    "\n",
    "    return tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_obs_combined, X_val_obs_combined, y_train_obs_combined, y_val_obs_combined, X_train_est_combined, X_val_est_combined, y_train_est_combined, y_val_est_combined = fetch_preprocessed_data()\n",
    "x_test_whole = get_preprocessed_test_data()\n",
    "\n",
    "X_train_obs_combined[\"estimated_flag\"] = 0\n",
    "X_val_obs_combined[\"estimated_flag\"] = 0\n",
    "X_train_est_combined[\"estimated_flag\"] = 1\n",
    "X_val_est_combined[\"estimated_flag\"] = 1\n",
    "x_test_whole[\"estimated_flag\"] = 1\n",
    "\n",
    "\n",
    "x_whole = pd.concat([X_train_obs_combined, X_val_obs_combined, X_train_est_combined, X_val_est_combined])\n",
    "y_whole = pd.concat([y_train_obs_combined, y_val_obs_combined, y_train_est_combined, y_val_est_combined])\n",
    "x_whole.reset_index(drop=True, inplace=True)\n",
    "y_whole.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole[\"pv_measurement\"] = y_whole\n",
    "df_shuffled = x_whole.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "x_whole_a = df_shuffled[df_shuffled['location_a'] == 1]\n",
    "x_whole_b = df_shuffled[df_shuffled['location_b'] == 1]\n",
    "x_whole_c = df_shuffled[df_shuffled['location_c'] == 1]\n",
    "\n",
    "y_whole_a = x_whole_a[\"pv_measurement\"]\n",
    "x_whole_a = x_whole_a.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_a', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_b', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_c', axis = 1)\n",
    "\n",
    "y_whole_b = x_whole_b[\"pv_measurement\"]\n",
    "x_whole_b = x_whole_b.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_a', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_b', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_c', axis = 1)\n",
    "\n",
    "y_whole_c = x_whole_c[\"pv_measurement\"]\n",
    "x_whole_c = x_whole_c.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_a', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_b', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_c', axis = 1)\n",
    "cat_features = [\"estimated_flag\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "RES_PATH = 'results/output/'\n",
    "\n",
    "\n",
    "def save_predictions(test: pd.DataFrame, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the 'id' and 'prediction' columns of the test DataFrame to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        test (pd.DataFrame): A 1D DataFrame containing only the predictions.\n",
    "        filename (str): The name of the file where the predictions will be saved.\n",
    "    \"\"\"\n",
    "    model = pd.DataFrame()\n",
    "    \n",
    "    model[\"prediction\"] = test\n",
    "    model['id'] = model.index\n",
    "\n",
    "    model['prediction'] = model['prediction'].apply(lambda x: max(0, x))\n",
    "    \n",
    "    # Reorder the columns to ensure 'id' comes before 'prediction'\n",
    "    model = model[['id', 'prediction']]\n",
    "    \n",
    "\n",
    "    # Save the resulting DataFrame to a CSV file\n",
    "    model.to_csv(f'{RES_PATH}{filename}.csv', index=False)\n",
    "    \n",
    "    # Display the first few rows of the saved DataFrame\n",
    "    print(model.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 615.2289223\ttotal: 251ms\tremaining: 4m 10s\n",
      "100:\tlearn: 201.4072982\ttotal: 9.25s\tremaining: 1m 22s\n",
      "200:\tlearn: 186.0497580\ttotal: 19s\tremaining: 1m 15s\n",
      "300:\tlearn: 181.1303390\ttotal: 28.3s\tremaining: 1m 5s\n",
      "400:\tlearn: 177.2485579\ttotal: 38.9s\tremaining: 58.1s\n",
      "500:\tlearn: 172.6623044\ttotal: 49.2s\tremaining: 49s\n",
      "600:\tlearn: 163.7204714\ttotal: 58.5s\tremaining: 38.8s\n",
      "700:\tlearn: 154.4498268\ttotal: 1m 6s\tremaining: 28.5s\n",
      "800:\tlearn: 146.6204772\ttotal: 1m 15s\tremaining: 18.8s\n",
      "900:\tlearn: 139.9470005\ttotal: 1m 24s\tremaining: 9.26s\n",
      "999:\tlearn: 134.5556506\ttotal: 1m 31s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x1ace6e51280>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_a = CatBoostRegressor(\n",
    "    max_depth=9,\n",
    "    cat_features=cat_features,\n",
    "    loss_function=\"MAE\",\n",
    "    verbose = 100\n",
    ")\n",
    "best_model_a.fit(x_whole_a, y_whole_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 94.2791670\ttotal: 138ms\tremaining: 2m 17s\n",
      "100:\tlearn: 25.1394341\ttotal: 8.47s\tremaining: 1m 15s\n",
      "200:\tlearn: 21.7299397\ttotal: 18.9s\tremaining: 1m 15s\n",
      "300:\tlearn: 20.2992389\ttotal: 27.2s\tremaining: 1m 3s\n",
      "400:\tlearn: 19.0627633\ttotal: 35.3s\tremaining: 52.8s\n",
      "500:\tlearn: 17.9527669\ttotal: 43.3s\tremaining: 43.1s\n",
      "600:\tlearn: 17.1682651\ttotal: 51.3s\tremaining: 34s\n",
      "700:\tlearn: 16.6932226\ttotal: 59s\tremaining: 25.1s\n",
      "800:\tlearn: 15.8845163\ttotal: 1m 6s\tremaining: 16.6s\n",
      "900:\tlearn: 15.3506132\ttotal: 1m 14s\tremaining: 8.21s\n",
      "999:\tlearn: 14.9515366\ttotal: 1m 22s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x1ace6e51190>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_b = CatBoostRegressor(\n",
    "    max_depth=9,\n",
    "    cat_features=cat_features,\n",
    "    loss_function=\"MAE\",\n",
    "    verbose = 100\n",
    ")\n",
    "best_model_b.fit(x_whole_b, y_whole_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 78.8208115\ttotal: 93.2ms\tremaining: 1m 33s\n",
      "100:\tlearn: 21.4396391\ttotal: 8.3s\tremaining: 1m 13s\n",
      "200:\tlearn: 18.5151414\ttotal: 16.4s\tremaining: 1m 5s\n",
      "300:\tlearn: 17.0987782\ttotal: 24.1s\tremaining: 56.1s\n",
      "400:\tlearn: 16.0406782\ttotal: 32.3s\tremaining: 48.2s\n",
      "500:\tlearn: 15.0533016\ttotal: 40.4s\tremaining: 40.2s\n",
      "600:\tlearn: 14.2314945\ttotal: 49s\tremaining: 32.6s\n",
      "700:\tlearn: 13.6329705\ttotal: 57.5s\tremaining: 24.5s\n",
      "800:\tlearn: 13.1507093\ttotal: 1m 7s\tremaining: 16.8s\n",
      "900:\tlearn: 12.7614489\ttotal: 1m 16s\tremaining: 8.39s\n",
      "999:\tlearn: 12.4840959\ttotal: 1m 24s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x1ace6e51100>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_c = CatBoostRegressor(\n",
    "    max_depth=9,\n",
    "    cat_features=cat_features,\n",
    "    loss_function = \"MAE\",\n",
    "    verbose = 100\n",
    ")\n",
    "best_model_c.fit(x_whole_c, y_whole_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "find_time_sin = lambda hour: math.sin(2 * math.pi * (hour) / 24)\n",
    "find_time_cos = lambda hour: math.cos(2 * math.pi * (hour) / 24)\n",
    "\n",
    "def postprocess_data(x_test: pd.DataFrame, y_pred: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Postprocess the data to set the predicted values to 0 at the correct times.\"\"\"\n",
    "    \n",
    "    # Cap the min and max values for each location for each hour\n",
    "    y_pred = cap_min_max_values(x_test, y_pred)\n",
    "\n",
    "    # Set the predicted values to 0 at the correct times, the hours 22, 23 and 0 are set to zero as the PV measurements almost always are 0 at these times\n",
    "    y_pred = set_0_pv_at_times(x_test, y_pred, \"a\", [22, 23, 0])\n",
    "    y_pred = set_0_pv_at_times(x_test, y_pred, \"b\", [22, 23, 0])\n",
    "    y_pred = set_0_pv_at_times(x_test, y_pred, \"c\", [22, 23, 0])\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def cap_min_max_values(x_test: pd.DataFrame, y_pred: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Cap the min and max values for each location for each hour.\"\"\"\n",
    "    for hour in range(24):\n",
    "        # Get the min and max values for each location for each hour\n",
    "        min_value_a, max_value_a = get_min_max_values_for_location_at_hour(\"a\", hour)\n",
    "        min_value_b, max_value_b = get_min_max_values_for_location_at_hour(\"b\", hour)\n",
    "        min_value_c, max_value_c = get_min_max_values_for_location_at_hour(\"c\", hour)\n",
    "        print(f\"hour: {hour}, min_value_a: {min_value_a}, max_value_a: {max_value_a}, min_value_b: {min_value_b}, max_value_b: {max_value_b}, min_value_c: {min_value_c}, max_value_c: {max_value_c}\")\n",
    "        # Cap the values between min_value and max_value\n",
    "        y_pred = cap_min_max_values_for_hour(x_test, y_pred, \"a\", hour, min_value_a, max_value_a)\n",
    "        y_pred = cap_min_max_values_for_hour(x_test, y_pred, \"b\", hour, min_value_b, max_value_b)\n",
    "        y_pred = cap_min_max_values_for_hour(x_test, y_pred, \"c\", hour, min_value_c, max_value_c)\n",
    "    return y_pred\n",
    "\n",
    "X_train_obs_combined, X_val_obs_combined, y_train_obs_combined, y_val_obs_combined, X_train_est_combined, X_val_est_combined, y_train_est_combined, y_val_est_combined = fetch_preprocessed_data(drop_features=False)\n",
    "x_whole_with_time = pd.concat([X_train_obs_combined, X_val_obs_combined, X_train_est_combined, X_val_est_combined])\n",
    "\n",
    "def get_min_max_values_for_location_at_hour(location: str, hour: int) -> tuple[float, float]:\n",
    "    \"\"\"Get the min and max values for a specific location at a specific hour.\"\"\"\n",
    "    # Get the x and y for the given hour and location\n",
    "    hour_sin = find_time_sin(hour)\n",
    "    hour_cos = find_time_cos(hour)\n",
    "    # find the min and max values for the given hour and location\n",
    "    min_value = x_whole_with_time[(x_whole_with_time[\"location_\" + location] == 1) & (x_whole_with_time[\"sin_hour\"] == hour_sin) & (x_whole_with_time[\"cos_hour\"] == hour_cos)][\"pv_measurement\"].min()\n",
    "    max_value = x_whole_with_time[(x_whole_with_time[\"location_\" + location] == 1) & (x_whole_with_time[\"sin_hour\"] == hour_sin) & (x_whole_with_time[\"cos_hour\"] == hour_cos)][\"pv_measurement\"].max()\n",
    "    \n",
    "    return (min_value, max_value)\n",
    "\n",
    "def cap_min_max_values_for_hour(x_test: pd.DataFrame, y_pred: pd.DataFrame, location: str, hour: int, min_value: float, max_value: float) -> pd.DataFrame:\n",
    "    \"\"\"Cap the min and max values for a specific hour.\"\"\"\n",
    "    \n",
    "    # Calculate sin and cos values for the given hour\n",
    "    hour_sin = find_time_sin(hour)\n",
    "    hour_cos = find_time_cos(hour)\n",
    "    \n",
    "    # Find indices corresponding to the given hour at the given location\n",
    "    indices = x_test[(x_test[\"location_\" + location] == 1) & (x_test[\"sin_hour\"] == hour_sin) & (x_test[\"cos_hour\"] == hour_cos)].index\n",
    "    \n",
    "    # Cap the values between min_value and max_value\n",
    "    y_pred.loc[indices] = y_pred.loc[indices].clip(min_value, max_value)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def set_0_pv_at_times(x_test: pd.DataFrame, y_pred: pd.DataFrame, location: str, hours: list[int]) -> pd.DataFrame:\n",
    "    \"\"\"Find the correct predicted values at the given times and locaiton and set them to 0.\"\"\"\n",
    "    hours_to_set_0_sin = [find_time_sin(hour) for hour in hours]\n",
    "    hours_to_set_0_cos = [find_time_cos(hour) for hour in hours]\n",
    "\n",
    "\n",
    "    indices = x_test[(x_test[\"location_\" + location] == 1) & (x_test[\"sin_hour\"].isin(hours_to_set_0_sin) & (x_test[\"cos_hour\"].isin(hours_to_set_0_cos)))].index\n",
    "    for index in indices:\n",
    "        y_pred.loc[index] = 0\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole_a = x_test_whole[x_test_whole['location_a'] == 1]\n",
    "x_whole_b = x_test_whole[x_test_whole['location_b'] == 1]\n",
    "x_whole_c = x_test_whole[x_test_whole['location_c'] == 1]\n",
    "\n",
    "x_whole_a = x_whole_a.drop('location_a', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_b', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_c', axis = 1)\n",
    "\n",
    "x_whole_b = x_whole_b.drop('location_a', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_b', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_c', axis = 1)\n",
    "\n",
    "x_whole_c = x_whole_c.drop('location_a', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_b', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_c', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hour: 0, min_value_a: 0.0, max_value_a: 3.3, min_value_b: -0.0, max_value_b: -0.0, min_value_c: 0.0, max_value_c: 0.0\n",
      "hour: 1, min_value_a: 0.0, max_value_a: 53.68, min_value_b: -0.0, max_value_b: 12.075, min_value_c: 0.0, max_value_c: 9.8\n",
      "hour: 2, min_value_a: 0.0, max_value_a: 233.64000000000001, min_value_b: -0.0, max_value_b: 68.1375, min_value_c: 0.0, max_value_c: 39.2\n",
      "hour: 3, min_value_a: 0.0, max_value_a: 439.12, min_value_b: -0.0, max_value_b: 138.0, min_value_c: 0.0, max_value_c: 88.2\n",
      "hour: 4, min_value_a: 0.0, max_value_a: 1046.98, min_value_b: -0.0, max_value_b: 307.05, min_value_c: 0.0, max_value_c: 176.4\n",
      "hour: 5, min_value_a: 0.0, max_value_a: 2049.08, min_value_b: -0.0, max_value_b: 452.8125, min_value_c: 0.0, max_value_c: 264.6\n",
      "hour: 6, min_value_a: 0.0, max_value_a: 3244.78, min_value_b: -0.0, max_value_b: 681.375, min_value_c: 0.0, max_value_c: 499.8\n",
      "hour: 7, min_value_a: 0.0, max_value_a: 4266.46, min_value_b: -0.0, max_value_b: 865.0875, min_value_c: 0.0, max_value_c: 705.6\n",
      "hour: 8, min_value_a: 0.0, max_value_a: 5048.780000000001, min_value_b: -0.0, max_value_b: 997.9125, min_value_c: 0.0, max_value_c: 784.0\n",
      "hour: 9, min_value_a: 0.0, max_value_a: 5477.339999999999, min_value_b: -0.0, max_value_b: 1091.925, min_value_c: 0.0, max_value_c: 882.0000000000001\n",
      "hour: 10, min_value_a: 0.0, max_value_a: 5733.42, min_value_b: -0.0, max_value_b: 1146.2625, min_value_c: 0.0, max_value_c: 980.0000000000001\n",
      "hour: 11, min_value_a: 0.0, max_value_a: 5651.8, min_value_b: -0.0, max_value_b: 1120.3875, min_value_c: 0.0, max_value_c: 999.6\n",
      "hour: 12, min_value_a: 0.0, max_value_a: 5344.46, min_value_b: 0.0, max_value_b: 1152.3, min_value_c: 0.0, max_value_c: 950.6\n",
      "hour: 13, min_value_a: 0.0, max_value_a: 5058.900000000001, min_value_b: -0.0, max_value_b: 1053.9750000000001, min_value_c: 0.0, max_value_c: 911.4000000000001\n",
      "hour: 14, min_value_a: 0.0, max_value_a: 4491.3, min_value_b: -0.0, max_value_b: 823.6875, min_value_c: 0.0, max_value_c: 803.6\n",
      "hour: 15, min_value_a: 0.0, max_value_a: 3633.96, min_value_b: -0.0, max_value_b: 723.6375, min_value_c: 0.0, max_value_c: 597.8000000000001\n",
      "hour: 16, min_value_a: 0.0, max_value_a: 2710.4, min_value_b: -0.0, max_value_b: 601.1625, min_value_c: 0.0, max_value_c: 494.90000000000003\n",
      "hour: 17, min_value_a: 0.0, max_value_a: 1606.0, min_value_b: -0.0, max_value_b: 363.1125, min_value_c: 0.0, max_value_c: 303.8\n",
      "hour: 18, min_value_a: 0.0, max_value_a: 849.64, min_value_b: -0.0, max_value_b: 242.3625, min_value_c: 0.0, max_value_c: 176.4\n",
      "hour: 19, min_value_a: 0.0, max_value_a: 554.4, min_value_b: -0.0, max_value_b: 145.7625, min_value_c: 0.0, max_value_c: 107.80000000000001\n",
      "hour: 20, min_value_a: 0.0, max_value_a: 197.12, min_value_b: -0.0, max_value_b: 33.637499999999996, min_value_c: 0.0, max_value_c: 29.400000000000002\n",
      "hour: 21, min_value_a: 0.0, max_value_a: 19.139999999999997, min_value_b: -0.0, max_value_b: 13.8, min_value_c: 0.0, max_value_c: 9.8\n",
      "hour: 22, min_value_a: 0.0, max_value_a: 0.66, min_value_b: -0.0, max_value_b: -0.0, min_value_c: -0.0, max_value_c: -0.0\n",
      "hour: 23, min_value_a: 0.0, max_value_a: 0.0, min_value_b: -0.0, max_value_b: -0.0, min_value_c: -0.0, max_value_c: -0.0\n"
     ]
    }
   ],
   "source": [
    "y_predictions_a = best_model_a.predict(x_whole_a)\n",
    "y_predictions_b = best_model_b.predict(x_whole_b)\n",
    "y_predictions_c = best_model_c.predict(x_whole_c)\n",
    "y_predictions = pd.concat([pd.Series(y_predictions_a), pd.Series(y_predictions_b), pd.Series(y_predictions_c)])\n",
    "y_predictions = y_predictions.reset_index(drop=True)\n",
    "\n",
    "# Save the model\n",
    "y_predictions_catboost_1 = postprocess_data(x_test_whole, pd.DataFrame(y_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second catboost model with \"cloud_interaction\" feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole = x_whole.drop(\"effective_solar_elevation_squared\", axis=1)\n",
    "x_test_whole = x_test_whole.drop(\"effective_solar_elevation_squared\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole[\"pv_measurement\"] = y_whole\n",
    "df_shuffled = x_whole.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "x_whole_a = df_shuffled[df_shuffled['location_a'] == 1]\n",
    "x_whole_b = df_shuffled[df_shuffled['location_b'] == 1]\n",
    "x_whole_c = df_shuffled[df_shuffled['location_c'] == 1]\n",
    "\n",
    "y_whole_a = x_whole_a[\"pv_measurement\"]\n",
    "x_whole_a = x_whole_a.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_a', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_b', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_c', axis = 1)\n",
    "\n",
    "y_whole_b = x_whole_b[\"pv_measurement\"]\n",
    "x_whole_b = x_whole_b.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_a', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_b', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_c', axis = 1)\n",
    "\n",
    "y_whole_c = x_whole_c[\"pv_measurement\"]\n",
    "x_whole_c = x_whole_c.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_a', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_b', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_c', axis = 1)\n",
    "cat_features = [\"estimated_flag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 616.9363093\ttotal: 145ms\tremaining: 2m 24s\n",
      "100:\tlearn: 199.9656401\ttotal: 10.6s\tremaining: 1m 34s\n",
      "200:\tlearn: 185.3356686\ttotal: 18.3s\tremaining: 1m 12s\n",
      "300:\tlearn: 180.5070612\ttotal: 28.8s\tremaining: 1m 6s\n",
      "400:\tlearn: 176.7196537\ttotal: 42.9s\tremaining: 1m 4s\n",
      "500:\tlearn: 172.6197441\ttotal: 53.6s\tremaining: 53.3s\n",
      "600:\tlearn: 163.7581476\ttotal: 1m 3s\tremaining: 42.4s\n",
      "700:\tlearn: 153.8238135\ttotal: 1m 12s\tremaining: 31s\n",
      "800:\tlearn: 147.6203702\ttotal: 1m 20s\tremaining: 20s\n",
      "900:\tlearn: 141.6572310\ttotal: 1m 28s\tremaining: 9.73s\n",
      "999:\tlearn: 136.7908527\ttotal: 1m 36s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x1ac876f7fd0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_a = CatBoostRegressor(\n",
    "    max_depth=9,\n",
    "    cat_features=cat_features,\n",
    "    loss_function=\"MAE\",\n",
    "    verbose = 100\n",
    ")\n",
    "best_model_a.fit(x_whole_a, y_whole_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 94.1836286\ttotal: 139ms\tremaining: 2m 19s\n",
      "100:\tlearn: 25.3949714\ttotal: 8.3s\tremaining: 1m 13s\n",
      "200:\tlearn: 22.1209997\ttotal: 16.4s\tremaining: 1m 5s\n",
      "300:\tlearn: 20.7702607\ttotal: 24s\tremaining: 55.7s\n",
      "400:\tlearn: 19.4111036\ttotal: 32.5s\tremaining: 48.5s\n",
      "500:\tlearn: 18.1584056\ttotal: 39.9s\tremaining: 39.7s\n",
      "600:\tlearn: 17.4038913\ttotal: 48.2s\tremaining: 32s\n",
      "700:\tlearn: 16.7939747\ttotal: 56s\tremaining: 23.9s\n",
      "800:\tlearn: 16.1720278\ttotal: 1m 3s\tremaining: 15.8s\n",
      "900:\tlearn: 15.6136213\ttotal: 1m 10s\tremaining: 7.77s\n",
      "999:\tlearn: 15.1423810\ttotal: 1m 18s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x1ac876f7b20>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_b = CatBoostRegressor(\n",
    "    max_depth=9,\n",
    "    cat_features=cat_features,\n",
    "    loss_function = \"MAE\",\n",
    "    verbose = 100\n",
    ")\n",
    "best_model_b.fit(x_whole_b, y_whole_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 78.7273693\ttotal: 115ms\tremaining: 1m 55s\n",
      "100:\tlearn: 21.3078061\ttotal: 8.2s\tremaining: 1m 13s\n",
      "200:\tlearn: 18.2258783\ttotal: 17.8s\tremaining: 1m 10s\n",
      "300:\tlearn: 17.0883950\ttotal: 25.8s\tremaining: 60s\n",
      "400:\tlearn: 16.1611729\ttotal: 34.1s\tremaining: 51s\n",
      "500:\tlearn: 15.2436549\ttotal: 42.9s\tremaining: 42.7s\n",
      "600:\tlearn: 14.4853484\ttotal: 53.5s\tremaining: 35.5s\n",
      "700:\tlearn: 13.7033024\ttotal: 1m 3s\tremaining: 27.1s\n",
      "800:\tlearn: 13.1848967\ttotal: 1m 11s\tremaining: 17.8s\n",
      "900:\tlearn: 12.7181220\ttotal: 1m 21s\tremaining: 8.97s\n",
      "999:\tlearn: 12.3918690\ttotal: 1m 30s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x1ac876f79d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_c = CatBoostRegressor(\n",
    "    max_depth=9,\n",
    "    cat_features=cat_features,\n",
    "    loss_function = \"MAE\",\n",
    "    verbose = 100\n",
    ")\n",
    "best_model_c.fit(x_whole_c, y_whole_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole_a = x_test_whole[x_test_whole['location_a'] == 1]\n",
    "x_whole_b = x_test_whole[x_test_whole['location_b'] == 1]\n",
    "x_whole_c = x_test_whole[x_test_whole['location_c'] == 1]\n",
    "\n",
    "x_whole_a = x_whole_a.drop('location_a', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_b', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_c', axis = 1)\n",
    "\n",
    "x_whole_b = x_whole_b.drop('location_a', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_b', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_c', axis = 1)\n",
    "\n",
    "x_whole_c = x_whole_c.drop('location_a', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_b', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_c', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hour: 0, min_value_a: 0.0, max_value_a: 3.3, min_value_b: -0.0, max_value_b: -0.0, min_value_c: 0.0, max_value_c: 0.0\n",
      "hour: 1, min_value_a: 0.0, max_value_a: 53.68, min_value_b: -0.0, max_value_b: 12.075, min_value_c: 0.0, max_value_c: 9.8\n",
      "hour: 2, min_value_a: 0.0, max_value_a: 233.64000000000001, min_value_b: -0.0, max_value_b: 68.1375, min_value_c: 0.0, max_value_c: 39.2\n",
      "hour: 3, min_value_a: 0.0, max_value_a: 439.12, min_value_b: -0.0, max_value_b: 138.0, min_value_c: 0.0, max_value_c: 88.2\n",
      "hour: 4, min_value_a: 0.0, max_value_a: 1046.98, min_value_b: -0.0, max_value_b: 307.05, min_value_c: 0.0, max_value_c: 176.4\n",
      "hour: 5, min_value_a: 0.0, max_value_a: 2049.08, min_value_b: -0.0, max_value_b: 452.8125, min_value_c: 0.0, max_value_c: 264.6\n",
      "hour: 6, min_value_a: 0.0, max_value_a: 3244.78, min_value_b: -0.0, max_value_b: 681.375, min_value_c: 0.0, max_value_c: 499.8\n",
      "hour: 7, min_value_a: 0.0, max_value_a: 4266.46, min_value_b: -0.0, max_value_b: 865.0875, min_value_c: 0.0, max_value_c: 705.6\n",
      "hour: 8, min_value_a: 0.0, max_value_a: 5048.780000000001, min_value_b: -0.0, max_value_b: 997.9125, min_value_c: 0.0, max_value_c: 784.0\n",
      "hour: 9, min_value_a: 0.0, max_value_a: 5477.339999999999, min_value_b: -0.0, max_value_b: 1091.925, min_value_c: 0.0, max_value_c: 882.0000000000001\n",
      "hour: 10, min_value_a: 0.0, max_value_a: 5733.42, min_value_b: -0.0, max_value_b: 1146.2625, min_value_c: 0.0, max_value_c: 980.0000000000001\n",
      "hour: 11, min_value_a: 0.0, max_value_a: 5651.8, min_value_b: -0.0, max_value_b: 1120.3875, min_value_c: 0.0, max_value_c: 999.6\n",
      "hour: 12, min_value_a: 0.0, max_value_a: 5344.46, min_value_b: 0.0, max_value_b: 1152.3, min_value_c: 0.0, max_value_c: 950.6\n",
      "hour: 13, min_value_a: 0.0, max_value_a: 5058.900000000001, min_value_b: -0.0, max_value_b: 1053.9750000000001, min_value_c: 0.0, max_value_c: 911.4000000000001\n",
      "hour: 14, min_value_a: 0.0, max_value_a: 4491.3, min_value_b: -0.0, max_value_b: 823.6875, min_value_c: 0.0, max_value_c: 803.6\n",
      "hour: 15, min_value_a: 0.0, max_value_a: 3633.96, min_value_b: -0.0, max_value_b: 723.6375, min_value_c: 0.0, max_value_c: 597.8000000000001\n",
      "hour: 16, min_value_a: 0.0, max_value_a: 2710.4, min_value_b: -0.0, max_value_b: 601.1625, min_value_c: 0.0, max_value_c: 494.90000000000003\n",
      "hour: 17, min_value_a: 0.0, max_value_a: 1606.0, min_value_b: -0.0, max_value_b: 363.1125, min_value_c: 0.0, max_value_c: 303.8\n",
      "hour: 18, min_value_a: 0.0, max_value_a: 849.64, min_value_b: -0.0, max_value_b: 242.3625, min_value_c: 0.0, max_value_c: 176.4\n",
      "hour: 19, min_value_a: 0.0, max_value_a: 554.4, min_value_b: -0.0, max_value_b: 145.7625, min_value_c: 0.0, max_value_c: 107.80000000000001\n",
      "hour: 20, min_value_a: 0.0, max_value_a: 197.12, min_value_b: -0.0, max_value_b: 33.637499999999996, min_value_c: 0.0, max_value_c: 29.400000000000002\n",
      "hour: 21, min_value_a: 0.0, max_value_a: 19.139999999999997, min_value_b: -0.0, max_value_b: 13.8, min_value_c: 0.0, max_value_c: 9.8\n",
      "hour: 22, min_value_a: 0.0, max_value_a: 0.66, min_value_b: -0.0, max_value_b: -0.0, min_value_c: -0.0, max_value_c: -0.0\n",
      "hour: 23, min_value_a: 0.0, max_value_a: 0.0, min_value_b: -0.0, max_value_b: -0.0, min_value_c: -0.0, max_value_c: -0.0\n"
     ]
    }
   ],
   "source": [
    "y_predictions_a = best_model_a.predict(x_whole_a)\n",
    "y_predictions_b = best_model_b.predict(x_whole_b)\n",
    "y_predictions_c = best_model_c.predict(x_whole_c)\n",
    "y_predictions = pd.concat([pd.Series(y_predictions_a), pd.Series(y_predictions_b), pd.Series(y_predictions_c)])\n",
    "y_predictions = y_predictions.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Save the model\n",
    "y_predictions_catboost_2 = postprocess_data(x_test_whole, pd.DataFrame(y_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id    prediction\n",
      "0   0  0.000000e+00\n",
      "1   1  2.646294e-07\n",
      "2   2  2.654392e-07\n",
      "3   3  5.696562e+01\n",
      "4   4  3.288108e+02\n"
     ]
    }
   ],
   "source": [
    "average_prediction = (y_predictions_catboost_1 + y_predictions_catboost_2) / 2\n",
    "save_predictions(average_prediction, \"short_notebook_1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
