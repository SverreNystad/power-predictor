{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import KFold\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.metrics import accuracy_score as acs_score\n",
    "\n",
    "from src.data.data_fetcher import get_raw_data\n",
    "from src.features.preprocess_data import get_preprocessed_test_data, fetch_preprocessed_data\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c = get_raw_data()\n",
    "\n",
    "X_train_obs_combined, X_val_obs_combined, y_train_obs_combined, y_val_obs_combined, X_train_est_combined, X_val_est_combined, y_train_est_combined, y_val_est_combined = fetch_preprocessed_data()\n",
    "x_test_whole = get_preprocessed_test_data()\n",
    "\n",
    "x_whole = pd.concat([X_train_obs_combined, X_val_obs_combined])\n",
    "y_whole = pd.concat([y_train_obs_combined, y_val_obs_combined])\n",
    "x_whole.reset_index(drop=True, inplace=True)\n",
    "y_whole.reset_index(drop=True, inplace=True)\n",
    "\n",
    "x_whole_obs = pd.concat([X_train_obs_combined, X_val_obs_combined])\n",
    "y_whole_obs = pd.concat([y_train_obs_combined, y_val_obs_combined])\n",
    "\n",
    "x_whole_est = pd.concat([X_train_est_combined, X_val_est_combined])\n",
    "y_whole_est = pd.concat([y_train_est_combined, y_val_est_combined])\n",
    "\n",
    "x_whole.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole[\"pv_measurement\"] = y_whole\n",
    "df_shuffled = x_whole.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "x_whole_a = df_shuffled[df_shuffled['location_a'] == 1]\n",
    "x_whole_b = df_shuffled[df_shuffled['location_b'] == 1]\n",
    "x_whole_c = df_shuffled[df_shuffled['location_c'] == 1]\n",
    "\n",
    "y_whole_a = x_whole_a[\"pv_measurement\"]\n",
    "x_whole_a = x_whole_a.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_a', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_b', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_c', axis = 1)\n",
    "\n",
    "y_whole_b = x_whole_b[\"pv_measurement\"]\n",
    "x_whole_b = x_whole_b.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_a', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_b', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_c', axis = 1)\n",
    "\n",
    "y_whole_c = x_whole_c[\"pv_measurement\"]\n",
    "x_whole_c = x_whole_c.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_a', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_b', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_c', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 1, 10),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'iterations': trial.suggest_int('iterations', 100, 2000),  # Optimizing the number of iterations\n",
    "        'eval_metric': 'MAE',\n",
    "        'random_seed': 42,\n",
    "        'verbose': 200,\n",
    "        'loss_function': 'MAE',\n",
    "        # Add more parameters here if you want\n",
    "    }\n",
    "    \n",
    "    fold_mae = []\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_index, test_index in kf.split(y_whole_a):\n",
    "        X_train_fold, X_val_fold = x_whole_a.iloc[train_index], x_whole_a.iloc[test_index]\n",
    "        y_train_fold, y_val_fold = y_whole_a.iloc[train_index], y_whole_a.iloc[test_index]\n",
    "        \n",
    "        model = CatBoostRegressor(**param)\n",
    "        model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], early_stopping_rounds=100)\n",
    "        \n",
    "        y_pred_fold = model.predict(X_val_fold)\n",
    "        fold_mae.append(mean_absolute_error(y_val_fold, y_pred_fold))\n",
    "    \n",
    "    return np.mean(fold_mae)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)  # You can increase n_trials to try more combinations\n",
    "\n",
    "# Best hyperparameters\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)\n",
    "\n",
    "# Retrain with best parameters\n",
    "best_params = study.best_trial.params\n",
    "best_model_a = CatBoostRegressor(**best_params)\n",
    "best_model_a.fit(x_whole_a, y_whole_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 1, 10),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'iterations': trial.suggest_int('iterations', 100, 2000),  # Optimizing the number of iterations\n",
    "        'eval_metric': 'MAE',\n",
    "        'random_seed': 42,\n",
    "        'verbose': 200,\n",
    "        'loss_function': 'MAE',\n",
    "        # Add more parameters here if you want\n",
    "    }\n",
    "    \n",
    "    fold_mae = []\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_index, test_index in kf.split(x_whole_b):\n",
    "        X_train_fold, X_val_fold = x_whole_b.iloc[train_index], x_whole_b.iloc[test_index]\n",
    "        y_train_fold, y_val_fold = y_whole_b.iloc[train_index], y_whole_b.iloc[test_index]\n",
    "        \n",
    "        model = CatBoostRegressor(**param)\n",
    "        model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], early_stopping_rounds=100)\n",
    "        \n",
    "        y_pred_fold = model.predict(X_val_fold)\n",
    "        fold_mae.append(mean_absolute_error(y_val_fold, y_pred_fold))\n",
    "    \n",
    "    return np.mean(fold_mae)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)  # You can increase n_trials to try more combinations\n",
    "\n",
    "# Best hyperparameters\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)\n",
    "\n",
    "# Retrain with best parameters\n",
    "best_params = study.best_trial.params\n",
    "best_model_b = CatBoostRegressor(**best_params)\n",
    "best_model_b.fit(x_whole_b, y_whole_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 1, 10),\n",
    "        'depth': trial.suggest_int('depth', 4, 10),\n",
    "        'iterations': trial.suggest_int('iterations', 100, 2000),  # Optimizing the number of iterations\n",
    "        'eval_metric': 'MAE',\n",
    "        'random_seed': 42,\n",
    "        'verbose': 200,\n",
    "        'loss_function': 'MAE',\n",
    "        # Add more parameters here if you want\n",
    "    }\n",
    "    \n",
    "    fold_mae = []\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_index, test_index in kf.split(x_whole_c):\n",
    "        X_train_fold, X_val_fold = x_whole_c.iloc[train_index], x_whole_c.iloc[test_index]\n",
    "        y_train_fold, y_val_fold = y_whole_c.iloc[train_index], y_whole_c.iloc[test_index]\n",
    "        \n",
    "        model = CatBoostRegressor(**param)\n",
    "        model.fit(X_train_fold, y_train_fold, eval_set=[(X_val_fold, y_val_fold)], early_stopping_rounds=100)\n",
    "        \n",
    "        y_pred_fold = model.predict(X_val_fold)\n",
    "        fold_mae.append(mean_absolute_error(y_val_fold, y_pred_fold))\n",
    "    \n",
    "    return np.mean(fold_mae)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)  # You can increase n_trials to try more combinations\n",
    "\n",
    "# Best hyperparameters\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)\n",
    "\n",
    "# Retrain with best parameters\n",
    "best_params = study.best_trial.params\n",
    "best_model_c = CatBoostRegressor(**best_params)\n",
    "best_model_c.fit(x_whole_c, y_whole_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole_a = x_test_whole[x_test_whole['location_a'] == 1]\n",
    "x_whole_b = x_test_whole[x_test_whole['location_b'] == 1]\n",
    "x_whole_c = x_test_whole[x_test_whole['location_c'] == 1]\n",
    "\n",
    "x_whole_a = x_whole_a.drop('location_a', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_b', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_c', axis = 1)\n",
    "\n",
    "x_whole_b = x_whole_b.drop('location_a', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_b', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_c', axis = 1)\n",
    "\n",
    "x_whole_c = x_whole_c.drop('location_a', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_b', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_c', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_predict(x_values :pd.DataFrame, models) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function for predicting on multiple models and averaging the results\n",
    "    \"\"\"\n",
    "    results = models[0].predict(x_values)\n",
    "    for model in models[1:]:\n",
    "        model: xgb.XGBRegressor\n",
    "        prediction = model.predict(x_values)\n",
    "        results += prediction\n",
    "    \n",
    "    results = results / len(models)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_b = best_model_a.predict(x_whole_a)\n",
    "y_pred_c = best_model_b.predict(x_whole_b)\n",
    "y_pred_a = best_model_c.predict(x_whole_c)\n",
    "y_pred = pd.concat([y_pred_a, y_pred_b, y_pred_c])\n",
    "y_pred = y_pred.reset_index(drop=True)\n",
    "# Save the model\n",
    "from src.models.saving import save_predictions\n",
    "from src.features.postprocess_data import postprocess_data\n",
    "\n",
    "y_pred = postprocess_data(x_test_whole, pd.DataFrame(y_pred))\n",
    "save_predictions(y_pred, 'catboost optuna 3 modeller 1')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
