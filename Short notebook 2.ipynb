{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost stack and autogluon stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autogluon in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (0.8.2)\n",
      "Requirement already satisfied: autogluon.core[all]==0.8.2 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon) (0.8.2)\n",
      "Requirement already satisfied: autogluon.features==0.8.2 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon) (0.8.2)\n",
      "Requirement already satisfied: autogluon.tabular[all]==0.8.2 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon) (0.8.2)\n",
      "Requirement already satisfied: autogluon.multimodal==0.8.2 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon) (0.8.2)\n",
      "Requirement already satisfied: autogluon.timeseries[all]==0.8.2 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon) (0.8.2)\n",
      "Requirement already satisfied: numpy<1.27,>=1.21 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (1.23.5)\n",
      "Requirement already satisfied: scipy<1.12,>=1.5.4 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (1.9.3)\n",
      "Requirement already satisfied: scikit-learn<1.3,>=1.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (1.2.2)\n",
      "Requirement already satisfied: networkx<4,>=3.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (3.1)\n",
      "Collecting pandas<1.6,>=1.4.1 (from autogluon.core[all]==0.8.2->autogluon)\n",
      "  Using cached pandas-1.5.3-cp39-cp39-win_amd64.whl (10.9 MB)\n",
      "Requirement already satisfied: tqdm<5,>=4.38 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (4.65.2)\n",
      "Requirement already satisfied: requests in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (2.28.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (3.8.0)\n",
      "Requirement already satisfied: boto3<2,>=1.10 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (1.28.56)\n",
      "Requirement already satisfied: autogluon.common==0.8.2 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (0.8.2)\n",
      "Requirement already satisfied: hyperopt<0.2.8,>=0.2.7 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (0.2.7)\n",
      "Requirement already satisfied: ray[default]<2.4,>=2.3 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (2.3.1)\n",
      "Requirement already satisfied: pydantic<2.0,>=1.10.4 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (1.10.12)\n",
      "Requirement already satisfied: grpcio<=1.50.0,>=1.42.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (1.50.0)\n",
      "Requirement already satisfied: Pillow<9.6,>=9.3 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (9.5.0)\n",
      "Requirement already satisfied: jsonschema<4.18,>=4.14 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (4.17.3)\n",
      "Requirement already satisfied: seqeval<1.3.0,>=1.2.2 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (1.2.2)\n",
      "Requirement already satisfied: evaluate<0.4.0,>=0.2.2 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.3.0)\n",
      "Requirement already satisfied: accelerate<0.17,>=0.9 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.16.0)\n",
      "Requirement already satisfied: timm<0.10.0,>=0.9.2 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.9.7)\n",
      "Requirement already satisfied: torch<1.14,>=1.9 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (1.13.1)\n",
      "Requirement already satisfied: torchvision<0.15.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.14.1)\n",
      "Requirement already satisfied: scikit-image<0.20.0,>=0.19.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.19.3)\n",
      "Requirement already satisfied: pytorch-lightning<1.10.0,>=1.9.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (1.9.5)\n",
      "Requirement already satisfied: text-unidecode<1.4,>=1.3 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (1.3)\n",
      "Requirement already satisfied: torchmetrics<0.12.0,>=0.11.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.11.4)\n",
      "Requirement already satisfied: transformers[sentencepiece]<4.27.0,>=4.23.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (4.26.1)\n",
      "Requirement already satisfied: nptyping<2.5.0,>=1.4.4 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (2.4.1)\n",
      "Requirement already satisfied: omegaconf<2.3.0,>=2.1.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (2.2.3)\n",
      "Requirement already satisfied: pytorch-metric-learning<2.0,>=1.3.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (1.7.3)\n",
      "Requirement already satisfied: nlpaug<1.2.0,>=1.1.10 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (1.1.11)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.4.5 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (3.8.1)\n",
      "Requirement already satisfied: openmim<0.4.0,>=0.3.7 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.3.9)\n",
      "Requirement already satisfied: defusedxml<0.7.2,>=0.7.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.7.1)\n",
      "Requirement already satisfied: jinja2<3.2,>=3.0.3 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (3.1.2)\n",
      "Requirement already satisfied: tensorboard<3,>=2.9 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (2.13.0)\n",
      "Requirement already satisfied: pytesseract<0.3.11,>=0.3.9 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.3.10)\n",
      "Requirement already satisfied: lightgbm<3.4,>=3.3 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.tabular[all]==0.8.2->autogluon) (3.3.5)\n",
      "Collecting xgboost<1.8,>=1.6 (from autogluon.tabular[all]==0.8.2->autogluon)\n",
      "  Obtaining dependency information for xgboost<1.8,>=1.6 from https://files.pythonhosted.org/packages/75/dd/9afe0d9d0f61a5384c3932626a022e38c396a5d88e6f5345ad2f7b576747/xgboost-1.7.6-py3-none-win_amd64.whl.metadata\n",
      "  Using cached xgboost-1.7.6-py3-none-win_amd64.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: fastai<2.8,>=2.3.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.tabular[all]==0.8.2->autogluon) (2.7.12)\n",
      "Requirement already satisfied: catboost<1.3,>=1.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.tabular[all]==0.8.2->autogluon) (1.2.2)\n",
      "Requirement already satisfied: joblib<2,>=1.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.timeseries[all]==0.8.2->autogluon) (1.3.2)\n",
      "Requirement already satisfied: statsmodels<0.15,>=0.13.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.timeseries[all]==0.8.2->autogluon) (0.14.0)\n",
      "Requirement already satisfied: gluonts<0.14,>=0.13.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.timeseries[all]==0.8.2->autogluon) (0.13.5)\n",
      "Requirement already satisfied: statsforecast<1.5,>=1.4.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.timeseries[all]==0.8.2->autogluon) (1.4.0)\n",
      "Requirement already satisfied: mlforecast<0.7.4,>=0.7.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.timeseries[all]==0.8.2->autogluon) (0.7.3)\n",
      "Requirement already satisfied: ujson<6,>=5 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.timeseries[all]==0.8.2->autogluon) (5.8.0)\n",
      "Requirement already satisfied: psutil<6,>=5.7.3 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.common==0.8.2->autogluon.core[all]==0.8.2->autogluon) (5.9.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from autogluon.common==0.8.2->autogluon.core[all]==0.8.2->autogluon) (60.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from accelerate<0.17,>=0.9->autogluon.multimodal==0.8.2->autogluon) (23.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from accelerate<0.17,>=0.9->autogluon.multimodal==0.8.2->autogluon) (6.0.1)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.56 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from boto3<2,>=1.10->autogluon.core[all]==0.8.2->autogluon) (1.31.56)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from boto3<2,>=1.10->autogluon.core[all]==0.8.2->autogluon) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from boto3<2,>=1.10->autogluon.core[all]==0.8.2->autogluon) (0.7.0)\n",
      "Requirement already satisfied: graphviz in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==0.8.2->autogluon) (0.20.1)\n",
      "Requirement already satisfied: plotly in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==0.8.2->autogluon) (5.17.0)\n",
      "Requirement already satisfied: six in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==0.8.2->autogluon) (1.16.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (2.14.5)\n",
      "Requirement already satisfied: dill in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (0.3.7)\n",
      "Requirement already satisfied: xxhash in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (0.17.3)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (0.18.0)\n",
      "Requirement already satisfied: pip in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (23.2.1)\n",
      "Requirement already satisfied: fastdownload<2,>=0.0.5 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (0.0.7)\n",
      "Requirement already satisfied: fastcore<1.6,>=1.5.29 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (1.5.29)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (1.0.3)\n",
      "Requirement already satisfied: spacy<4 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (3.6.1)\n",
      "Requirement already satisfied: toolz~=0.10 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from gluonts<0.14,>=0.13.1->autogluon.timeseries[all]==0.8.2->autogluon) (0.12.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from gluonts<0.14,>=0.13.1->autogluon.timeseries[all]==0.8.2->autogluon) (4.5.0)\n",
      "Requirement already satisfied: future in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==0.8.2->autogluon) (0.18.3)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==0.8.2->autogluon) (2.2.1)\n",
      "Requirement already satisfied: py4j in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==0.8.2->autogluon) (0.10.9.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from jinja2<3.2,>=3.0.3->autogluon.multimodal==0.8.2->autogluon) (2.1.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from jsonschema<4.18,>=4.14->autogluon.multimodal==0.8.2->autogluon) (23.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from jsonschema<4.18,>=4.14->autogluon.multimodal==0.8.2->autogluon) (0.19.3)\n",
      "Requirement already satisfied: wheel in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from lightgbm<3.4,>=3.3->autogluon.tabular[all]==0.8.2->autogluon) (0.41.2)\n",
      "Requirement already satisfied: numba in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from mlforecast<0.7.4,>=0.7.0->autogluon.timeseries[all]==0.8.2->autogluon) (0.58.0)\n",
      "Requirement already satisfied: window-ops in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from mlforecast<0.7.4,>=0.7.0->autogluon.timeseries[all]==0.8.2->autogluon) (0.0.14)\n",
      "Requirement already satisfied: gdown>=4.0.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==0.8.2->autogluon) (4.7.1)\n",
      "Requirement already satisfied: click in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==0.8.2->autogluon) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==0.8.2->autogluon) (2023.8.8)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from omegaconf<2.3.0,>=2.1.1->autogluon.multimodal==0.8.2->autogluon) (4.9.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (0.4.6)\n",
      "Requirement already satisfied: model-index in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (0.1.11)\n",
      "Requirement already satisfied: opendatalab in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (0.0.10)\n",
      "Requirement already satisfied: rich in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (13.4.2)\n",
      "Requirement already satisfied: tabulate in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (0.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from pandas<1.6,>=1.4.1->autogluon.core[all]==0.8.2->autogluon) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from pandas<1.6,>=1.4.1->autogluon.core[all]==0.8.2->autogluon) (2023.3.post1)\n",
      "Requirement already satisfied: lightning-utilities>=0.6.0.post0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from pytorch-lightning<1.10.0,>=1.9.0->autogluon.multimodal==0.8.2->autogluon) (0.9.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (3.12.4)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (1.0.6)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (4.24.3)\n",
      "Requirement already satisfied: aiosignal in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (1.3.1)\n",
      "Requirement already satisfied: frozenlist in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (1.4.0)\n",
      "Requirement already satisfied: virtualenv>=20.0.24 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (20.24.5)\n",
      "Requirement already satisfied: aiohttp>=3.7 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (3.8.5)\n",
      "Requirement already satisfied: aiohttp-cors in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (0.7.0)\n",
      "Requirement already satisfied: colorful in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (0.5.5)\n",
      "Requirement already satisfied: py-spy>=0.2.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (0.3.14)\n",
      "Requirement already satisfied: gpustat>=1.0.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (1.1.1)\n",
      "Requirement already satisfied: opencensus in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (0.11.3)\n",
      "Requirement already satisfied: prometheus-client>=0.7.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (0.17.1)\n",
      "Requirement already satisfied: smart-open in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (6.4.0)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (2.6.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from requests->autogluon.core[all]==0.8.2->autogluon) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from requests->autogluon.core[all]==0.8.2->autogluon) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from requests->autogluon.core[all]==0.8.2->autogluon) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from requests->autogluon.core[all]==0.8.2->autogluon) (2023.7.22)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from scikit-image<0.20.0,>=0.19.1->autogluon.multimodal==0.8.2->autogluon) (2.31.4)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from scikit-image<0.20.0,>=0.19.1->autogluon.multimodal==0.8.2->autogluon) (2023.9.26)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from scikit-image<0.20.0,>=0.19.1->autogluon.multimodal==0.8.2->autogluon) (1.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from scikit-learn<1.3,>=1.0->autogluon.core[all]==0.8.2->autogluon) (3.2.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from statsmodels<0.15,>=0.13.0->autogluon.timeseries[all]==0.8.2->autogluon) (0.5.3)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (2.0.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (2.23.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (3.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (2.3.7)\n",
      "Requirement already satisfied: safetensors in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from timm<0.10.0,>=0.9.2->autogluon.multimodal==0.8.2->autogluon) (0.3.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from transformers[sentencepiece]<4.27.0,>=4.23.0->autogluon.multimodal==0.8.2->autogluon) (0.13.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from transformers[sentencepiece]<4.27.0,>=4.23.0->autogluon.multimodal==0.8.2->autogluon) (0.1.99)\n",
      "Collecting protobuf!=3.19.5,>=3.15.3 (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon)\n",
      "  Using cached protobuf-3.20.2-cp39-cp39-win_amd64.whl (904 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from matplotlib->autogluon.core[all]==0.8.2->autogluon) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from matplotlib->autogluon.core[all]==0.8.2->autogluon) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from matplotlib->autogluon.core[all]==0.8.2->autogluon) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from matplotlib->autogluon.core[all]==0.8.2->autogluon) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from matplotlib->autogluon.core[all]==0.8.2->autogluon) (3.1.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from matplotlib->autogluon.core[all]==0.8.2->autogluon) (6.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from aiohttp>=3.7->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from aiohttp>=3.7->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from aiohttp>=3.7->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (1.9.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from datasets>=2.0.0->evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (13.0.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==0.8.2->autogluon) (4.12.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (1.3.1)\n",
      "Requirement already satisfied: nvidia-ml-py>=11.450.129 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from gpustat>=1.0.0->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (12.535.108)\n",
      "Requirement already satisfied: blessed>=1.17.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from gpustat>=1.0.0->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (1.20.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->autogluon.core[all]==0.8.2->autogluon) (3.17.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from markdown>=2.6.8->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (6.8.0)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from numba->mlforecast<0.7.4,>=0.7.0->autogluon.timeseries[all]==0.8.2->autogluon) (0.41.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (2.0.10)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (0.10.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (3.3.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from virtualenv>=20.0.24->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (0.3.7)\n",
      "Requirement already satisfied: platformdirs<4,>=3.9.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from virtualenv>=20.0.24->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (3.10.0)\n",
      "Requirement already satisfied: ordered-set in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (4.1.0)\n",
      "Requirement already satisfied: opencensus-context>=0.1.3 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from opencensus->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (0.1.3)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from opencensus->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (2.12.0)\n",
      "Requirement already satisfied: pycryptodome in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (3.19.0)\n",
      "Requirement already satisfied: openxlab in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (0.0.26)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (306)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from plotly->catboost<1.3,>=1.1->autogluon.tabular[all]==0.8.2->autogluon) (8.2.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (2.16.1)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (0.2.6)\n",
      "Requirement already satisfied: jinxed>=1.1.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (1.2.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (1.60.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (0.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (3.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (0.1.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==0.8.2->autogluon) (2.5)\n",
      "Requirement already satisfied: oss2~=2.17.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (2.17.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from requests->autogluon.core[all]==0.8.2->autogluon) (1.7.1)\n",
      "Requirement already satisfied: ansicon in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from jinxed>=1.1.0->blessed>=1.17.1->gpustat>=1.0.0->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (1.89.0)\n",
      "Requirement already satisfied: crcmod>=1.7 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (1.7)\n",
      "Requirement already satisfied: aliyun-python-sdk-kms>=2.4.1 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (2.16.2)\n",
      "Requirement already satisfied: aliyun-python-sdk-core>=2.13.12 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (2.14.0)\n",
      "Requirement already satisfied: cryptography>=2.6.0 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (41.0.4)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from cffi>=1.12->cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (2.21)\n",
      "Using cached xgboost-1.7.6-py3-none-win_amd64.whl (70.9 MB)\n",
      "Installing collected packages: protobuf, xgboost, pandas\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.24.3\n",
      "    Uninstalling protobuf-4.24.3:\n",
      "      Successfully uninstalled protobuf-4.24.3\n",
      "  Attempting uninstall: xgboost\n",
      "    Found existing installation: xgboost 2.0.1\n",
      "    Uninstalling xgboost-2.0.1:\n",
      "      Successfully uninstalled xgboost-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Ingen tilgang: 'C:\\\\Users\\\\Bruker\\\\OneDrive\\\\NTNU semester 05\\\\TDT4173 Maskinlæring\\\\ml_power_predictor\\\\venv\\\\Lib\\\\site-packages\\\\~-boost\\\\lib\\\\xgboost.dll'\n",
      "Check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pip install autogluon\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.metrics import accuracy_score as acs_score\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import skew\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0.1: Preprocessing of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_RAW_DATA_LOCATION = \"data/raw/\"\n",
    "\n",
    "def get_raw_data():\n",
    "    \"\"\"\n",
    "    Utility function to load the raw data from the data/raw folder.\n",
    "\n",
    "    Returns:\n",
    "        train_a (pd.DataFrame): The training targets for the A dataset.\n",
    "        train_b (pd.DataFrame): The training targets for the B dataset.\n",
    "        train_c (pd.DataFrame): The training targets for the C dataset.\n",
    "        X_train_estimated_a (pd.DataFrame): The estimated training features for the A dataset.\n",
    "        X_train_estimated_b (pd.DataFrame): The estimated training features for the B dataset.\n",
    "        X_train_estimated_c (pd.DataFrame): The estimated training features for the C dataset.\n",
    "        X_train_observed_a (pd.DataFrame): The observed training features for the A dataset.\n",
    "        X_train_observed_b (pd.DataFrame): The observed training features for the B dataset.\n",
    "        X_train_observed_c (pd.DataFrame): The observed training features for the C dataset.\n",
    "        X_test_estimated_a (pd.DataFrame): The estimated test features for the A dataset.\n",
    "        X_test_estimated_b (pd.DataFrame): The estimated test features for the B dataset.\n",
    "        X_test_estimated_c (pd.DataFrame): The estimated test features for the C dataset.\n",
    "    \"\"\"\n",
    "    train_a = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}A/train_targets.parquet')\n",
    "    train_b = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}B/train_targets.parquet')\n",
    "    train_c = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}C/train_targets.parquet')\n",
    "    X_train_estimated_a = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}A/X_train_estimated.parquet')\n",
    "    X_train_estimated_b = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}B/X_train_estimated.parquet')\n",
    "    X_train_estimated_c = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}C/X_train_estimated.parquet')\n",
    "    X_train_observed_a = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}A/X_train_observed.parquet')\n",
    "    X_train_observed_b = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}B/X_train_observed.parquet')\n",
    "    X_train_observed_c = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}C/X_train_observed.parquet')\n",
    "    X_test_estimated_a = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}A/X_test_estimated.parquet')\n",
    "    X_test_estimated_b = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}B/X_test_estimated.parquet')\n",
    "    X_test_estimated_c = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}C/X_test_estimated.parquet')\n",
    "\n",
    "    return train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(\n",
    "    train_observed: pd.DataFrame,\n",
    "    train_estimated: pd.DataFrame,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    drop_features: bool = True,\n",
    ") -> Tuple[\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.Series,\n",
    "    pd.Series,\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.Series,\n",
    "    pd.Series,\n",
    "]:\n",
    "    \"\"\"\n",
    "    Prepares the data for modeling by handling missing values and splitting the data.\n",
    "\n",
    "    Args:\n",
    "    train_observed (pd.DataFrame): The aligned training DataFrame with observed features.\n",
    "    train_estimated (pd.DataFrame): The aligned training DataFrame with estimated features.\n",
    "    test_size (float): The proportion of the dataset to include in the test split.\n",
    "    random_state (int): Controls the shuffling applied to the data before applying the split.\n",
    "\n",
    "    Returns:\n",
    "    X_train_obs (pd.DataFrame): The training features with observed data.\n",
    "    X_val_obs (pd.DataFrame): The validation features with observed data.\n",
    "    y_train_obs (pd.Series): The training target with observed data.\n",
    "    y_val_obs (pd.Series): The validation target with observed data.\n",
    "    X_train_est (pd.DataFrame): The training features with estimated data.\n",
    "    X_val_est (pd.DataFrame): The validation features with estimated data.\n",
    "    y_train_est (pd.Series): The training target with estimated data.\n",
    "    y_val_est (pd.Series): The validation target with estimated data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove missing features\n",
    "    train_observed = remove_missing_features(train_observed)\n",
    "    train_estimated = remove_missing_features(train_estimated)\n",
    "\n",
    "    # Handle missing values (e.g., imputation, removal)\n",
    "    train_observed_clean = train_observed.dropna(\n",
    "        subset=[\"visibility:m\", \"pv_measurement\"]\n",
    "    )\n",
    "    train_estimated_clean = train_estimated.dropna(\n",
    "        subset=[\"visibility:m\", \"pv_measurement\"]\n",
    "    )\n",
    "\n",
    "    # Remove discrepancies\n",
    "    train_observed_clean = clean_pv_data(train_observed_clean)\n",
    "    train_estimated_clean = clean_pv_data(train_estimated_clean)\n",
    "\n",
    "    # Feature engineer\n",
    "    train_observed_clean = feature_engineer(train_observed_clean)\n",
    "    train_estimated_clean = feature_engineer(train_estimated_clean)\n",
    "\n",
    "    # Split the data into features (X) and target (y)\n",
    "    y_obs = train_observed_clean[\"pv_measurement\"]\n",
    "\n",
    "    if drop_features:\n",
    "        X_obs = train_observed_clean.drop(\n",
    "            columns=[\"time\", \"pv_measurement\", \"date_forecast\", \"date_calc\"],\n",
    "            errors=\"ignore\",\n",
    "        )\n",
    "    else:\n",
    "        X_obs = train_observed_clean\n",
    "\n",
    "    if drop_features:\n",
    "        X_est = train_estimated_clean.drop(\n",
    "            columns=[\"time\", \"pv_measurement\", \"date_calc\", \"date_forecast\"],\n",
    "            errors=\"ignore\",\n",
    "        )\n",
    "    else:\n",
    "        X_est = train_estimated_clean\n",
    "\n",
    "    y_est = train_estimated_clean[\"pv_measurement\"]\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train_obs, X_val_obs, y_train_obs, y_val_obs = train_test_split(\n",
    "        X_obs, y_obs, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    X_train_est, X_val_est, y_train_est, y_val_est = train_test_split(\n",
    "        X_est, y_est, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        X_train_obs,\n",
    "        X_val_obs,\n",
    "        y_train_obs,\n",
    "        y_val_obs,\n",
    "        X_train_est,\n",
    "        X_val_est,\n",
    "        y_train_est,\n",
    "        y_val_est,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_location_datasets(\n",
    "    df: pd.DataFrame,\n",
    ") -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    locations = [\"location_a\", \"location_b\", \"location_c\"]\n",
    "    x_a = df[df[\"location_a\"] == 1]\n",
    "    x_a = x_a.drop(locations, axis=1)\n",
    "    y_a = x_a[\"pv_measurement\"]\n",
    "    if \"pv_measurement\" in x_a.columns:\n",
    "        x_a = x_a.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "    x_b = df[df[\"location_b\"] == 1]\n",
    "    x_b = x_b.drop(locations, axis=1)\n",
    "    y_b = x_b[\"pv_measurement\"]\n",
    "    if \"pv_measurement\" in x_b.columns:\n",
    "        x_b = x_b.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "    x_c = df[df[\"location_c\"] == 1]\n",
    "    x_c = x_c.drop(locations, axis=1)\n",
    "    y_c = x_c[\"pv_measurement\"]\n",
    "    if \"pv_measurement\" in x_b.columns:\n",
    "        x_b = x_b.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "    return (x_a, x_b, x_c, y_a, y_b, y_c)\n",
    "\n",
    "\n",
    "def remove_missing_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Remove features with more than 50% missing values or Constant features\n",
    "    df = df.drop(\"snow_density:kgm3\", axis=1)\n",
    "    df = df.drop(\"ceiling_height_agl:m\", axis=1)\n",
    "    df = df.drop(\"elevation:m\", axis=1)\n",
    "    df[\"cloud_base_agl:m\"] = df[\"cloud_base_agl:m\"].fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_pv_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply a series of filters to clean PV data in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame containing PV measurement data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Cleaned DataFrame after applying all filters.\n",
    "    \"\"\"\n",
    "    df = filter_pv_measurements_at_night(df)\n",
    "    df = filter_constant_pv_measurements(df)\n",
    "    df = filter_zero_pv_measurements(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_pv_measurements_at_night(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter out positive PV measurements at night based on specific conditions. As there are no PV measurements at night, these are likely to be measurement errors.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame containing PV measurement and time-of-day data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with unrealistic positive PV measurements at night removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Condition: Positive PV measurement when it's not daytime and the measurement is the same as the previous timestep\n",
    "    night_time_with_positive_pv = (df[\"is_day:idx\"] == 0) & (df[\"pv_measurement\"] > 0)\n",
    "    same_as_previous_step = df[\"pv_measurement\"] == df[\"pv_measurement\"].shift(1)\n",
    "    condition1 = night_time_with_positive_pv & same_as_previous_step\n",
    "\n",
    "    # Condition: Positive PV measurement when sun elevation is below a certain threshold\n",
    "    sun_elevation_threshold = -10\n",
    "    low_sun_elevation_with_positive_pv = (df[\"sun_elevation:d\"] < sun_elevation_threshold) & (df[\"pv_measurement\"] > 0)\n",
    "    \n",
    "    # Combined condition to filter\n",
    "    conditions_to_remove = condition1 | low_sun_elevation_with_positive_pv\n",
    "    df = df.drop(df[conditions_to_remove].index)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_constant_pv_measurements(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter out rows where PV measurement is constant and non-zero for 6 or more consecutive timesteps.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame containing PV measurement data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with specified discrepancies removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Mark changes in pv_measurement and zero values\n",
    "    measurement_change_or_zero = (df[\"pv_measurement\"] != df[\"pv_measurement\"].shift()) | (df[\"pv_measurement\"] == 0)\n",
    "    \n",
    "    # Step 2: Create groups for consecutive measurements\n",
    "    df[\"temp_group\"] = measurement_change_or_zero.cumsum()\n",
    "\n",
    "    # Step 3: Count entries in each group\n",
    "    group_counts = df.groupby(\"temp_group\")[\"pv_measurement\"].transform(\"count\")\n",
    "\n",
    "    # Step 4: Determine rows to remove (constant non-zero measurements for 6+ timesteps)\n",
    "    rows_to_remove = (group_counts >= 6) & (df[\"pv_measurement\"] != 0)\n",
    "\n",
    "    # Step 5: Remove specified rows and the temporary grouping column\n",
    "    df_filtered = df[~rows_to_remove].drop(columns=[\"temp_group\"])\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "\n",
    "def filter_zero_pv_measurements(\n",
    "    un_filtered_df: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove entries where PV measurements are zero despite significant radiation.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame containing radiation and PV measurement data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Filtered DataFrame.\n",
    "    \"\"\"\n",
    "    # Trail and error on total_radiation_threshold, tried 0.5, 5 and 30\n",
    "    total_radiation_threshold = 30\n",
    "    is_significant_radiation = (un_filtered_df[\"diffuse_rad:W\"] + un_filtered_df[\"direct_rad:W\"]) >= total_radiation_threshold\n",
    "    is_zero_pv_measurement = un_filtered_df[\"pv_measurement\"] == 0\n",
    "    filtered_df = un_filtered_df[~(is_significant_radiation & is_zero_pv_measurement)]\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def feature_engineer(data_frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    data_frame = create_time_features_from_date(data_frame)\n",
    "    data_frame[\"solar_radiation_interaction\"] = data_frame[\"diffuse_rad:W\"] * data_frame[\"direct_rad:W\"]\n",
    "\n",
    "    data_frame[\"effective_solar_elevation\"] = np.where(\n",
    "        data_frame[\"sun_elevation:d\"] <= 0,\n",
    "        0,\n",
    "        np.sin(np.radians(data_frame[\"sun_elevation:d\"])),\n",
    "    )\n",
    "    data_frame = data_frame.drop(\"sun_elevation:d\", axis=1)\n",
    "\n",
    "    data_frame[\"effective_radiation\"] = np.where(\n",
    "        data_frame[\"clear_sky_energy_1h:J\"] == 0,\n",
    "        0,  # or your specified value\n",
    "        data_frame[\"direct_rad_1h:J\"] / data_frame[\"clear_sky_energy_1h:J\"],\n",
    "    )\n",
    "\n",
    "    data_frame[\"net_clear_sky_residual\"] = (\n",
    "        data_frame[\"clear_sky_rad:W\"]\n",
    "        - data_frame[\"direct_rad:W\"]\n",
    "        - data_frame[\"diffuse_rad:W\"]\n",
    "    )\n",
    "\n",
    "    data_frame[\"cloud_ratio\"] = np.where(\n",
    "        data_frame[\"total_cloud_cover:p\"] == 0,\n",
    "        0,  # or your specified value\n",
    "        data_frame[\"effective_cloud_cover:p\"] / data_frame[\"total_cloud_cover:p\"],\n",
    "    )\n",
    "\n",
    "    data_frame[\"low_cloud_diffuse_rad\"] = data_frame[\n",
    "        \"diffuse_rad:W\"\n",
    "    ].where(data_frame[\"effective_cloud_cover:p\"] < 0.3, 0)\n",
    "\n",
    "    data_frame[\"cloud_cover_over_30%\"] = np.where(\n",
    "        data_frame[\"effective_cloud_cover:p\"] > 30, 1, 0\n",
    "    )\n",
    "\n",
    "    data_frame[\"global_horizontal_irradiation\"] = (\n",
    "        data_frame[\"diffuse_rad:W\"] + data_frame[\"direct_rad:W\"]\n",
    "    )\n",
    "\n",
    "    data_frame[\"direct_rad_cloud_adjustment\"] = data_frame[\"direct_rad:W\"] * (\n",
    "        100 - data_frame[\"effective_cloud_cover:p\"]\n",
    "    )\n",
    "\n",
    "    data_frame[\"effective_solar_elevation_squared\"] = (\n",
    "        data_frame[\"effective_solar_elevation\"] ** 0.5\n",
    "    )\n",
    "    \n",
    "    snow_columns = [\n",
    "        \"snow_depth:cm\",\n",
    "        \"fresh_snow_12h:cm\",\n",
    "        \"fresh_snow_1h:cm\",\n",
    "        \"fresh_snow_24h:cm\",\n",
    "        \"fresh_snow_3h:cm\",\n",
    "        \"fresh_snow_6h:cm\",\n",
    "    ]\n",
    "\n",
    "    data_frame[\"is_freezing\"] = (data_frame[\"t_1000hPa:K\"] < 273).astype(int)\n",
    "\n",
    "    data_frame[\"is_snow\"] = (data_frame[snow_columns] > 0).any(axis=1).astype(int)\n",
    "    data_frame[\"is_rain\"] = (data_frame[\"precip_5min:mm\"] > 0).astype(int)\n",
    "\n",
    "    data_frame = data_frame.drop(\"snow_drift:idx\", axis=1)\n",
    "\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "def create_time_features_from_date(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a new data frame with new features from date_forecast column.\n",
    "    This will create temporal features from date_forecast that are easier to learn by the model.\n",
    "    It creates the following features: month, season, year, day_of_year, day_segment.\n",
    "    All of the new features are int type.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Data frame with date_forecast column.\n",
    "    Returns:\n",
    "        pd.DataFrame: Data frame copy with new features.\n",
    "\n",
    "    \"\"\"\n",
    "    df[\"sin_day_of_year\"] = df[\"date_forecast\"].apply(get_sin_day)\n",
    "    df[\"cos_day_of_year\"] = df[\"date_forecast\"].apply(get_cos_day)\n",
    "    df[\"sin_hour\"] = df[\"date_forecast\"].apply(get_sin_hour)\n",
    "    df[\"cos_hour\"] = df[\"date_forecast\"].apply(get_cos_hour)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_sin_hour(date: datetime) -> float:\n",
    "    HOURS_OF_DAY = 24\n",
    "    return math.sin(2 * math.pi * (date.hour) / HOURS_OF_DAY)\n",
    "\n",
    "\n",
    "def get_cos_hour(date: datetime) -> float:\n",
    "    HOURS_OF_DAY = 24\n",
    "    return math.cos(2 * math.pi * (date.hour) / HOURS_OF_DAY)\n",
    "\n",
    "\n",
    "def get_sin_day(date: datetime) -> float:\n",
    "    DAY_OF_YEAR = 365.25  # Add 0.25 to account for leap years\n",
    "    return math.sin(2 * math.pi * (date.timetuple().tm_yday - 1) / DAY_OF_YEAR)\n",
    "\n",
    "\n",
    "def get_cos_day(date: datetime) -> float:\n",
    "    DAY_OF_YEAR = 365.25  # Add 0.25 to account for leap years\n",
    "    return math.cos(2 * math.pi * (date.timetuple().tm_yday - 1) / DAY_OF_YEAR)\n",
    "\n",
    "\n",
    "def add_location(data_frame: pd.DataFrame, location: str):\n",
    "    if location.lower() == \"a\":\n",
    "        data_frame[\"location_a\"] = 1\n",
    "    else:\n",
    "        data_frame[\"location_a\"] = 0\n",
    "\n",
    "    if location.lower() == \"b\":\n",
    "        data_frame[\"location_b\"] = 1\n",
    "    else:\n",
    "        data_frame[\"location_b\"] = 0\n",
    "\n",
    "    if location.lower() == \"c\":\n",
    "        data_frame[\"location_c\"] = 1\n",
    "    else:\n",
    "        data_frame[\"location_c\"] = 0\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "# Define a function to align the temporal resolution of the datasets\n",
    "def temporal_alignment(\n",
    "    train: pd.DataFrame, observed: pd.DataFrame, estimated: pd.DataFrame\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Aligns the temporal resolution of the datasets by aggregating the 15-min interval weather data to hourly intervals.\n",
    "\n",
    "    Args:\n",
    "        train (pd.DataFrame): The training targets DataFrame.\n",
    "        observed (pd.DataFrame): The observed training features DataFrame.\n",
    "        estimated (pd.DataFrame): The estimated training features DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        train_observed (pd.DataFrame): The aligned training DataFrame with observed features.\n",
    "        train_estimated (pd.DataFrame): The aligned training DataFrame with estimated features.\n",
    "    \"\"\"\n",
    "    # Convert the time columns to datetime objects\n",
    "    train[\"time\"] = pd.to_datetime(train[\"time\"])\n",
    "    observed[\"date_forecast\"] = pd.to_datetime(observed[\"date_forecast\"])\n",
    "    estimated[\"date_forecast\"] = pd.to_datetime(estimated[\"date_forecast\"])\n",
    "\n",
    "    # Set the date_forecast column as index for resampling\n",
    "    observed.set_index(\"date_forecast\", inplace=True)\n",
    "    estimated.set_index(\"date_forecast\", inplace=True)\n",
    "\n",
    "    # Resample the weather data to hourly intervals and aggregate the values by mean\n",
    "    observed_resampled = observed.resample(\"1H\").mean()\n",
    "    estimated_resampled = estimated.resample(\"1H\").mean()\n",
    "\n",
    "    # Reset the index after resampling\n",
    "    observed_resampled.reset_index(inplace=True)\n",
    "    estimated_resampled.reset_index(inplace=True)\n",
    "\n",
    "    # Merge the aggregated weather data with the solar production data based on the timestamp\n",
    "    train_observed = pd.merge(\n",
    "        train, observed_resampled, how=\"left\", left_on=\"time\", right_on=\"date_forecast\"\n",
    "    )\n",
    "    train_estimated = pd.merge(\n",
    "        train, estimated_resampled, how=\"left\", left_on=\"time\", right_on=\"date_forecast\"\n",
    "    )\n",
    "\n",
    "    return train_observed, train_estimated\n",
    "\n",
    "\n",
    "def temporal_alignment_tests(test: pd.DataFrame) -> Tuple[pd.DataFrame]:\n",
    "    return aggregate_rows(test)\n",
    "\n",
    "\n",
    "def aggregate_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Create a 'group' column to group every 4 rows together\n",
    "    df[\"group\"] = df.index // 4\n",
    "\n",
    "    # Define the aggregation functions\n",
    "    aggregation = {col: \"mean\" for col in df.columns if col != \"date_forecast\"}\n",
    "    aggregation[\"date_forecast\"] = \"first\"\n",
    "\n",
    "    # Group by the 'group' column and aggregate\n",
    "    df_agg = df.groupby(\"group\").agg(aggregation).reset_index(drop=True)\n",
    "\n",
    "    # Drop the 'group' column from the original dataframe\n",
    "    df_agg.drop(\"group\", axis=1, inplace=True)\n",
    "\n",
    "    return df_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_preprocessed_data(drop_features: bool = True) -> (\n",
    "    Tuple[\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "    ]\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch the preprocessed data for training and validation.\n",
    "\n",
    "    Returns:\n",
    "        X_train_obs_combined: The observed data for training\n",
    "        X_val_obs_combined: The observed data for validation\n",
    "        y_train_obs_combined: The observed labels for training\n",
    "        y_val_obs_combined: The observed labels for validation\n",
    "        X_train_est_combined: The estimated data for training\n",
    "        X_val_est_combined: The estimated data for validation\n",
    "        y_train_est_combined: The estimated labels for training\n",
    "        y_val_est_combined: The estimated labels for validation\n",
    "    \"\"\"\n",
    "    (\n",
    "        train_a,\n",
    "        train_b,\n",
    "        train_c,\n",
    "        X_train_estimated_a,\n",
    "        X_train_estimated_b,\n",
    "        X_train_estimated_c,\n",
    "        X_train_observed_a,\n",
    "        X_train_observed_b,\n",
    "        X_train_observed_c,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "    ) = get_raw_data()\n",
    "\n",
    "    # Temporally align the data from all three locations to the same time.\n",
    "    train_observed_a, train_estimated_a = temporal_alignment(\n",
    "        train_a, X_train_observed_a, X_train_estimated_a\n",
    "    )\n",
    "    train_observed_b, train_estimated_b = temporal_alignment(\n",
    "        train_b, X_train_observed_b, X_train_estimated_b\n",
    "    )\n",
    "    train_observed_c, train_estimated_c = temporal_alignment(\n",
    "        train_c, X_train_observed_c, X_train_estimated_c\n",
    "    )\n",
    "\n",
    "    # Add location data\n",
    "    train_observed_a = add_location(train_observed_a, \"a\")\n",
    "    train_estimated_a = add_location(train_estimated_a, \"a\")\n",
    "\n",
    "    train_observed_b = add_location(train_observed_b, \"b\")\n",
    "    train_estimated_b = add_location(train_estimated_b, \"b\")\n",
    "\n",
    "    train_observed_c = add_location(train_observed_c, \"c\")\n",
    "    train_estimated_c = add_location(train_estimated_c, \"c\")\n",
    "\n",
    "    # Combine the temporally aligned datasets from all three locations\n",
    "    train_observed_combined = pd.concat(\n",
    "        [train_observed_a, train_observed_b, train_observed_c], ignore_index=True\n",
    "    )\n",
    "    train_estimated_combined = pd.concat(\n",
    "        [train_estimated_a, train_estimated_b, train_estimated_c], ignore_index=True\n",
    "    )\n",
    "\n",
    "\n",
    "    # # Add boolean flag for estimated vs observed\n",
    "    # train_observed_combined[\"estimated_flag\"] = 0\n",
    "    # train_estimated_combined[\"estimated_flag\"] = 1\n",
    "\n",
    "    # Prepare the combined dataset by handling missing values and splitting the data\n",
    "    (\n",
    "        X_train_obs_combined,\n",
    "        X_val_obs_combined,\n",
    "        y_train_obs_combined,\n",
    "        y_val_obs_combined,\n",
    "        X_train_est_combined,\n",
    "        X_val_est_combined,\n",
    "        y_train_est_combined,\n",
    "        y_val_est_combined,\n",
    "    ) = prepare_data(train_observed_combined, train_estimated_combined, drop_features=drop_features)\n",
    "\n",
    "    return (\n",
    "        X_train_obs_combined,\n",
    "        X_val_obs_combined,\n",
    "        y_train_obs_combined,\n",
    "        y_val_obs_combined,\n",
    "        X_train_est_combined,\n",
    "        X_val_est_combined,\n",
    "        y_train_est_combined,\n",
    "        y_val_est_combined,\n",
    "    )\n",
    "\n",
    "def get_preprocessed_test_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the preprocessed test data without the 'date_forecast' column.\n",
    "    \"\"\"\n",
    "    (\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        X_test_estimated_a,\n",
    "        X_test_estimated_b,\n",
    "        X_test_estimated_c,\n",
    "    ) = get_raw_data()\n",
    "\n",
    "    # Align the test data to the same time as the training data\n",
    "    X_test_estimated_a = temporal_alignment_tests(X_test_estimated_a)\n",
    "    X_test_estimated_b = temporal_alignment_tests(X_test_estimated_b)\n",
    "    X_test_estimated_c = temporal_alignment_tests(X_test_estimated_c)\n",
    "    print(\"After temporal alignment\")\n",
    "    print(f\"X_test_estimated_a.shape = {X_test_estimated_a.shape}, X_test_estimated_b.shape = {X_test_estimated_b.shape}, X_test_estimated_c.shape = {X_test_estimated_c.shape}\")\n",
    "\n",
    "    X_test_estimated_a = remove_missing_features(X_test_estimated_a)\n",
    "    X_test_estimated_b = remove_missing_features(X_test_estimated_b)\n",
    "    X_test_estimated_c = remove_missing_features(X_test_estimated_c)\n",
    "\n",
    "    # Add location data\n",
    "    X_test_estimated_a = add_location(X_test_estimated_a, \"a\")\n",
    "    X_test_estimated_b = add_location(X_test_estimated_b, \"b\")\n",
    "    X_test_estimated_c = add_location(X_test_estimated_c, \"c\")\n",
    "\n",
    "    X_test_a_correct_features = feature_engineer(X_test_estimated_a)\n",
    "    X_test_b_correct_features = feature_engineer(X_test_estimated_b)\n",
    "    X_test_c_correct_features = feature_engineer(X_test_estimated_c)\n",
    "\n",
    "    # X_train_obs_combined, X_val_obs_combined, y_train_obs_combined, y_val_obs_combined, X_train_est_combined, X_val_est_combined, y_train_est_combined, y_val_est_combined = fetch_preprocessed_data()\n",
    "    \n",
    "    # # Add historical data so that the model can use it for prediction\n",
    "    # # Add mean_pv_measurement with same day and hour from previous years\n",
    "    # X_test_estimated_a_with_historical_data = add_expected_pv_to_test_data(X_test_a_correct_features, X_train_obs_combined)\n",
    "    # X_test_estimated_b_with_historical_data = add_expected_pv_to_test_data(X_test_b_correct_features, X_train_obs_combined)\n",
    "    # X_test_estimated_c_with_historical_data = add_expected_pv_to_test_data(X_test_c_correct_features, X_train_obs_combined)\n",
    "\n",
    "    # Drop the 'date_calc' and 'date_forecast' columns from the test data\n",
    "    X_test_estimated_a_processed = X_test_a_correct_features.drop(\n",
    "        columns=[\"date_calc\", \"date_forecast\"], errors='ignore'\n",
    "    )\n",
    "    X_test_estimated_b_processed = X_test_b_correct_features.drop(\n",
    "        columns=[\"date_calc\", \"date_forecast\"], errors='ignore'\n",
    "    )\n",
    "    X_test_estimated_c_processed = X_test_c_correct_features.drop(\n",
    "        columns=[\"date_calc\", \"date_forecast\"], errors='ignore'\n",
    "    )\n",
    "\n",
    "    # # # Handle NaN values in the test data by filling them with the mean value of the respective column from the training data\n",
    "    # X_test_estimated_a_processed.dropna()\n",
    "    # X_test_estimated_b_processed.dropna()\n",
    "    # X_test_estimated_c_processed.dropna()\n",
    "    print(f\"X_test_estimated_a_processed.shape = {X_test_estimated_a_processed.shape}, X_test_estimated_b_processed.shape = {X_test_estimated_b_processed.shape}, X_test_estimated_c_processed.shape = {X_test_estimated_c_processed.shape}\")\n",
    "    tests = pd.concat([X_test_estimated_a_processed, X_test_estimated_b_processed, X_test_estimated_c_processed], ignore_index=True)\n",
    "\n",
    "    # Add boolean flag for estimated vs observed data\n",
    "    # tests[\"estimated_flag\"] = 1\n",
    "    return tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After temporal alignment\n",
      "X_test_estimated_a.shape = (720, 47), X_test_estimated_b.shape = (720, 47), X_test_estimated_c.shape = (720, 47)\n",
      "X_test_estimated_a_processed.shape = (720, 60), X_test_estimated_b_processed.shape = (720, 60), X_test_estimated_c_processed.shape = (720, 60)\n"
     ]
    }
   ],
   "source": [
    "X_train_obs_combined, X_val_obs_combined, y_train_obs_combined, y_val_obs_combined, X_train_est_combined, X_val_est_combined, y_train_est_combined, y_val_est_combined = fetch_preprocessed_data()\n",
    "x_test_whole = get_preprocessed_test_data()\n",
    "\n",
    "#estimated_flag\n",
    "X_train_obs_combined[\"estimated_flag\"] = 0\n",
    "X_val_obs_combined[\"estimated_flag\"] = 0\n",
    "X_train_est_combined[\"estimated_flag\"] = 1\n",
    "X_val_est_combined[\"estimated_flag\"] = 1\n",
    "x_test_whole[\"estimated_flag\"] = 1\n",
    "\n",
    "\n",
    "x_whole = pd.concat([X_train_obs_combined, X_val_obs_combined, X_train_est_combined, X_val_est_combined])\n",
    "y_whole = pd.concat([y_train_obs_combined, y_val_obs_combined, y_train_est_combined, y_val_est_combined])\n",
    "x_whole.reset_index(drop=True, inplace=True)\n",
    "y_whole.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole[\"pv_measurement\"] = y_whole\n",
    "df_shuffled = x_whole.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "x_whole_a = df_shuffled[df_shuffled['location_a'] == 1]\n",
    "x_whole_b = df_shuffled[df_shuffled['location_b'] == 1]\n",
    "x_whole_c = df_shuffled[df_shuffled['location_c'] == 1]\n",
    "\n",
    "y_whole_a = x_whole_a[\"pv_measurement\"]\n",
    "x_whole_a = x_whole_a.drop('location_a', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_b', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_c', axis = 1)\n",
    "\n",
    "y_whole_b = x_whole_b[\"pv_measurement\"]\n",
    "x_whole_b = x_whole_b.drop('location_a', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_b', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_c', axis = 1)\n",
    "\n",
    "y_whole_c = x_whole_c[\"pv_measurement\"]\n",
    "x_whole_c = x_whole_c.drop('location_a', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_b', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_c', axis = 1)\n",
    "cat_features = [\"estimated_flag\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autogluon for A, B and C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231112_200006\\\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 1s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20231112_200006\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.9.13\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   13.16 GB / 254.94 GB (5.2%)\n",
      "Train Data Rows:    34046\n",
      "Train Data Columns: 58\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3566.71 MB\n",
      "\tTrain Data (Original)  Memory Usage: 8.58 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m', ...]\n",
      "\t\t('int', [])   :  5 | ['cloud_cover_over_30%', 'is_freezing', 'is_snow', 'is_rain', 'estimated_flag']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 53 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m', ...]\n",
      "\t\t('int', ['bool']) :  5 | ['cloud_cover_over_30%', 'is_freezing', 'is_snow', 'is_rain', 'estimated_flag']\n",
      "\t0.4s = Fit runtime\n",
      "\t58 features in original data used to generate 58 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.93 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.42s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 0.38s of the 0.57s of remaining time.\n",
      "\tNot enough time to generate out-of-fold predictions for model. Estimated time required was 43.85s compared to 10s of available time.\n",
      "\tTime limit exceeded... Skipping KNeighborsUnif_BAG_L1.\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "No base models to train on, skipping auxiliary stack level 2...\n",
      "No base models to train on, skipping stack level 2...\n",
      "No base models to train on, skipping auxiliary stack level 3...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "AutoGluon did not successfully train any models",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Bruker\\OneDrive\\NTNU semester 05\\TDT4173 Maskinlæring\\ml_power_predictor\\Short notebook 2.ipynb Cell 15\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Bruker/OneDrive/NTNU%20semester%2005/TDT4173%20Maskinl%C3%A6ring/ml_power_predictor/Short%20notebook%202.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mautogluon\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtabular\u001b[39;00m \u001b[39mimport\u001b[39;00m TabularPredictor\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Bruker/OneDrive/NTNU%20semester%2005/TDT4173%20Maskinl%C3%A6ring/ml_power_predictor/Short%20notebook%202.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Initialize the TabularPredictor\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Bruker/OneDrive/NTNU%20semester%2005/TDT4173%20Maskinl%C3%A6ring/ml_power_predictor/Short%20notebook%202.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m best_model_a \u001b[39m=\u001b[39m TabularPredictor(label\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mpv_measurement\u001b[39;49m\u001b[39m'\u001b[39;49m, eval_metric\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmean_absolute_error\u001b[39;49m\u001b[39m'\u001b[39;49m, problem_type\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mregression\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Bruker/OneDrive/NTNU%20semester%2005/TDT4173%20Maskinl%C3%A6ring/ml_power_predictor/Short%20notebook%202.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     train_data\u001b[39m=\u001b[39;49mx_whole_a,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Bruker/OneDrive/NTNU%20semester%2005/TDT4173%20Maskinl%C3%A6ring/ml_power_predictor/Short%20notebook%202.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     presets\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mbest_quality\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Bruker/OneDrive/NTNU%20semester%2005/TDT4173%20Maskinl%C3%A6ring/ml_power_predictor/Short%20notebook%202.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     time_limit\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Bruker/OneDrive/NTNU%20semester%2005/TDT4173%20Maskinl%C3%A6ring/ml_power_predictor/Short%20notebook%202.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Bruker\\OneDrive\\NTNU semester 05\\TDT4173 Maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages\\autogluon\\core\\utils\\decorators.py:31\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     30\u001b[0m     gargs, gkwargs \u001b[39m=\u001b[39m g(\u001b[39m*\u001b[39mother_args, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 31\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39mgargs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mgkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Bruker\\OneDrive\\NTNU semester 05\\TDT4173 Maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages\\autogluon\\tabular\\predictor\\predictor.py:986\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[1;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, calibrate_decision_threshold, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m     aux_kwargs[\u001b[39m\"\u001b[39m\u001b[39mfit_weighted_ensemble\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    985\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave(silent\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# Save predictor to disk to enable prediction and training after interrupt\u001b[39;00m\n\u001b[1;32m--> 986\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_learner\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m    987\u001b[0m     X\u001b[39m=\u001b[39;49mtrain_data,\n\u001b[0;32m    988\u001b[0m     X_val\u001b[39m=\u001b[39;49mtuning_data,\n\u001b[0;32m    989\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49munlabeled_data,\n\u001b[0;32m    990\u001b[0m     holdout_frac\u001b[39m=\u001b[39;49mholdout_frac,\n\u001b[0;32m    991\u001b[0m     num_bag_folds\u001b[39m=\u001b[39;49mnum_bag_folds,\n\u001b[0;32m    992\u001b[0m     num_bag_sets\u001b[39m=\u001b[39;49mnum_bag_sets,\n\u001b[0;32m    993\u001b[0m     num_stack_levels\u001b[39m=\u001b[39;49mnum_stack_levels,\n\u001b[0;32m    994\u001b[0m     hyperparameters\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[0;32m    995\u001b[0m     core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs,\n\u001b[0;32m    996\u001b[0m     aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs,\n\u001b[0;32m    997\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[0;32m    998\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[0;32m    999\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[0;32m   1000\u001b[0m     verbosity\u001b[39m=\u001b[39;49mverbosity,\n\u001b[0;32m   1001\u001b[0m     use_bag_holdout\u001b[39m=\u001b[39;49muse_bag_holdout,\n\u001b[0;32m   1002\u001b[0m )\n\u001b[0;32m   1003\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_post_fit_vars()\n\u001b[0;32m   1005\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_post_fit(\n\u001b[0;32m   1006\u001b[0m     keep_only_best\u001b[39m=\u001b[39mkwargs[\u001b[39m\"\u001b[39m\u001b[39mkeep_only_best\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   1007\u001b[0m     refit_full\u001b[39m=\u001b[39mkwargs[\u001b[39m\"\u001b[39m\u001b[39mrefit_full\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     infer_limit\u001b[39m=\u001b[39minfer_limit,\n\u001b[0;32m   1013\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Bruker\\OneDrive\\NTNU semester 05\\TDT4173 Maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages\\autogluon\\tabular\\learner\\abstract_learner.py:159\u001b[0m, in \u001b[0;36mAbstractTabularLearner.fit\u001b[1;34m(self, X, X_val, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLearner is already fit.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    158\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_fit_input(X\u001b[39m=\u001b[39mX, X_val\u001b[39m=\u001b[39mX_val, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 159\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(X\u001b[39m=\u001b[39mX, X_val\u001b[39m=\u001b[39mX_val, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Bruker\\OneDrive\\NTNU semester 05\\TDT4173 Maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages\\autogluon\\tabular\\learner\\default_learner.py:157\u001b[0m, in \u001b[0;36mDefaultLearner._fit\u001b[1;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_metric \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39meval_metric\n\u001b[0;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave()\n\u001b[1;32m--> 157\u001b[0m trainer\u001b[39m.\u001b[39mfit(\n\u001b[0;32m    158\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[0;32m    159\u001b[0m     y\u001b[39m=\u001b[39my,\n\u001b[0;32m    160\u001b[0m     X_val\u001b[39m=\u001b[39mX_val,\n\u001b[0;32m    161\u001b[0m     y_val\u001b[39m=\u001b[39my_val,\n\u001b[0;32m    162\u001b[0m     X_unlabeled\u001b[39m=\u001b[39mX_unlabeled,\n\u001b[0;32m    163\u001b[0m     holdout_frac\u001b[39m=\u001b[39mholdout_frac,\n\u001b[0;32m    164\u001b[0m     time_limit\u001b[39m=\u001b[39mtime_limit_trainer,\n\u001b[0;32m    165\u001b[0m     infer_limit\u001b[39m=\u001b[39minfer_limit,\n\u001b[0;32m    166\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39minfer_limit_batch_size,\n\u001b[0;32m    167\u001b[0m     groups\u001b[39m=\u001b[39mgroups,\n\u001b[0;32m    168\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrainer_fit_kwargs,\n\u001b[0;32m    169\u001b[0m )\n\u001b[0;32m    170\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_trainer(trainer\u001b[39m=\u001b[39mtrainer)\n\u001b[0;32m    171\u001b[0m time_end \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "File \u001b[1;32mc:\\Users\\Bruker\\OneDrive\\NTNU semester 05\\TDT4173 Maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages\\autogluon\\tabular\\trainer\\auto_trainer.py:114\u001b[0m, in \u001b[0;36mAutoTrainer.fit\u001b[1;34m(self, X, y, hyperparameters, X_val, y_val, X_unlabeled, holdout_frac, num_stack_levels, core_kwargs, aux_kwargs, time_limit, infer_limit, infer_limit_batch_size, use_bag_holdout, groups, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m log_str \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m}\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    112\u001b[0m logger\u001b[39m.\u001b[39mlog(\u001b[39m20\u001b[39m, log_str)\n\u001b[1;32m--> 114\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_multi_and_ensemble(\n\u001b[0;32m    115\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    116\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m    117\u001b[0m     X_val\u001b[39m=\u001b[39;49mX_val,\n\u001b[0;32m    118\u001b[0m     y_val\u001b[39m=\u001b[39;49my_val,\n\u001b[0;32m    119\u001b[0m     X_unlabeled\u001b[39m=\u001b[39;49mX_unlabeled,\n\u001b[0;32m    120\u001b[0m     hyperparameters\u001b[39m=\u001b[39;49mhyperparameters,\n\u001b[0;32m    121\u001b[0m     num_stack_levels\u001b[39m=\u001b[39;49mnum_stack_levels,\n\u001b[0;32m    122\u001b[0m     time_limit\u001b[39m=\u001b[39;49mtime_limit,\n\u001b[0;32m    123\u001b[0m     core_kwargs\u001b[39m=\u001b[39;49mcore_kwargs,\n\u001b[0;32m    124\u001b[0m     aux_kwargs\u001b[39m=\u001b[39;49maux_kwargs,\n\u001b[0;32m    125\u001b[0m     infer_limit\u001b[39m=\u001b[39;49minfer_limit,\n\u001b[0;32m    126\u001b[0m     infer_limit_batch_size\u001b[39m=\u001b[39;49minfer_limit_batch_size,\n\u001b[0;32m    127\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[0;32m    128\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Bruker\\OneDrive\\NTNU semester 05\\TDT4173 Maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py:2384\u001b[0m, in \u001b[0;36mAbstractTrainer._train_multi_and_ensemble\u001b[1;34m(self, X, y, X_val, y_val, hyperparameters, X_unlabeled, num_stack_levels, time_limit, groups, **kwargs)\u001b[0m\n\u001b[0;32m   2371\u001b[0m model_names_fit \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_multi_levels(\n\u001b[0;32m   2372\u001b[0m     X,\n\u001b[0;32m   2373\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2381\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2382\u001b[0m )\n\u001b[0;32m   2383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_model_names()) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2384\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mAutoGluon did not successfully train any models\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   2385\u001b[0m \u001b[39mreturn\u001b[39;00m model_names_fit\n",
      "\u001b[1;31mValueError\u001b[0m: AutoGluon did not successfully train any models"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "\n",
    "# Initialize the TabularPredictor\n",
    "best_model_a = TabularPredictor(label='pv_measurement', eval_metric='mean_absolute_error', problem_type=\"regression\").fit(\n",
    "    train_data=x_whole_a,\n",
    "    presets='best_quality',\n",
    "    time_limit=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231112_193407\\\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 1s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20231112_193407\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.0\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   37.02 GB / 1022.87 GB (3.6%)\n",
      "Train Data Rows:    28688\n",
      "Train Data Columns: 58\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4582.84 MB\n",
      "\tTrain Data (Original)  Memory Usage: 7.23 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m', ...]\n",
      "\t\t('int', [])   :  5 | ['cloud_cover_over_30%', 'is_freezing', 'is_snow', 'is_rain', 'estimated_flag']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 53 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m', ...]\n",
      "\t\t('int', ['bool']) :  5 | ['cloud_cover_over_30%', 'is_freezing', 'is_snow', 'is_rain', 'estimated_flag']\n",
      "\t0.2s = Fit runtime\n",
      "\t58 features in original data used to generate 58 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 6.68 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.2s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 0.53s of the 0.8s of remaining time.\n",
      "\t-35.9509\t = Validation score   (-mean_absolute_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.8s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 0.8s of the -0.15s of remaining time.\n",
      "\t-35.9509\t = Validation score   (-mean_absolute_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "No base models to train on, skipping auxiliary stack level 3...\n",
      "AutoGluon training complete, total runtime = 1.24s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231112_193407\\\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# Initialize the TabularPredictor\n",
    "best_model_b = TabularPredictor(label='pv_measurement', eval_metric='mean_absolute_error', problem_type=\"regression\").fit(\n",
    "    train_data=x_whole_b,\n",
    "    presets='best_quality',\n",
    "    time_limit=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231112_193409\\\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 1s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20231112_193409\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.0\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   37.00 GB / 1022.87 GB (3.6%)\n",
      "Train Data Rows:    25142\n",
      "Train Data Columns: 58\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4571.88 MB\n",
      "\tTrain Data (Original)  Memory Usage: 6.34 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m', ...]\n",
      "\t\t('int', [])   :  5 | ['cloud_cover_over_30%', 'is_freezing', 'is_snow', 'is_rain', 'estimated_flag']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 53 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m', ...]\n",
      "\t\t('int', ['bool']) :  5 | ['cloud_cover_over_30%', 'is_freezing', 'is_snow', 'is_rain', 'estimated_flag']\n",
      "\t0.2s = Fit runtime\n",
      "\t58 features in original data used to generate 58 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.86 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.21s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 0.52s of the 0.78s of remaining time.\n",
      "\t-27.1106\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.63s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 0.78s of the 0.01s of remaining time.\n",
      "\t-27.1106\t = Validation score   (-mean_absolute_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "No base models to train on, skipping auxiliary stack level 3...\n",
      "AutoGluon training complete, total runtime = 1.08s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231112_193409\\\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "\n",
    "# Initialize the TabularPredictor\n",
    "best_model_c = TabularPredictor(label='pv_measurement', eval_metric='mean_absolute_error', problem_type=\"regression\").fit(\n",
    "    train_data=x_whole_c,\n",
    "    presets='best_quality',\n",
    "    time_limit=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from src.features.preprocess_data import fetch_preprocessed_data\n",
    "\n",
    "find_time_sin = lambda hour: math.sin(2 * math.pi * (hour) / 24)\n",
    "find_time_cos = lambda hour: math.cos(2 * math.pi * (hour) / 24)\n",
    "\n",
    "def postprocess_data(x_test: pd.DataFrame, y_pred: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Postprocess the data to set the predicted values to 0 at the correct times.\"\"\"\n",
    "    \n",
    "    # Cap the min and max values for each location for each hour\n",
    "    y_pred = cap_min_max_values(x_test, y_pred)\n",
    "\n",
    "    # Set the predicted values to 0 at the correct times\n",
    "    y_pred = set_0_pv_at_times(x_test, y_pred, \"a\", [22, 23, 0])\n",
    "    y_pred = set_0_pv_at_times(x_test, y_pred, \"b\", [22, 23, 0])\n",
    "    y_pred = set_0_pv_at_times(x_test, y_pred, \"c\", [22, 23, 0])\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def cap_min_max_values(x_test: pd.DataFrame, y_pred: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Cap the min and max values for each location for each hour.\"\"\"\n",
    "    for hour in range(24):\n",
    "        # Get the min and max values for each location for each hour\n",
    "        min_value_a, max_value_a = get_min_max_values_for_location_at_hour(\"a\", hour)\n",
    "        min_value_b, max_value_b = get_min_max_values_for_location_at_hour(\"b\", hour)\n",
    "        min_value_c, max_value_c = get_min_max_values_for_location_at_hour(\"c\", hour)\n",
    "        print(f\"hour: {hour}, min_value_a: {min_value_a}, max_value_a: {max_value_a}, min_value_b: {min_value_b}, max_value_b: {max_value_b}, min_value_c: {min_value_c}, max_value_c: {max_value_c}\")\n",
    "        # Cap the values between min_value and max_value\n",
    "        y_pred = cap_min_max_values_for_hour(x_test, y_pred, \"a\", hour, min_value_a, max_value_a)\n",
    "        y_pred = cap_min_max_values_for_hour(x_test, y_pred, \"b\", hour, min_value_b, max_value_b)\n",
    "        y_pred = cap_min_max_values_for_hour(x_test, y_pred, \"c\", hour, min_value_c, max_value_c)\n",
    "    return y_pred\n",
    "\n",
    "X_train_obs_combined, X_val_obs_combined, y_train_obs_combined, y_val_obs_combined, X_train_est_combined, X_val_est_combined, y_train_est_combined, y_val_est_combined = fetch_preprocessed_data(drop_features=False)\n",
    "x_whole_with_time = pd.concat([X_train_obs_combined, X_val_obs_combined, X_train_est_combined, X_val_est_combined])\n",
    "\n",
    "def get_min_max_values_for_location_at_hour(location: str, hour: int) -> tuple[float, float]:\n",
    "    \"\"\"Get the min and max values for a specific location at a specific hour.\"\"\"\n",
    "    # Get the x and y for the given hour and location\n",
    "    hour_sin = find_time_sin(hour)\n",
    "    hour_cos = find_time_cos(hour)\n",
    "    # find the min and max values for the given hour and location\n",
    "    min_value = x_whole_with_time[(x_whole_with_time[\"location_\" + location] == 1) & (x_whole_with_time[\"sin_hour\"] == hour_sin) & (x_whole_with_time[\"cos_hour\"] == hour_cos)][\"pv_measurement\"].min()\n",
    "    max_value = x_whole_with_time[(x_whole_with_time[\"location_\" + location] == 1) & (x_whole_with_time[\"sin_hour\"] == hour_sin) & (x_whole_with_time[\"cos_hour\"] == hour_cos)][\"pv_measurement\"].max()\n",
    "    \n",
    "    return (min_value, max_value)\n",
    "\n",
    "def cap_min_max_values_for_hour(x_test: pd.DataFrame, y_pred: pd.DataFrame, location: str, hour: int, min_value: float, max_value: float) -> pd.DataFrame:\n",
    "    \"\"\"Cap the min and max values for a specific hour.\"\"\"\n",
    "    \n",
    "    # Calculate sin and cos values for the given hour\n",
    "    hour_sin = find_time_sin(hour)\n",
    "    hour_cos = find_time_cos(hour)\n",
    "    \n",
    "    # Find indices corresponding to the given hour at the given location\n",
    "    indices = x_test[(x_test[\"location_\" + location] == 1) & (x_test[\"sin_hour\"] == hour_sin) & (x_test[\"cos_hour\"] == hour_cos)].index\n",
    "    \n",
    "    # Cap the values between min_value and max_value\n",
    "    y_pred.loc[indices] = y_pred.loc[indices].clip(min_value, max_value)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def set_0_pv_at_times(x_test: pd.DataFrame, y_pred: pd.DataFrame, location: str, hours: list[int]) -> pd.DataFrame:\n",
    "    \"\"\"Find the correct predicted values at the given times and locaiton and set them to 0.\"\"\"\n",
    "    hours_to_set_0_sin = [find_time_sin(hour) for hour in hours]\n",
    "    hours_to_set_0_cos = [find_time_cos(hour) for hour in hours]\n",
    "\n",
    "\n",
    "    indices = x_test[(x_test[\"location_\" + location] == 1) & (x_test[\"sin_hour\"].isin(hours_to_set_0_sin) & (x_test[\"cos_hour\"].isin(hours_to_set_0_cos)))].index\n",
    "    for index in indices:\n",
    "        y_pred.loc[index] = 0\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole_a = x_test_whole[x_test_whole['location_a'] == 1]\n",
    "x_whole_b = x_test_whole[x_test_whole['location_b'] == 1]\n",
    "x_whole_c = x_test_whole[x_test_whole['location_c'] == 1]\n",
    "\n",
    "x_whole_a = x_whole_a.drop('location_a', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_b', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_c', axis = 1)\n",
    "\n",
    "x_whole_b = x_whole_b.drop('location_a', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_b', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_c', axis = 1)\n",
    "\n",
    "x_whole_c = x_whole_c.drop('location_a', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_b', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_c', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "RES_PATH = 'results/output/'\n",
    "\n",
    "\n",
    "def save_predictions(test: pd.DataFrame, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the 'id' and 'prediction' columns of the test DataFrame to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        test (pd.DataFrame): A 1D DataFrame containing only the predictions.\n",
    "        filename (str): The name of the file where the predictions will be saved.\n",
    "    \"\"\"\n",
    "    model = pd.DataFrame()\n",
    "    \n",
    "    model[\"prediction\"] = test\n",
    "    model['id'] = model.index\n",
    "\n",
    "    model['prediction'] = model['prediction'].apply(lambda x: max(0, x))\n",
    "    \n",
    "    # Reorder the columns to ensure 'id' comes before 'prediction'\n",
    "    model = model[['id', 'prediction']]\n",
    "    \n",
    "\n",
    "    # Save the resulting DataFrame to a CSV file\n",
    "    model.to_csv(f'{RES_PATH}{filename}.csv', index=False)\n",
    "    \n",
    "    # Display the first few rows of the saved DataFrame\n",
    "    print(model.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hour: 0, min_value_a: 0.0, max_value_a: 3.3, min_value_b: -0.0, max_value_b: -0.0, min_value_c: 0.0, max_value_c: 0.0\n",
      "hour: 1, min_value_a: 0.0, max_value_a: 53.68, min_value_b: -0.0, max_value_b: 12.075, min_value_c: 0.0, max_value_c: 9.8\n",
      "hour: 2, min_value_a: 0.0, max_value_a: 233.64000000000001, min_value_b: -0.0, max_value_b: 68.1375, min_value_c: 0.0, max_value_c: 39.2\n",
      "hour: 3, min_value_a: 0.0, max_value_a: 439.12, min_value_b: -0.0, max_value_b: 138.0, min_value_c: 0.0, max_value_c: 88.2\n",
      "hour: 4, min_value_a: 0.0, max_value_a: 1046.98, min_value_b: -0.0, max_value_b: 307.05, min_value_c: 0.0, max_value_c: 176.4\n",
      "hour: 5, min_value_a: 0.0, max_value_a: 2049.08, min_value_b: -0.0, max_value_b: 452.8125, min_value_c: 0.0, max_value_c: 264.6\n",
      "hour: 6, min_value_a: 0.0, max_value_a: 3244.78, min_value_b: -0.0, max_value_b: 681.375, min_value_c: 0.0, max_value_c: 499.8\n",
      "hour: 7, min_value_a: 0.0, max_value_a: 4266.46, min_value_b: -0.0, max_value_b: 865.0875, min_value_c: 0.0, max_value_c: 705.6\n",
      "hour: 8, min_value_a: 0.0, max_value_a: 5048.780000000001, min_value_b: -0.0, max_value_b: 997.9125, min_value_c: 0.0, max_value_c: 784.0\n",
      "hour: 9, min_value_a: 0.0, max_value_a: 5477.339999999999, min_value_b: -0.0, max_value_b: 1091.925, min_value_c: 0.0, max_value_c: 882.0000000000001\n",
      "hour: 10, min_value_a: 0.0, max_value_a: 5733.42, min_value_b: -0.0, max_value_b: 1146.2625, min_value_c: 0.0, max_value_c: 980.0000000000001\n",
      "hour: 11, min_value_a: 0.0, max_value_a: 5651.8, min_value_b: -0.0, max_value_b: 1120.3875, min_value_c: 0.0, max_value_c: 999.6\n",
      "hour: 12, min_value_a: 0.0, max_value_a: 5344.46, min_value_b: 0.0, max_value_b: 1152.3, min_value_c: 0.0, max_value_c: 950.6\n",
      "hour: 13, min_value_a: 0.0, max_value_a: 5058.900000000001, min_value_b: -0.0, max_value_b: 1053.9750000000001, min_value_c: 0.0, max_value_c: 911.4000000000001\n",
      "hour: 14, min_value_a: 0.0, max_value_a: 4491.3, min_value_b: -0.0, max_value_b: 823.6875, min_value_c: 0.0, max_value_c: 803.6\n",
      "hour: 15, min_value_a: 0.0, max_value_a: 3633.96, min_value_b: -0.0, max_value_b: 723.6375, min_value_c: 0.0, max_value_c: 597.8000000000001\n",
      "hour: 16, min_value_a: 0.0, max_value_a: 2710.4, min_value_b: -0.0, max_value_b: 601.1625, min_value_c: 0.0, max_value_c: 494.90000000000003\n",
      "hour: 17, min_value_a: 0.0, max_value_a: 1606.0, min_value_b: -0.0, max_value_b: 363.1125, min_value_c: 0.0, max_value_c: 303.8\n",
      "hour: 18, min_value_a: 0.0, max_value_a: 849.64, min_value_b: -0.0, max_value_b: 242.3625, min_value_c: 0.0, max_value_c: 176.4\n",
      "hour: 19, min_value_a: 0.0, max_value_a: 554.4, min_value_b: -0.0, max_value_b: 145.7625, min_value_c: 0.0, max_value_c: 107.80000000000001\n",
      "hour: 20, min_value_a: 0.0, max_value_a: 197.12, min_value_b: -0.0, max_value_b: 33.637499999999996, min_value_c: 0.0, max_value_c: 29.400000000000002\n",
      "hour: 21, min_value_a: 0.0, max_value_a: 19.139999999999997, min_value_b: -0.0, max_value_b: 13.8, min_value_c: 0.0, max_value_c: 9.8\n",
      "hour: 22, min_value_a: 0.0, max_value_a: 0.66, min_value_b: -0.0, max_value_b: -0.0, min_value_c: -0.0, max_value_c: -0.0\n",
      "hour: 23, min_value_a: 0.0, max_value_a: 0.0, min_value_b: -0.0, max_value_b: -0.0, min_value_c: -0.0, max_value_c: -0.0\n",
      "   id  prediction\n",
      "0   0    0.000000\n",
      "1   1    0.000000\n",
      "2   2    1.452000\n",
      "3   3   39.776001\n",
      "4   4  164.516006\n"
     ]
    }
   ],
   "source": [
    "y_predictions_a = best_model_a.predict(x_whole_a)\n",
    "y_predictions_b = best_model_b.predict(x_whole_b)\n",
    "y_predictions_c = best_model_c.predict(x_whole_c)\n",
    "y_predictions = pd.concat([pd.Series(y_predictions_a), pd.Series(y_predictions_b), pd.Series(y_predictions_c)])\n",
    "y_predictions = y_predictions.reset_index(drop=True)\n",
    "# Save the model\n",
    "y_predictions_autogluon = postprocess_data(x_test_whole, pd.DataFrame(y_predictions))\n",
    "save_predictions(y_predictions, 'autogluon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catboost model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole[\"pv_measurement\"] = y_whole\n",
    "df_shuffled = x_whole.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "x_whole_a = df_shuffled[df_shuffled['location_a'] == 1]\n",
    "x_whole_b = df_shuffled[df_shuffled['location_b'] == 1]\n",
    "x_whole_c = df_shuffled[df_shuffled['location_c'] == 1]\n",
    "\n",
    "y_whole_a = x_whole_a[\"pv_measurement\"]\n",
    "x_whole_a = x_whole_a.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_a', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_b', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_c', axis = 1)\n",
    "\n",
    "y_whole_b = x_whole_b[\"pv_measurement\"]\n",
    "x_whole_b = x_whole_b.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_a', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_b', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_c', axis = 1)\n",
    "\n",
    "y_whole_c = x_whole_c[\"pv_measurement\"]\n",
    "x_whole_c = x_whole_c.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_a', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_b', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_c', axis = 1)\n",
    "cat_features = [\"estimated_flag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 615.2289223\ttotal: 187ms\tremaining: 3m 6s\n",
      "100:\tlearn: 201.4072982\ttotal: 4.44s\tremaining: 39.5s\n",
      "200:\tlearn: 186.0497580\ttotal: 8.63s\tremaining: 34.3s\n",
      "300:\tlearn: 181.1303390\ttotal: 12.6s\tremaining: 29.3s\n",
      "400:\tlearn: 177.2485579\ttotal: 16.5s\tremaining: 24.6s\n",
      "500:\tlearn: 172.6623044\ttotal: 20.7s\tremaining: 20.6s\n",
      "600:\tlearn: 163.7204714\ttotal: 24.7s\tremaining: 16.4s\n",
      "700:\tlearn: 154.4498268\ttotal: 28.9s\tremaining: 12.3s\n",
      "800:\tlearn: 146.6204772\ttotal: 32.9s\tremaining: 8.17s\n",
      "900:\tlearn: 139.9470005\ttotal: 36.9s\tremaining: 4.06s\n",
      "999:\tlearn: 134.5556506\ttotal: 41s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x26584bf2b00>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_a = CatBoostRegressor(\n",
    "    max_depth=9,\n",
    "    cat_features=cat_features,\n",
    "    loss_function=\"MAE\",\n",
    "    verbose = 100\n",
    ")\n",
    "best_model_a.fit(x_whole_a, y_whole_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 94.2791670\ttotal: 43.4ms\tremaining: 43.4s\n",
      "100:\tlearn: 25.1394341\ttotal: 4.21s\tremaining: 37.5s\n",
      "200:\tlearn: 21.7299397\ttotal: 8.34s\tremaining: 33.2s\n",
      "300:\tlearn: 20.2992389\ttotal: 12.3s\tremaining: 28.5s\n",
      "400:\tlearn: 19.0627633\ttotal: 16.2s\tremaining: 24.2s\n",
      "500:\tlearn: 17.9527669\ttotal: 20.1s\tremaining: 20s\n",
      "600:\tlearn: 17.1682651\ttotal: 24.1s\tremaining: 16s\n",
      "700:\tlearn: 16.6932226\ttotal: 27.9s\tremaining: 11.9s\n",
      "800:\tlearn: 15.8845163\ttotal: 31.8s\tremaining: 7.89s\n",
      "900:\tlearn: 15.3506132\ttotal: 35.7s\tremaining: 3.92s\n",
      "999:\tlearn: 14.9515366\ttotal: 39.5s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x26584bf3220>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_b = CatBoostRegressor(\n",
    "    max_depth=9,\n",
    "    cat_features=cat_features,\n",
    "    loss_function=\"MAE\",\n",
    "    verbose = 100\n",
    ")\n",
    "best_model_b.fit(x_whole_b, y_whole_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 78.8208115\ttotal: 23ms\tremaining: 23s\n",
      "100:\tlearn: 21.4396391\ttotal: 4.01s\tremaining: 35.7s\n",
      "200:\tlearn: 18.5151414\ttotal: 8.03s\tremaining: 31.9s\n",
      "300:\tlearn: 17.0987782\ttotal: 12s\tremaining: 27.9s\n",
      "400:\tlearn: 16.0406782\ttotal: 15.9s\tremaining: 23.7s\n",
      "500:\tlearn: 15.0533016\ttotal: 19.8s\tremaining: 19.7s\n",
      "600:\tlearn: 14.2314945\ttotal: 23.7s\tremaining: 15.7s\n",
      "700:\tlearn: 13.6329705\ttotal: 27.6s\tremaining: 11.8s\n",
      "800:\tlearn: 13.1507093\ttotal: 31.7s\tremaining: 7.87s\n",
      "900:\tlearn: 12.7614489\ttotal: 35.7s\tremaining: 3.92s\n",
      "999:\tlearn: 12.4840959\ttotal: 39.6s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x26584bf32b0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_c = CatBoostRegressor(\n",
    "    max_depth=9,\n",
    "    cat_features=cat_features,\n",
    "    loss_function = \"MAE\",\n",
    "    verbose = 100\n",
    ")\n",
    "best_model_c.fit(x_whole_c, y_whole_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole_a = x_test_whole[x_test_whole['location_a'] == 1]\n",
    "x_whole_b = x_test_whole[x_test_whole['location_b'] == 1]\n",
    "x_whole_c = x_test_whole[x_test_whole['location_c'] == 1]\n",
    "\n",
    "x_whole_a = x_whole_a.drop('location_a', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_b', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_c', axis = 1)\n",
    "\n",
    "x_whole_b = x_whole_b.drop('location_a', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_b', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_c', axis = 1)\n",
    "\n",
    "x_whole_c = x_whole_c.drop('location_a', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_b', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_c', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hour: 0, min_value_a: 0.0, max_value_a: 3.3, min_value_b: -0.0, max_value_b: -0.0, min_value_c: 0.0, max_value_c: 0.0\n",
      "hour: 1, min_value_a: 0.0, max_value_a: 53.68, min_value_b: -0.0, max_value_b: 12.075, min_value_c: 0.0, max_value_c: 9.8\n",
      "hour: 2, min_value_a: 0.0, max_value_a: 233.64000000000001, min_value_b: -0.0, max_value_b: 68.1375, min_value_c: 0.0, max_value_c: 39.2\n",
      "hour: 3, min_value_a: 0.0, max_value_a: 439.12, min_value_b: -0.0, max_value_b: 138.0, min_value_c: 0.0, max_value_c: 88.2\n",
      "hour: 4, min_value_a: 0.0, max_value_a: 1046.98, min_value_b: -0.0, max_value_b: 307.05, min_value_c: 0.0, max_value_c: 176.4\n",
      "hour: 5, min_value_a: 0.0, max_value_a: 2049.08, min_value_b: -0.0, max_value_b: 452.8125, min_value_c: 0.0, max_value_c: 264.6\n",
      "hour: 6, min_value_a: 0.0, max_value_a: 3244.78, min_value_b: -0.0, max_value_b: 681.375, min_value_c: 0.0, max_value_c: 499.8\n",
      "hour: 7, min_value_a: 0.0, max_value_a: 4266.46, min_value_b: -0.0, max_value_b: 865.0875, min_value_c: 0.0, max_value_c: 705.6\n",
      "hour: 8, min_value_a: 0.0, max_value_a: 5048.780000000001, min_value_b: -0.0, max_value_b: 997.9125, min_value_c: 0.0, max_value_c: 784.0\n",
      "hour: 9, min_value_a: 0.0, max_value_a: 5477.339999999999, min_value_b: -0.0, max_value_b: 1091.925, min_value_c: 0.0, max_value_c: 882.0000000000001\n",
      "hour: 10, min_value_a: 0.0, max_value_a: 5733.42, min_value_b: -0.0, max_value_b: 1146.2625, min_value_c: 0.0, max_value_c: 980.0000000000001\n",
      "hour: 11, min_value_a: 0.0, max_value_a: 5651.8, min_value_b: -0.0, max_value_b: 1120.3875, min_value_c: 0.0, max_value_c: 999.6\n",
      "hour: 12, min_value_a: 0.0, max_value_a: 5344.46, min_value_b: 0.0, max_value_b: 1152.3, min_value_c: 0.0, max_value_c: 950.6\n",
      "hour: 13, min_value_a: 0.0, max_value_a: 5058.900000000001, min_value_b: -0.0, max_value_b: 1053.9750000000001, min_value_c: 0.0, max_value_c: 911.4000000000001\n",
      "hour: 14, min_value_a: 0.0, max_value_a: 4491.3, min_value_b: -0.0, max_value_b: 823.6875, min_value_c: 0.0, max_value_c: 803.6\n",
      "hour: 15, min_value_a: 0.0, max_value_a: 3633.96, min_value_b: -0.0, max_value_b: 723.6375, min_value_c: 0.0, max_value_c: 597.8000000000001\n",
      "hour: 16, min_value_a: 0.0, max_value_a: 2710.4, min_value_b: -0.0, max_value_b: 601.1625, min_value_c: 0.0, max_value_c: 494.90000000000003\n",
      "hour: 17, min_value_a: 0.0, max_value_a: 1606.0, min_value_b: -0.0, max_value_b: 363.1125, min_value_c: 0.0, max_value_c: 303.8\n",
      "hour: 18, min_value_a: 0.0, max_value_a: 849.64, min_value_b: -0.0, max_value_b: 242.3625, min_value_c: 0.0, max_value_c: 176.4\n",
      "hour: 19, min_value_a: 0.0, max_value_a: 554.4, min_value_b: -0.0, max_value_b: 145.7625, min_value_c: 0.0, max_value_c: 107.80000000000001\n",
      "hour: 20, min_value_a: 0.0, max_value_a: 197.12, min_value_b: -0.0, max_value_b: 33.637499999999996, min_value_c: 0.0, max_value_c: 29.400000000000002\n",
      "hour: 21, min_value_a: 0.0, max_value_a: 19.139999999999997, min_value_b: -0.0, max_value_b: 13.8, min_value_c: 0.0, max_value_c: 9.8\n",
      "hour: 22, min_value_a: 0.0, max_value_a: 0.66, min_value_b: -0.0, max_value_b: -0.0, min_value_c: -0.0, max_value_c: -0.0\n",
      "hour: 23, min_value_a: 0.0, max_value_a: 0.0, min_value_b: -0.0, max_value_b: -0.0, min_value_c: -0.0, max_value_c: -0.0\n"
     ]
    }
   ],
   "source": [
    "y_predictions_a = best_model_a.predict(x_whole_a)\n",
    "y_predictions_b = best_model_b.predict(x_whole_b)\n",
    "y_predictions_c = best_model_c.predict(x_whole_c)\n",
    "y_predictions = pd.concat([pd.Series(y_predictions_a), pd.Series(y_predictions_b), pd.Series(y_predictions_c)])\n",
    "y_predictions = y_predictions.reset_index(drop=True)\n",
    "\n",
    "# Save the model\n",
    "y_predictions_catboost_1 = postprocess_data(x_test_whole, pd.DataFrame(y_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catboost model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole = x_whole.drop(\"effective_solar_elevation_squared\", axis=1)\n",
    "x_test_whole = x_test_whole.drop(\"effective_solar_elevation_squared\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole[\"pv_measurement\"] = y_whole\n",
    "df_shuffled = x_whole.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "x_whole_a = df_shuffled[df_shuffled['location_a'] == 1]\n",
    "x_whole_b = df_shuffled[df_shuffled['location_b'] == 1]\n",
    "x_whole_c = df_shuffled[df_shuffled['location_c'] == 1]\n",
    "\n",
    "y_whole_a = x_whole_a[\"pv_measurement\"]\n",
    "x_whole_a = x_whole_a.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_a', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_b', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_c', axis = 1)\n",
    "\n",
    "y_whole_b = x_whole_b[\"pv_measurement\"]\n",
    "x_whole_b = x_whole_b.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_a', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_b', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_c', axis = 1)\n",
    "\n",
    "y_whole_c = x_whole_c[\"pv_measurement\"]\n",
    "x_whole_c = x_whole_c.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_a', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_b', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_c', axis = 1)\n",
    "cat_features = [\"estimated_flag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 616.9363093\ttotal: 46.8ms\tremaining: 46.8s\n",
      "100:\tlearn: 199.9656401\ttotal: 4.05s\tremaining: 36.1s\n",
      "200:\tlearn: 185.3356686\ttotal: 8.01s\tremaining: 31.8s\n",
      "300:\tlearn: 180.5070612\ttotal: 11.7s\tremaining: 27.2s\n",
      "400:\tlearn: 176.7196537\ttotal: 15.5s\tremaining: 23.2s\n",
      "500:\tlearn: 172.6197441\ttotal: 19.5s\tremaining: 19.5s\n",
      "600:\tlearn: 163.7581476\ttotal: 23.5s\tremaining: 15.6s\n",
      "700:\tlearn: 153.8238135\ttotal: 27.4s\tremaining: 11.7s\n",
      "800:\tlearn: 147.6203702\ttotal: 31.4s\tremaining: 7.8s\n",
      "900:\tlearn: 141.6572310\ttotal: 35.3s\tremaining: 3.88s\n",
      "999:\tlearn: 136.7908527\ttotal: 39.3s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x26584ed9540>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_a = CatBoostRegressor(\n",
    "    max_depth=9,\n",
    "    cat_features=cat_features,\n",
    "    loss_function=\"MAE\",\n",
    "    verbose = 100\n",
    ")\n",
    "best_model_a.fit(x_whole_a, y_whole_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 94.1836286\ttotal: 37.9ms\tremaining: 37.9s\n",
      "100:\tlearn: 25.3949714\ttotal: 4.04s\tremaining: 36s\n",
      "200:\tlearn: 22.1209997\ttotal: 7.99s\tremaining: 31.7s\n",
      "300:\tlearn: 20.7702607\ttotal: 11.9s\tremaining: 27.6s\n",
      "400:\tlearn: 19.4111036\ttotal: 15.7s\tremaining: 23.4s\n",
      "500:\tlearn: 18.1584056\ttotal: 19.5s\tremaining: 19.4s\n",
      "600:\tlearn: 17.4038913\ttotal: 23.3s\tremaining: 15.5s\n",
      "700:\tlearn: 16.7939747\ttotal: 27.2s\tremaining: 11.6s\n",
      "800:\tlearn: 16.1720278\ttotal: 31s\tremaining: 7.7s\n",
      "900:\tlearn: 15.6136213\ttotal: 35.1s\tremaining: 3.86s\n",
      "999:\tlearn: 15.1423810\ttotal: 39.3s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x26584eda710>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_b = CatBoostRegressor(\n",
    "    max_depth=9,\n",
    "    cat_features=cat_features,\n",
    "    loss_function = \"MAE\",\n",
    "    verbose = 100\n",
    ")\n",
    "best_model_b.fit(x_whole_b, y_whole_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 78.7273693\ttotal: 46.1ms\tremaining: 46.1s\n",
      "100:\tlearn: 21.3078061\ttotal: 4.06s\tremaining: 36.2s\n",
      "200:\tlearn: 18.2258783\ttotal: 8.08s\tremaining: 32.1s\n",
      "300:\tlearn: 17.0883950\ttotal: 12.1s\tremaining: 28s\n",
      "400:\tlearn: 16.1611729\ttotal: 16.1s\tremaining: 24s\n",
      "500:\tlearn: 15.2436549\ttotal: 20s\tremaining: 20s\n",
      "600:\tlearn: 14.4853484\ttotal: 24s\tremaining: 16s\n",
      "700:\tlearn: 13.7033024\ttotal: 28s\tremaining: 11.9s\n",
      "800:\tlearn: 13.1848967\ttotal: 32.1s\tremaining: 7.96s\n",
      "900:\tlearn: 12.7181220\ttotal: 36.1s\tremaining: 3.96s\n",
      "999:\tlearn: 12.3918690\ttotal: 39.8s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x26584ed9690>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_c = CatBoostRegressor(\n",
    "    max_depth=9,\n",
    "    cat_features=cat_features,\n",
    "    loss_function = \"MAE\",\n",
    "    verbose = 100\n",
    ")\n",
    "best_model_c.fit(x_whole_c, y_whole_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole_a = x_test_whole[x_test_whole['location_a'] == 1]\n",
    "x_whole_b = x_test_whole[x_test_whole['location_b'] == 1]\n",
    "x_whole_c = x_test_whole[x_test_whole['location_c'] == 1]\n",
    "\n",
    "x_whole_a = x_whole_a.drop('location_a', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_b', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_c', axis = 1)\n",
    "\n",
    "x_whole_b = x_whole_b.drop('location_a', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_b', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_c', axis = 1)\n",
    "\n",
    "x_whole_c = x_whole_c.drop('location_a', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_b', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_c', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hour: 0, min_value_a: 0.0, max_value_a: 3.3, min_value_b: -0.0, max_value_b: -0.0, min_value_c: 0.0, max_value_c: 0.0\n",
      "hour: 1, min_value_a: 0.0, max_value_a: 53.68, min_value_b: -0.0, max_value_b: 12.075, min_value_c: 0.0, max_value_c: 9.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hour: 2, min_value_a: 0.0, max_value_a: 233.64000000000001, min_value_b: -0.0, max_value_b: 68.1375, min_value_c: 0.0, max_value_c: 39.2\n",
      "hour: 3, min_value_a: 0.0, max_value_a: 439.12, min_value_b: -0.0, max_value_b: 138.0, min_value_c: 0.0, max_value_c: 88.2\n",
      "hour: 4, min_value_a: 0.0, max_value_a: 1046.98, min_value_b: -0.0, max_value_b: 307.05, min_value_c: 0.0, max_value_c: 176.4\n",
      "hour: 5, min_value_a: 0.0, max_value_a: 2049.08, min_value_b: -0.0, max_value_b: 452.8125, min_value_c: 0.0, max_value_c: 264.6\n",
      "hour: 6, min_value_a: 0.0, max_value_a: 3244.78, min_value_b: -0.0, max_value_b: 681.375, min_value_c: 0.0, max_value_c: 499.8\n",
      "hour: 7, min_value_a: 0.0, max_value_a: 4266.46, min_value_b: -0.0, max_value_b: 865.0875, min_value_c: 0.0, max_value_c: 705.6\n",
      "hour: 8, min_value_a: 0.0, max_value_a: 5048.780000000001, min_value_b: -0.0, max_value_b: 997.9125, min_value_c: 0.0, max_value_c: 784.0\n",
      "hour: 9, min_value_a: 0.0, max_value_a: 5477.339999999999, min_value_b: -0.0, max_value_b: 1091.925, min_value_c: 0.0, max_value_c: 882.0000000000001\n",
      "hour: 10, min_value_a: 0.0, max_value_a: 5733.42, min_value_b: -0.0, max_value_b: 1146.2625, min_value_c: 0.0, max_value_c: 980.0000000000001\n",
      "hour: 11, min_value_a: 0.0, max_value_a: 5651.8, min_value_b: -0.0, max_value_b: 1120.3875, min_value_c: 0.0, max_value_c: 999.6\n",
      "hour: 12, min_value_a: 0.0, max_value_a: 5344.46, min_value_b: 0.0, max_value_b: 1152.3, min_value_c: 0.0, max_value_c: 950.6\n",
      "hour: 13, min_value_a: 0.0, max_value_a: 5058.900000000001, min_value_b: -0.0, max_value_b: 1053.9750000000001, min_value_c: 0.0, max_value_c: 911.4000000000001\n",
      "hour: 14, min_value_a: 0.0, max_value_a: 4491.3, min_value_b: -0.0, max_value_b: 823.6875, min_value_c: 0.0, max_value_c: 803.6\n",
      "hour: 15, min_value_a: 0.0, max_value_a: 3633.96, min_value_b: -0.0, max_value_b: 723.6375, min_value_c: 0.0, max_value_c: 597.8000000000001\n",
      "hour: 16, min_value_a: 0.0, max_value_a: 2710.4, min_value_b: -0.0, max_value_b: 601.1625, min_value_c: 0.0, max_value_c: 494.90000000000003\n",
      "hour: 17, min_value_a: 0.0, max_value_a: 1606.0, min_value_b: -0.0, max_value_b: 363.1125, min_value_c: 0.0, max_value_c: 303.8\n",
      "hour: 18, min_value_a: 0.0, max_value_a: 849.64, min_value_b: -0.0, max_value_b: 242.3625, min_value_c: 0.0, max_value_c: 176.4\n",
      "hour: 19, min_value_a: 0.0, max_value_a: 554.4, min_value_b: -0.0, max_value_b: 145.7625, min_value_c: 0.0, max_value_c: 107.80000000000001\n",
      "hour: 20, min_value_a: 0.0, max_value_a: 197.12, min_value_b: -0.0, max_value_b: 33.637499999999996, min_value_c: 0.0, max_value_c: 29.400000000000002\n",
      "hour: 21, min_value_a: 0.0, max_value_a: 19.139999999999997, min_value_b: -0.0, max_value_b: 13.8, min_value_c: 0.0, max_value_c: 9.8\n",
      "hour: 22, min_value_a: 0.0, max_value_a: 0.66, min_value_b: -0.0, max_value_b: -0.0, min_value_c: -0.0, max_value_c: -0.0\n",
      "hour: 23, min_value_a: 0.0, max_value_a: 0.0, min_value_b: -0.0, max_value_b: -0.0, min_value_c: -0.0, max_value_c: -0.0\n",
      "   id    prediction\n",
      "0   0  0.000000e+00\n",
      "1   1  5.292587e-07\n",
      "2   2  5.308785e-07\n",
      "3   3  4.747112e+01\n",
      "4   4  3.363666e+02\n"
     ]
    }
   ],
   "source": [
    "y_predictions_a = best_model_a.predict(x_whole_a)\n",
    "y_predictions_b = best_model_b.predict(x_whole_b)\n",
    "y_predictions_c = best_model_c.predict(x_whole_c)\n",
    "y_predictions = pd.concat([pd.Series(y_predictions_a), pd.Series(y_predictions_b), pd.Series(y_predictions_c)])\n",
    "y_predictions = y_predictions.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Save the model\n",
    "y_predictions_catboost_2 = postprocess_data(x_test_whole, pd.DataFrame(y_predictions))\n",
    "save_predictions(y_predictions, 'catboost without modified solar elevation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id    prediction\n",
      "0   0  0.000000e+00\n",
      "1   1  1.764196e-07\n",
      "2   2  4.840002e-01\n",
      "3   3  5.123574e+01\n",
      "4   4  2.740459e+02\n"
     ]
    }
   ],
   "source": [
    "y_predictions_autogluon.rename(columns={'existing_column_name': 'pred_autogluon'}, inplace=True)\n",
    "y_predictions_catboost_1.rename(columns={'existing_column_name': 'pred_catboost_1'}, inplace=True)\n",
    "y_predictions_catboost_2.rename(columns={'existing_column_name': 'pred_catboost_2'}, inplace=True)\n",
    "\n",
    "combined_df = pd.concat([y_predictions_autogluon, y_predictions_catboost_1, y_predictions_catboost_2], axis=1)\n",
    "\n",
    "\n",
    "combined_df['average_prediction'] = combined_df.mean(axis=1)\n",
    "\n",
    "save_predictions(combined_df[\"average_prediction\"], \"average autogluon cat stack 1435\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
