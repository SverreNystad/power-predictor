{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost stack and autogluon stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autogluon in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (0.8.2)\n",
      "Requirement already satisfied: autogluon.features==0.8.2 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon) (0.8.2)\n",
      "Requirement already satisfied: autogluon.timeseries[all]==0.8.2 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon) (0.8.2)\n",
      "Requirement already satisfied: autogluon.multimodal==0.8.2 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon) (0.8.2)\n",
      "Requirement already satisfied: autogluon.tabular[all]==0.8.2 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon) (0.8.2)\n",
      "Requirement already satisfied: autogluon.core[all]==0.8.2 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon) (0.8.2)\n",
      "Requirement already satisfied: numpy<1.27,>=1.21 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (1.26.0)\n",
      "Requirement already satisfied: networkx<4,>=3.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (3.2.1)\n",
      "Requirement already satisfied: pandas<1.6,>=1.4.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (1.5.3)\n",
      "Requirement already satisfied: scipy<1.12,>=1.5.4 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (1.11.2)\n",
      "Requirement already satisfied: boto3<2,>=1.10 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (1.28.80)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (3.8.0)\n",
      "Requirement already satisfied: scikit-learn<1.3,>=1.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (1.2.2)\n",
      "Requirement already satisfied: tqdm<5,>=4.38 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (4.65.2)\n",
      "Requirement already satisfied: autogluon.common==0.8.2 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (0.8.2)\n",
      "Requirement already satisfied: requests in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (2.28.2)\n",
      "Requirement already satisfied: hyperopt<0.2.8,>=0.2.7 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (0.2.7)\n",
      "Requirement already satisfied: ray[default]<2.4,>=2.3 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (2.3.1)\n",
      "Requirement already satisfied: pydantic<2.0,>=1.10.4 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (1.10.13)\n",
      "Requirement already satisfied: grpcio<=1.50.0,>=1.42.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.core[all]==0.8.2->autogluon) (1.50.0)\n",
      "Requirement already satisfied: psutil<6,>=5.7.3 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.common==0.8.2->autogluon.core[all]==0.8.2->autogluon) (5.9.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.common==0.8.2->autogluon.core[all]==0.8.2->autogluon) (60.2.0)\n",
      "Requirement already satisfied: defusedxml<0.7.2,>=0.7.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.7.1)\n",
      "Requirement already satisfied: timm<0.10.0,>=0.9.2 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.9.10)\n",
      "Requirement already satisfied: text-unidecode<1.4,>=1.3 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (1.3)\n",
      "Requirement already satisfied: jsonschema<4.18,>=4.14 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (4.17.3)\n",
      "Requirement already satisfied: Pillow<9.6,>=9.3 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (9.5.0)\n",
      "Requirement already satisfied: pytorch-metric-learning<2.0,>=1.3.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (1.7.3)\n",
      "Requirement already satisfied: pytesseract<0.3.11,>=0.3.9 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.3.10)\n",
      "Requirement already satisfied: tensorboard<3,>=2.9 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (2.15.1)\n",
      "Requirement already satisfied: nptyping<2.5.0,>=1.4.4 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (2.4.1)\n",
      "Requirement already satisfied: nlpaug<1.2.0,>=1.1.10 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (1.1.11)\n",
      "Requirement already satisfied: jinja2<3.2,>=3.0.3 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (3.1.2)\n",
      "Requirement already satisfied: torchvision<0.15.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.14.1)\n",
      "Requirement already satisfied: pytorch-lightning<1.10.0,>=1.9.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (1.9.5)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.4.5 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (3.8.1)\n",
      "Requirement already satisfied: omegaconf<2.3.0,>=2.1.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (2.2.3)\n",
      "Requirement already satisfied: evaluate<0.4.0,>=0.2.2 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.3.0)\n",
      "Requirement already satisfied: seqeval<1.3.0,>=1.2.2 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (1.2.2)\n",
      "Requirement already satisfied: transformers[sentencepiece]<4.27.0,>=4.23.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (4.26.1)\n",
      "Requirement already satisfied: openmim<0.4.0,>=0.3.7 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.3.9)\n",
      "Requirement already satisfied: torchmetrics<0.12.0,>=0.11.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.11.4)\n",
      "Requirement already satisfied: accelerate<0.17,>=0.9 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.16.0)\n",
      "Requirement already satisfied: torch<1.14,>=1.9 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (1.13.1)\n",
      "Requirement already satisfied: scikit-image<0.20.0,>=0.19.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.multimodal==0.8.2->autogluon) (0.19.3)\n",
      "Requirement already satisfied: catboost<1.3,>=1.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.tabular[all]==0.8.2->autogluon) (1.2.2)\n",
      "Requirement already satisfied: lightgbm<3.4,>=3.3 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.tabular[all]==0.8.2->autogluon) (3.3.5)\n",
      "Requirement already satisfied: fastai<2.8,>=2.3.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.tabular[all]==0.8.2->autogluon) (2.7.13)\n",
      "Requirement already satisfied: xgboost<1.8,>=1.6 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.tabular[all]==0.8.2->autogluon) (1.7.6)\n",
      "Requirement already satisfied: mlforecast<0.7.4,>=0.7.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.timeseries[all]==0.8.2->autogluon) (0.7.3)\n",
      "Requirement already satisfied: statsmodels<0.15,>=0.13.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.timeseries[all]==0.8.2->autogluon) (0.14.0)\n",
      "Requirement already satisfied: joblib<2,>=1.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.timeseries[all]==0.8.2->autogluon) (1.3.2)\n",
      "Requirement already satisfied: gluonts<0.14,>=0.13.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.timeseries[all]==0.8.2->autogluon) (0.13.7)\n",
      "Requirement already satisfied: ujson<6,>=5 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.timeseries[all]==0.8.2->autogluon) (5.8.0)\n",
      "Requirement already satisfied: statsforecast<1.5,>=1.4.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from autogluon.timeseries[all]==0.8.2->autogluon) (1.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from accelerate<0.17,>=0.9->autogluon.multimodal==0.8.2->autogluon) (23.1)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from accelerate<0.17,>=0.9->autogluon.multimodal==0.8.2->autogluon) (6.0.1)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from boto3<2,>=1.10->autogluon.core[all]==0.8.2->autogluon) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.8.0,>=0.7.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from boto3<2,>=1.10->autogluon.core[all]==0.8.2->autogluon) (0.7.0)\n",
      "Requirement already satisfied: botocore<1.32.0,>=1.31.80 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from boto3<2,>=1.10->autogluon.core[all]==0.8.2->autogluon) (1.31.80)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from botocore<1.32.0,>=1.31.80->boto3<2,>=1.10->autogluon.core[all]==0.8.2->autogluon) (2.8.2)\n",
      "Requirement already satisfied: urllib3<2.1,>=1.25.4 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from botocore<1.32.0,>=1.31.80->boto3<2,>=1.10->autogluon.core[all]==0.8.2->autogluon) (1.26.18)\n",
      "Requirement already satisfied: six in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==0.8.2->autogluon) (1.16.0)\n",
      "Requirement already satisfied: graphviz in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==0.8.2->autogluon) (0.20.1)\n",
      "Requirement already satisfied: plotly in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from catboost<1.3,>=1.1->autogluon.tabular[all]==0.8.2->autogluon) (5.18.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (2.14.6)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (0.19.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (2023.10.0)\n",
      "Requirement already satisfied: dill in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (0.3.7)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (0.70.15)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (0.18.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (3.4.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from datasets>=2.0.0->evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (3.8.6)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from datasets>=2.0.0->evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (13.0.0)\n",
      "Requirement already satisfied: fastcore<1.6,>=1.5.29 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (1.5.29)\n",
      "Requirement already satisfied: pip in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (21.2.3)\n",
      "Requirement already satisfied: fastprogress>=0.2.4 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (1.0.3)\n",
      "Requirement already satisfied: spacy<4 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (3.7.2)\n",
      "Requirement already satisfied: fastdownload<2,>=0.0.5 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (0.0.7)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (1.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (1.9.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (3.2.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from gluonts<0.14,>=0.13.1->autogluon.timeseries[all]==0.8.2->autogluon) (4.8.0)\n",
      "Requirement already satisfied: toolz~=0.10 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from gluonts<0.14,>=0.13.1->autogluon.timeseries[all]==0.8.2->autogluon) (0.12.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate<0.4.0,>=0.2.2->autogluon.multimodal==0.8.2->autogluon) (3.13.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==0.8.2->autogluon) (3.0.0)\n",
      "Requirement already satisfied: py4j in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==0.8.2->autogluon) (0.10.9.7)\n",
      "Requirement already satisfied: future in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==0.8.2->autogluon) (0.18.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from jinja2<3.2,>=3.0.3->autogluon.multimodal==0.8.2->autogluon) (2.1.3)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from jsonschema<4.18,>=4.14->autogluon.multimodal==0.8.2->autogluon) (0.20.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from lightgbm<3.4,>=3.3->autogluon.tabular[all]==0.8.2->autogluon) (0.41.3)\n",
      "Requirement already satisfied: window-ops in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from mlforecast<0.7.4,>=0.7.0->autogluon.timeseries[all]==0.8.2->autogluon) (0.0.14)\n",
      "Requirement already satisfied: numba in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from mlforecast<0.7.4,>=0.7.0->autogluon.timeseries[all]==0.8.2->autogluon) (0.58.1)\n",
      "Requirement already satisfied: gdown>=4.0.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==0.8.2->autogluon) (4.7.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==0.8.2->autogluon) (4.12.2)\n",
      "Requirement already satisfied: click in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==0.8.2->autogluon) (8.1.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from nltk<4.0.0,>=3.4.5->autogluon.multimodal==0.8.2->autogluon) (2023.10.3)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from omegaconf<2.3.0,>=2.1.1->autogluon.multimodal==0.8.2->autogluon) (4.9.3)\n",
      "Requirement already satisfied: tabulate in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (0.9.0)\n",
      "Requirement already satisfied: model-index in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (0.1.11)\n",
      "Requirement already satisfied: rich in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (13.4.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (0.4.6)\n",
      "Requirement already satisfied: opendatalab in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (0.0.10)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from pandas<1.6,>=1.4.1->autogluon.core[all]==0.8.2->autogluon) (2023.3.post1)\n",
      "Requirement already satisfied: lightning-utilities>=0.6.0.post0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from pytorch-lightning<1.10.0,>=1.9.0->autogluon.multimodal==0.8.2->autogluon) (0.9.0)\n",
      "Requirement already satisfied: virtualenv>=20.0.24 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (20.24.6)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (1.0.7)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (3.20.2)\n",
      "Requirement already satisfied: py-spy>=0.2.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (0.3.14)\n",
      "Requirement already satisfied: colorful in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (0.5.5)\n",
      "Requirement already satisfied: smart-open in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (6.4.0)\n",
      "Requirement already satisfied: gpustat>=1.0.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (1.0.0)\n",
      "Requirement already satisfied: prometheus-client>=0.7.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (0.17.1)\n",
      "Requirement already satisfied: aiohttp-cors in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (0.7.0)\n",
      "Requirement already satisfied: opencensus in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (0.11.3)\n",
      "Requirement already satisfied: nvidia-ml-py<=11.495.46,>=11.450.129 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from gpustat>=1.0.0->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (11.495.46)\n",
      "Requirement already satisfied: blessed>=1.17.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from gpustat>=1.0.0->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (1.20.0)\n",
      "Requirement already satisfied: jinxed>=1.1.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (1.2.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (0.2.6)\n",
      "Requirement already satisfied: ansicon in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from jinxed>=1.1.0->blessed>=1.17.1->gpustat>=1.0.0->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (1.89.0)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (2.6.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from requests->autogluon.core[all]==0.8.2->autogluon) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from requests->autogluon.core[all]==0.8.2->autogluon) (2023.7.22)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from scikit-image<0.20.0,>=0.19.1->autogluon.multimodal==0.8.2->autogluon) (2.32.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from scikit-image<0.20.0,>=0.19.1->autogluon.multimodal==0.8.2->autogluon) (1.4.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from scikit-image<0.20.0,>=0.19.1->autogluon.multimodal==0.8.2->autogluon) (2023.9.26)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from scikit-learn<1.3,>=1.0->autogluon.core[all]==0.8.2->autogluon) (3.2.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (0.9.0)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (0.3.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (2.4.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (1.1.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (1.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (3.0.12)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (2.0.10)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (8.2.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (3.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (1.0.10)\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from numba->mlforecast<0.7.4,>=0.7.0->autogluon.timeseries[all]==0.8.2->autogluon) (0.41.1)\n",
      "Requirement already satisfied: patsy>=0.5.2 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from statsmodels<0.15,>=0.13.0->autogluon.timeseries[all]==0.8.2->autogluon) (0.5.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (3.5)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (3.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (2.0.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (2.23.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (0.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (5.3.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<3,>=2.9->autogluon.multimodal==0.8.2->autogluon) (3.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (0.1.3)\n",
      "Requirement already satisfied: safetensors in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from timm<0.10.0,>=0.9.2->autogluon.multimodal==0.8.2->autogluon) (0.4.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from transformers[sentencepiece]<4.27.0,>=4.23.0->autogluon.multimodal==0.8.2->autogluon) (0.13.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from transformers[sentencepiece]<4.27.0,>=4.23.0->autogluon.multimodal==0.8.2->autogluon) (0.1.99)\n",
      "Requirement already satisfied: distlib<1,>=0.3.7 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from virtualenv>=20.0.24->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (0.3.7)\n",
      "Requirement already satisfied: platformdirs<4,>=3.9.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from virtualenv>=20.0.24->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (3.10.0)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<4->fastai<2.8,>=2.3.1->autogluon.tabular[all]==0.8.2->autogluon) (0.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==0.8.2->autogluon) (2.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from matplotlib->autogluon.core[all]==0.8.2->autogluon) (1.1.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from matplotlib->autogluon.core[all]==0.8.2->autogluon) (1.4.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from matplotlib->autogluon.core[all]==0.8.2->autogluon) (3.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from matplotlib->autogluon.core[all]==0.8.2->autogluon) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from matplotlib->autogluon.core[all]==0.8.2->autogluon) (4.42.1)\n",
      "Requirement already satisfied: ordered-set in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (4.1.0)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from opencensus->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (2.14.0)\n",
      "Requirement already satisfied: opencensus-context>=0.1.3 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from opencensus->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (0.1.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]<2.4,>=2.3->autogluon.core[all]==0.8.2->autogluon) (1.61.0)\n",
      "Requirement already satisfied: pycryptodome in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (3.19.0)\n",
      "Requirement already satisfied: pywin32 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (306)\n",
      "Requirement already satisfied: openxlab in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (0.0.28)\n",
      "Requirement already satisfied: oss2~=2.17.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (2.17.0)\n",
      "Requirement already satisfied: crcmod>=1.7 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (1.7)\n",
      "Requirement already satisfied: aliyun-python-sdk-kms>=2.4.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (2.16.2)\n",
      "Requirement already satisfied: aliyun-python-sdk-core>=2.13.12 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (2.14.0)\n",
      "Requirement already satisfied: cryptography>=2.6.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (41.0.5)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from cffi>=1.12->cryptography>=2.6.0->aliyun-python-sdk-core>=2.13.12->oss2~=2.17.0->openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (2.21)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (2.16.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==0.8.2->autogluon) (0.1.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from plotly->catboost<1.3,>=1.1->autogluon.tabular[all]==0.8.2->autogluon) (8.2.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages (from requests->autogluon.core[all]==0.8.2->autogluon) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\gunna\\documents\\maskinlæring\\prosjekt\\power-predictor\\venv\\lib\\site-packages)\n",
      "WARNING: You are using pip version 21.2.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\gunna\\Documents\\Maskinlæring\\Prosjekt\\power-predictor\\venv\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%pip install autogluon\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.metrics import accuracy_score as acs_score\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import skew\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0.1: Preprocessing of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_RAW_DATA_LOCATION = \"data/raw/\"\n",
    "\n",
    "def get_raw_data():\n",
    "    \"\"\"\n",
    "    Utility function to load the raw data from the data/raw folder.\n",
    "\n",
    "    Returns:\n",
    "        train_a (pd.DataFrame): The training targets for the A dataset.\n",
    "        train_b (pd.DataFrame): The training targets for the B dataset.\n",
    "        train_c (pd.DataFrame): The training targets for the C dataset.\n",
    "        X_train_estimated_a (pd.DataFrame): The estimated training features for the A dataset.\n",
    "        X_train_estimated_b (pd.DataFrame): The estimated training features for the B dataset.\n",
    "        X_train_estimated_c (pd.DataFrame): The estimated training features for the C dataset.\n",
    "        X_train_observed_a (pd.DataFrame): The observed training features for the A dataset.\n",
    "        X_train_observed_b (pd.DataFrame): The observed training features for the B dataset.\n",
    "        X_train_observed_c (pd.DataFrame): The observed training features for the C dataset.\n",
    "        X_test_estimated_a (pd.DataFrame): The estimated test features for the A dataset.\n",
    "        X_test_estimated_b (pd.DataFrame): The estimated test features for the B dataset.\n",
    "        X_test_estimated_c (pd.DataFrame): The estimated test features for the C dataset.\n",
    "    \"\"\"\n",
    "    train_a = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}A/train_targets.parquet')\n",
    "    train_b = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}B/train_targets.parquet')\n",
    "    train_c = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}C/train_targets.parquet')\n",
    "    X_train_estimated_a = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}A/X_train_estimated.parquet')\n",
    "    X_train_estimated_b = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}B/X_train_estimated.parquet')\n",
    "    X_train_estimated_c = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}C/X_train_estimated.parquet')\n",
    "    X_train_observed_a = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}A/X_train_observed.parquet')\n",
    "    X_train_observed_b = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}B/X_train_observed.parquet')\n",
    "    X_train_observed_c = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}C/X_train_observed.parquet')\n",
    "    X_test_estimated_a = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}A/X_test_estimated.parquet')\n",
    "    X_test_estimated_b = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}B/X_test_estimated.parquet')\n",
    "    X_test_estimated_c = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}C/X_test_estimated.parquet')\n",
    "\n",
    "    return train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(\n",
    "    train_observed: pd.DataFrame,\n",
    "    train_estimated: pd.DataFrame,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    drop_features: bool = True,\n",
    ") -> Tuple[\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.Series,\n",
    "    pd.Series,\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.Series,\n",
    "    pd.Series,\n",
    "]:\n",
    "    \"\"\"\n",
    "    Prepares the data for modeling by handling missing values and splitting the data.\n",
    "\n",
    "    Args:\n",
    "    train_observed (pd.DataFrame): The aligned training DataFrame with observed features.\n",
    "    train_estimated (pd.DataFrame): The aligned training DataFrame with estimated features.\n",
    "    test_size (float): The proportion of the dataset to include in the test split.\n",
    "    random_state (int): Controls the shuffling applied to the data before applying the split.\n",
    "\n",
    "    Returns:\n",
    "    X_train_obs (pd.DataFrame): The training features with observed data.\n",
    "    X_val_obs (pd.DataFrame): The validation features with observed data.\n",
    "    y_train_obs (pd.Series): The training target with observed data.\n",
    "    y_val_obs (pd.Series): The validation target with observed data.\n",
    "    X_train_est (pd.DataFrame): The training features with estimated data.\n",
    "    X_val_est (pd.DataFrame): The validation features with estimated data.\n",
    "    y_train_est (pd.Series): The training target with estimated data.\n",
    "    y_val_est (pd.Series): The validation target with estimated data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove missing features\n",
    "    train_observed = remove_missing_features(train_observed)\n",
    "    train_estimated = remove_missing_features(train_estimated)\n",
    "\n",
    "    # Handle missing values (e.g., imputation, removal)\n",
    "    train_observed_clean = train_observed.dropna(\n",
    "        subset=[\"visibility:m\", \"pv_measurement\"]\n",
    "    )\n",
    "    train_estimated_clean = train_estimated.dropna(\n",
    "        subset=[\"visibility:m\", \"pv_measurement\"]\n",
    "    )\n",
    "\n",
    "    # Remove discrepancies\n",
    "    train_observed_clean = clean_pv_data(train_observed_clean)\n",
    "    train_estimated_clean = clean_pv_data(train_estimated_clean)\n",
    "\n",
    "    # Feature engineer\n",
    "    train_observed_clean = feature_engineer(train_observed_clean)\n",
    "    train_estimated_clean = feature_engineer(train_estimated_clean)\n",
    "\n",
    "    # Split the data into features (X) and target (y)\n",
    "    y_obs = train_observed_clean[\"pv_measurement\"]\n",
    "\n",
    "    if drop_features:\n",
    "        X_obs = train_observed_clean.drop(\n",
    "            columns=[\"time\", \"pv_measurement\", \"date_forecast\", \"date_calc\"],\n",
    "            errors=\"ignore\",\n",
    "        )\n",
    "    else:\n",
    "        X_obs = train_observed_clean\n",
    "\n",
    "    if drop_features:\n",
    "        X_est = train_estimated_clean.drop(\n",
    "            columns=[\"time\", \"pv_measurement\", \"date_calc\", \"date_forecast\"],\n",
    "            errors=\"ignore\",\n",
    "        )\n",
    "    else:\n",
    "        X_est = train_estimated_clean\n",
    "\n",
    "    y_est = train_estimated_clean[\"pv_measurement\"]\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train_obs, X_val_obs, y_train_obs, y_val_obs = train_test_split(\n",
    "        X_obs, y_obs, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    X_train_est, X_val_est, y_train_est, y_val_est = train_test_split(\n",
    "        X_est, y_est, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        X_train_obs,\n",
    "        X_val_obs,\n",
    "        y_train_obs,\n",
    "        y_val_obs,\n",
    "        X_train_est,\n",
    "        X_val_est,\n",
    "        y_train_est,\n",
    "        y_val_est,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_location_datasets(\n",
    "    df: pd.DataFrame,\n",
    ") -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    locations = [\"location_a\", \"location_b\", \"location_c\"]\n",
    "    x_a = df[df[\"location_a\"] == 1]\n",
    "    x_a = x_a.drop(locations, axis=1)\n",
    "    y_a = x_a[\"pv_measurement\"]\n",
    "    if \"pv_measurement\" in x_a.columns:\n",
    "        x_a = x_a.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "    x_b = df[df[\"location_b\"] == 1]\n",
    "    x_b = x_b.drop(locations, axis=1)\n",
    "    y_b = x_b[\"pv_measurement\"]\n",
    "    if \"pv_measurement\" in x_b.columns:\n",
    "        x_b = x_b.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "    x_c = df[df[\"location_c\"] == 1]\n",
    "    x_c = x_c.drop(locations, axis=1)\n",
    "    y_c = x_c[\"pv_measurement\"]\n",
    "    if \"pv_measurement\" in x_b.columns:\n",
    "        x_b = x_b.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "    return (x_a, x_b, x_c, y_a, y_b, y_c)\n",
    "\n",
    "\n",
    "def remove_missing_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Remove features with more than 50% missing values or Constant features\n",
    "    df = df.drop(\"snow_density:kgm3\", axis=1)\n",
    "    df = df.drop(\"ceiling_height_agl:m\", axis=1)\n",
    "    df = df.drop(\"elevation:m\", axis=1)\n",
    "    df[\"cloud_base_agl:m\"] = df[\"cloud_base_agl:m\"].fillna(0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_pv_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply a series of filters to clean PV data in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame containing PV measurement data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Cleaned DataFrame after applying all filters.\n",
    "    \"\"\"\n",
    "    df = filter_pv_measurements_at_night(df)\n",
    "    df = filter_constant_pv_measurements(df)\n",
    "    df = filter_zero_pv_measurements(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_pv_measurements_at_night(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter out positive PV measurements at night based on specific conditions. As there are no PV measurements at night, these are likely to be measurement errors.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame containing PV measurement and time-of-day data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with unrealistic positive PV measurements at night removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Condition: Positive PV measurement when it's not daytime and the measurement is the same as the previous timestep\n",
    "    night_time_with_positive_pv = (df[\"is_day:idx\"] == 0) & (df[\"pv_measurement\"] > 0)\n",
    "    same_as_previous_step = df[\"pv_measurement\"] == df[\"pv_measurement\"].shift(1)\n",
    "    condition1 = night_time_with_positive_pv & same_as_previous_step\n",
    "\n",
    "    # Condition: Positive PV measurement when sun elevation is below a certain threshold\n",
    "    sun_elevation_threshold = -10\n",
    "    low_sun_elevation_with_positive_pv = (df[\"sun_elevation:d\"] < sun_elevation_threshold) & (df[\"pv_measurement\"] > 0)\n",
    "    \n",
    "    # Combined condition to filter\n",
    "    conditions_to_remove = condition1 | low_sun_elevation_with_positive_pv\n",
    "    df = df.drop(df[conditions_to_remove].index)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_constant_pv_measurements(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter out rows where PV measurement is constant and non-zero for 6 or more consecutive timesteps.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame containing PV measurement data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with specified discrepancies removed.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Mark changes in pv_measurement and zero values\n",
    "    measurement_change_or_zero = (df[\"pv_measurement\"] != df[\"pv_measurement\"].shift()) | (df[\"pv_measurement\"] == 0)\n",
    "    \n",
    "    # Step 2: Create groups for consecutive measurements\n",
    "    df[\"temp_group\"] = measurement_change_or_zero.cumsum()\n",
    "\n",
    "    # Step 3: Count entries in each group\n",
    "    group_counts = df.groupby(\"temp_group\")[\"pv_measurement\"].transform(\"count\")\n",
    "\n",
    "    # Step 4: Determine rows to remove (constant non-zero measurements for 6+ timesteps)\n",
    "    rows_to_remove = (group_counts >= 6) & (df[\"pv_measurement\"] != 0)\n",
    "\n",
    "    # Step 5: Remove specified rows and the temporary grouping column\n",
    "    df_filtered = df[~rows_to_remove].drop(columns=[\"temp_group\"])\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "\n",
    "def filter_zero_pv_measurements(\n",
    "    un_filtered_df: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove entries where PV measurements are zero despite significant radiation.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame containing radiation and PV measurement data.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Filtered DataFrame.\n",
    "    \"\"\"\n",
    "    # Trail and error on total_radiation_threshold, tried 0.5, 5 and 30\n",
    "    total_radiation_threshold = 30\n",
    "    is_significant_radiation = (un_filtered_df[\"diffuse_rad:W\"] + un_filtered_df[\"direct_rad:W\"]) >= total_radiation_threshold\n",
    "    is_zero_pv_measurement = un_filtered_df[\"pv_measurement\"] == 0\n",
    "    filtered_df = un_filtered_df[~(is_significant_radiation & is_zero_pv_measurement)]\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def feature_engineer(data_frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    data_frame = create_time_features_from_date(data_frame)\n",
    "    data_frame[\"solar_radiation_interaction\"] = data_frame[\"diffuse_rad:W\"] * data_frame[\"direct_rad:W\"]\n",
    "\n",
    "    data_frame[\"effective_solar_elevation\"] = np.where(\n",
    "        data_frame[\"sun_elevation:d\"] <= 0,\n",
    "        0,\n",
    "        np.sin(np.radians(data_frame[\"sun_elevation:d\"])),\n",
    "    )\n",
    "    data_frame = data_frame.drop(\"sun_elevation:d\", axis=1)\n",
    "\n",
    "    data_frame[\"effective_radiation\"] = np.where(\n",
    "        data_frame[\"clear_sky_energy_1h:J\"] == 0,\n",
    "        0,  # or your specified value\n",
    "        data_frame[\"direct_rad_1h:J\"] / data_frame[\"clear_sky_energy_1h:J\"],\n",
    "    )\n",
    "\n",
    "    data_frame[\"net_clear_sky_residual\"] = (\n",
    "        data_frame[\"clear_sky_rad:W\"]\n",
    "        - data_frame[\"direct_rad:W\"]\n",
    "        - data_frame[\"diffuse_rad:W\"]\n",
    "    )\n",
    "\n",
    "    data_frame[\"cloud_ratio\"] = np.where(\n",
    "        data_frame[\"total_cloud_cover:p\"] == 0,\n",
    "        0,  # or your specified value\n",
    "        data_frame[\"effective_cloud_cover:p\"] / data_frame[\"total_cloud_cover:p\"],\n",
    "    )\n",
    "\n",
    "    data_frame[\"low_cloud_diffuse_rad\"] = data_frame[\n",
    "        \"diffuse_rad:W\"\n",
    "    ].where(data_frame[\"effective_cloud_cover:p\"] < 0.3, 0)\n",
    "\n",
    "    data_frame[\"cloud_cover_over_30%\"] = np.where(\n",
    "        data_frame[\"effective_cloud_cover:p\"] > 30, 1, 0\n",
    "    )\n",
    "\n",
    "    data_frame[\"global_horizontal_irradiation\"] = (\n",
    "        data_frame[\"diffuse_rad:W\"] + data_frame[\"direct_rad:W\"]\n",
    "    )\n",
    "\n",
    "    data_frame[\"direct_rad_cloud_adjustment\"] = data_frame[\"direct_rad:W\"] * (\n",
    "        100 - data_frame[\"effective_cloud_cover:p\"]\n",
    "    )\n",
    "\n",
    "    data_frame[\"effective_solar_elevation_squared\"] = (\n",
    "        data_frame[\"effective_solar_elevation\"] ** 0.5\n",
    "    )\n",
    "    \n",
    "    snow_columns = [\n",
    "        \"snow_depth:cm\",\n",
    "        \"fresh_snow_12h:cm\",\n",
    "        \"fresh_snow_1h:cm\",\n",
    "        \"fresh_snow_24h:cm\",\n",
    "        \"fresh_snow_3h:cm\",\n",
    "        \"fresh_snow_6h:cm\",\n",
    "    ]\n",
    "\n",
    "    data_frame[\"is_freezing\"] = (data_frame[\"t_1000hPa:K\"] < 273).astype(int)\n",
    "\n",
    "    data_frame[\"is_snow\"] = (data_frame[snow_columns] > 0).any(axis=1).astype(int)\n",
    "    data_frame[\"is_rain\"] = (data_frame[\"precip_5min:mm\"] > 0).astype(int)\n",
    "\n",
    "    data_frame = data_frame.drop(\"snow_drift:idx\", axis=1)\n",
    "\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "def create_time_features_from_date(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a new data frame with new features from date_forecast column.\n",
    "    This will create temporal features from date_forecast that are easier to learn by the model.\n",
    "    It creates the following features: month, season, year, day_of_year, day_segment.\n",
    "    All of the new features are int type.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Data frame with date_forecast column.\n",
    "    Returns:\n",
    "        pd.DataFrame: Data frame copy with new features.\n",
    "\n",
    "    \"\"\"\n",
    "    df[\"sin_day_of_year\"] = df[\"date_forecast\"].apply(get_sin_day)\n",
    "    df[\"cos_day_of_year\"] = df[\"date_forecast\"].apply(get_cos_day)\n",
    "    df[\"sin_hour\"] = df[\"date_forecast\"].apply(get_sin_hour)\n",
    "    df[\"cos_hour\"] = df[\"date_forecast\"].apply(get_cos_hour)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_sin_hour(date: datetime) -> float:\n",
    "    HOURS_OF_DAY = 24\n",
    "    return math.sin(2 * math.pi * (date.hour) / HOURS_OF_DAY)\n",
    "\n",
    "\n",
    "def get_cos_hour(date: datetime) -> float:\n",
    "    HOURS_OF_DAY = 24\n",
    "    return math.cos(2 * math.pi * (date.hour) / HOURS_OF_DAY)\n",
    "\n",
    "\n",
    "def get_sin_day(date: datetime) -> float:\n",
    "    DAY_OF_YEAR = 365.25  # Add 0.25 to account for leap years\n",
    "    return math.sin(2 * math.pi * (date.timetuple().tm_yday - 1) / DAY_OF_YEAR)\n",
    "\n",
    "\n",
    "def get_cos_day(date: datetime) -> float:\n",
    "    DAY_OF_YEAR = 365.25  # Add 0.25 to account for leap years\n",
    "    return math.cos(2 * math.pi * (date.timetuple().tm_yday - 1) / DAY_OF_YEAR)\n",
    "\n",
    "\n",
    "def add_location(data_frame: pd.DataFrame, location: str):\n",
    "    if location.lower() == \"a\":\n",
    "        data_frame[\"location_a\"] = 1\n",
    "    else:\n",
    "        data_frame[\"location_a\"] = 0\n",
    "\n",
    "    if location.lower() == \"b\":\n",
    "        data_frame[\"location_b\"] = 1\n",
    "    else:\n",
    "        data_frame[\"location_b\"] = 0\n",
    "\n",
    "    if location.lower() == \"c\":\n",
    "        data_frame[\"location_c\"] = 1\n",
    "    else:\n",
    "        data_frame[\"location_c\"] = 0\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "# Define a function to align the temporal resolution of the datasets\n",
    "def temporal_alignment(\n",
    "    train: pd.DataFrame, observed: pd.DataFrame, estimated: pd.DataFrame\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Aligns the temporal resolution of the datasets by aggregating the 15-min interval weather data to hourly intervals.\n",
    "\n",
    "    Args:\n",
    "        train (pd.DataFrame): The training targets DataFrame.\n",
    "        observed (pd.DataFrame): The observed training features DataFrame.\n",
    "        estimated (pd.DataFrame): The estimated training features DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        train_observed (pd.DataFrame): The aligned training DataFrame with observed features.\n",
    "        train_estimated (pd.DataFrame): The aligned training DataFrame with estimated features.\n",
    "    \"\"\"\n",
    "    # Convert the time columns to datetime objects\n",
    "    train[\"time\"] = pd.to_datetime(train[\"time\"])\n",
    "    observed[\"date_forecast\"] = pd.to_datetime(observed[\"date_forecast\"])\n",
    "    estimated[\"date_forecast\"] = pd.to_datetime(estimated[\"date_forecast\"])\n",
    "\n",
    "    # Set the date_forecast column as index for resampling\n",
    "    observed.set_index(\"date_forecast\", inplace=True)\n",
    "    estimated.set_index(\"date_forecast\", inplace=True)\n",
    "\n",
    "    # Resample the weather data to hourly intervals and aggregate the values by mean\n",
    "    observed_resampled = observed.resample(\"1H\").mean()\n",
    "    estimated_resampled = estimated.resample(\"1H\").mean()\n",
    "\n",
    "    # Reset the index after resampling\n",
    "    observed_resampled.reset_index(inplace=True)\n",
    "    estimated_resampled.reset_index(inplace=True)\n",
    "\n",
    "    # Merge the aggregated weather data with the solar production data based on the timestamp\n",
    "    train_observed = pd.merge(\n",
    "        train, observed_resampled, how=\"left\", left_on=\"time\", right_on=\"date_forecast\"\n",
    "    )\n",
    "    train_estimated = pd.merge(\n",
    "        train, estimated_resampled, how=\"left\", left_on=\"time\", right_on=\"date_forecast\"\n",
    "    )\n",
    "\n",
    "    return train_observed, train_estimated\n",
    "\n",
    "\n",
    "def temporal_alignment_tests(test: pd.DataFrame) -> Tuple[pd.DataFrame]:\n",
    "    return aggregate_rows(test)\n",
    "\n",
    "\n",
    "def aggregate_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Create a 'group' column to group every 4 rows together\n",
    "    df[\"group\"] = df.index // 4\n",
    "\n",
    "    # Define the aggregation functions\n",
    "    aggregation = {col: \"mean\" for col in df.columns if col != \"date_forecast\"}\n",
    "    aggregation[\"date_forecast\"] = \"first\"\n",
    "\n",
    "    # Group by the 'group' column and aggregate\n",
    "    df_agg = df.groupby(\"group\").agg(aggregation).reset_index(drop=True)\n",
    "\n",
    "    # Drop the 'group' column from the original dataframe\n",
    "    df_agg.drop(\"group\", axis=1, inplace=True)\n",
    "\n",
    "    return df_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_preprocessed_data(drop_features: bool = True) -> (\n",
    "    Tuple[\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "    ]\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch the preprocessed data for training and validation.\n",
    "\n",
    "    Returns:\n",
    "        X_train_obs_combined: The observed data for training\n",
    "        X_val_obs_combined: The observed data for validation\n",
    "        y_train_obs_combined: The observed labels for training\n",
    "        y_val_obs_combined: The observed labels for validation\n",
    "        X_train_est_combined: The estimated data for training\n",
    "        X_val_est_combined: The estimated data for validation\n",
    "        y_train_est_combined: The estimated labels for training\n",
    "        y_val_est_combined: The estimated labels for validation\n",
    "    \"\"\"\n",
    "    (\n",
    "        train_a,\n",
    "        train_b,\n",
    "        train_c,\n",
    "        X_train_estimated_a,\n",
    "        X_train_estimated_b,\n",
    "        X_train_estimated_c,\n",
    "        X_train_observed_a,\n",
    "        X_train_observed_b,\n",
    "        X_train_observed_c,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "    ) = get_raw_data()\n",
    "\n",
    "    # Temporally align the data from all three locations to the same time.\n",
    "    train_observed_a, train_estimated_a = temporal_alignment(\n",
    "        train_a, X_train_observed_a, X_train_estimated_a\n",
    "    )\n",
    "    train_observed_b, train_estimated_b = temporal_alignment(\n",
    "        train_b, X_train_observed_b, X_train_estimated_b\n",
    "    )\n",
    "    train_observed_c, train_estimated_c = temporal_alignment(\n",
    "        train_c, X_train_observed_c, X_train_estimated_c\n",
    "    )\n",
    "\n",
    "    # Add location data\n",
    "    train_observed_a = add_location(train_observed_a, \"a\")\n",
    "    train_estimated_a = add_location(train_estimated_a, \"a\")\n",
    "\n",
    "    train_observed_b = add_location(train_observed_b, \"b\")\n",
    "    train_estimated_b = add_location(train_estimated_b, \"b\")\n",
    "\n",
    "    train_observed_c = add_location(train_observed_c, \"c\")\n",
    "    train_estimated_c = add_location(train_estimated_c, \"c\")\n",
    "\n",
    "    # Combine the temporally aligned datasets from all three locations\n",
    "    train_observed_combined = pd.concat(\n",
    "        [train_observed_a, train_observed_b, train_observed_c], ignore_index=True\n",
    "    )\n",
    "    train_estimated_combined = pd.concat(\n",
    "        [train_estimated_a, train_estimated_b, train_estimated_c], ignore_index=True\n",
    "    )\n",
    "\n",
    "\n",
    "    # # Add boolean flag for estimated vs observed\n",
    "    # train_observed_combined[\"estimated_flag\"] = 0\n",
    "    # train_estimated_combined[\"estimated_flag\"] = 1\n",
    "\n",
    "    # Prepare the combined dataset by handling missing values and splitting the data\n",
    "    (\n",
    "        X_train_obs_combined,\n",
    "        X_val_obs_combined,\n",
    "        y_train_obs_combined,\n",
    "        y_val_obs_combined,\n",
    "        X_train_est_combined,\n",
    "        X_val_est_combined,\n",
    "        y_train_est_combined,\n",
    "        y_val_est_combined,\n",
    "    ) = prepare_data(train_observed_combined, train_estimated_combined, drop_features=drop_features)\n",
    "\n",
    "    return (\n",
    "        X_train_obs_combined,\n",
    "        X_val_obs_combined,\n",
    "        y_train_obs_combined,\n",
    "        y_val_obs_combined,\n",
    "        X_train_est_combined,\n",
    "        X_val_est_combined,\n",
    "        y_train_est_combined,\n",
    "        y_val_est_combined,\n",
    "    )\n",
    "\n",
    "def get_preprocessed_test_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the preprocessed test data without the 'date_forecast' column.\n",
    "    \"\"\"\n",
    "    (\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        X_test_estimated_a,\n",
    "        X_test_estimated_b,\n",
    "        X_test_estimated_c,\n",
    "    ) = get_raw_data()\n",
    "\n",
    "    # Align the test data to the same time as the training data\n",
    "    X_test_estimated_a = temporal_alignment_tests(X_test_estimated_a)\n",
    "    X_test_estimated_b = temporal_alignment_tests(X_test_estimated_b)\n",
    "    X_test_estimated_c = temporal_alignment_tests(X_test_estimated_c)\n",
    "    print(\"After temporal alignment\")\n",
    "    print(f\"X_test_estimated_a.shape = {X_test_estimated_a.shape}, X_test_estimated_b.shape = {X_test_estimated_b.shape}, X_test_estimated_c.shape = {X_test_estimated_c.shape}\")\n",
    "\n",
    "    X_test_estimated_a = remove_missing_features(X_test_estimated_a)\n",
    "    X_test_estimated_b = remove_missing_features(X_test_estimated_b)\n",
    "    X_test_estimated_c = remove_missing_features(X_test_estimated_c)\n",
    "\n",
    "    # Add location data\n",
    "    X_test_estimated_a = add_location(X_test_estimated_a, \"a\")\n",
    "    X_test_estimated_b = add_location(X_test_estimated_b, \"b\")\n",
    "    X_test_estimated_c = add_location(X_test_estimated_c, \"c\")\n",
    "\n",
    "    X_test_a_correct_features = feature_engineer(X_test_estimated_a)\n",
    "    X_test_b_correct_features = feature_engineer(X_test_estimated_b)\n",
    "    X_test_c_correct_features = feature_engineer(X_test_estimated_c)\n",
    "\n",
    "    # X_train_obs_combined, X_val_obs_combined, y_train_obs_combined, y_val_obs_combined, X_train_est_combined, X_val_est_combined, y_train_est_combined, y_val_est_combined = fetch_preprocessed_data()\n",
    "    \n",
    "    # # Add historical data so that the model can use it for prediction\n",
    "    # # Add mean_pv_measurement with same day and hour from previous years\n",
    "    # X_test_estimated_a_with_historical_data = add_expected_pv_to_test_data(X_test_a_correct_features, X_train_obs_combined)\n",
    "    # X_test_estimated_b_with_historical_data = add_expected_pv_to_test_data(X_test_b_correct_features, X_train_obs_combined)\n",
    "    # X_test_estimated_c_with_historical_data = add_expected_pv_to_test_data(X_test_c_correct_features, X_train_obs_combined)\n",
    "\n",
    "    # Drop the 'date_calc' and 'date_forecast' columns from the test data\n",
    "    X_test_estimated_a_processed = X_test_a_correct_features.drop(\n",
    "        columns=[\"date_calc\", \"date_forecast\"], errors='ignore'\n",
    "    )\n",
    "    X_test_estimated_b_processed = X_test_b_correct_features.drop(\n",
    "        columns=[\"date_calc\", \"date_forecast\"], errors='ignore'\n",
    "    )\n",
    "    X_test_estimated_c_processed = X_test_c_correct_features.drop(\n",
    "        columns=[\"date_calc\", \"date_forecast\"], errors='ignore'\n",
    "    )\n",
    "\n",
    "    # # # Handle NaN values in the test data by filling them with the mean value of the respective column from the training data\n",
    "    # X_test_estimated_a_processed.dropna()\n",
    "    # X_test_estimated_b_processed.dropna()\n",
    "    # X_test_estimated_c_processed.dropna()\n",
    "    print(f\"X_test_estimated_a_processed.shape = {X_test_estimated_a_processed.shape}, X_test_estimated_b_processed.shape = {X_test_estimated_b_processed.shape}, X_test_estimated_c_processed.shape = {X_test_estimated_c_processed.shape}\")\n",
    "    tests = pd.concat([X_test_estimated_a_processed, X_test_estimated_b_processed, X_test_estimated_c_processed], ignore_index=True)\n",
    "\n",
    "    # Add boolean flag for estimated vs observed data\n",
    "    # tests[\"estimated_flag\"] = 1\n",
    "    return tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After temporal alignment\n",
      "X_test_estimated_a.shape = (720, 47), X_test_estimated_b.shape = (720, 47), X_test_estimated_c.shape = (720, 47)\n",
      "X_test_estimated_a_processed.shape = (720, 60), X_test_estimated_b_processed.shape = (720, 60), X_test_estimated_c_processed.shape = (720, 60)\n"
     ]
    }
   ],
   "source": [
    "X_train_obs_combined, X_val_obs_combined, y_train_obs_combined, y_val_obs_combined, X_train_est_combined, X_val_est_combined, y_train_est_combined, y_val_est_combined = fetch_preprocessed_data()\n",
    "x_test_whole = get_preprocessed_test_data()\n",
    "\n",
    "#estimated_flag\n",
    "X_train_obs_combined[\"estimated_flag\"] = 0\n",
    "X_val_obs_combined[\"estimated_flag\"] = 0\n",
    "X_train_est_combined[\"estimated_flag\"] = 1\n",
    "X_val_est_combined[\"estimated_flag\"] = 1\n",
    "x_test_whole[\"estimated_flag\"] = 1\n",
    "\n",
    "\n",
    "x_whole = pd.concat([X_train_obs_combined, X_val_obs_combined, X_train_est_combined, X_val_est_combined])\n",
    "y_whole = pd.concat([y_train_obs_combined, y_val_obs_combined, y_train_est_combined, y_val_est_combined])\n",
    "x_whole.reset_index(drop=True, inplace=True)\n",
    "y_whole.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole[\"pv_measurement\"] = y_whole\n",
    "df_shuffled = x_whole.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "x_whole_a = df_shuffled[df_shuffled['location_a'] == 1]\n",
    "x_whole_b = df_shuffled[df_shuffled['location_b'] == 1]\n",
    "x_whole_c = df_shuffled[df_shuffled['location_c'] == 1]\n",
    "\n",
    "y_whole_a = x_whole_a[\"pv_measurement\"]\n",
    "x_whole_a = x_whole_a.drop('location_a', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_b', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_c', axis = 1)\n",
    "\n",
    "y_whole_b = x_whole_b[\"pv_measurement\"]\n",
    "x_whole_b = x_whole_b.drop('location_a', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_b', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_c', axis = 1)\n",
    "\n",
    "y_whole_c = x_whole_c[\"pv_measurement\"]\n",
    "x_whole_c = x_whole_c.drop('location_a', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_b', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_c', axis = 1)\n",
    "cat_features = [\"estimated_flag\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autogluon for A, B and C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231112_200244\\\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20231112_200244\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.0\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   36.97 GB / 1022.87 GB (3.6%)\n",
      "Train Data Rows:    34046\n",
      "Train Data Columns: 58\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3793.55 MB\n",
      "\tTrain Data (Original)  Memory Usage: 8.58 MB (0.2% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m', ...]\n",
      "\t\t('int', [])   :  5 | ['cloud_cover_over_30%', 'is_freezing', 'is_snow', 'is_rain', 'estimated_flag']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 53 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m', ...]\n",
      "\t\t('int', ['bool']) :  5 | ['cloud_cover_over_30%', 'is_freezing', 'is_snow', 'is_rain', 'estimated_flag']\n",
      "\t0.2s = Fit runtime\n",
      "\t58 features in original data used to generate 58 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 7.93 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.24s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 6.5s of the 9.75s of remaining time.\n",
      "\t-260.7917\t = Validation score   (-mean_absolute_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t1.33s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 4.99s of the 8.24s of remaining time.\n",
      "\t-260.6325\t = Validation score   (-mean_absolute_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t1.2s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 3.61s of the 6.86s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\tTime limit exceeded... Skipping LightGBMXT_BAG_L1.\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.76s of the -1.58s of remaining time.\n",
      "\t-260.3892\t = Validation score   (-mean_absolute_error)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "No base models to train on, skipping auxiliary stack level 3...\n",
      "AutoGluon training complete, total runtime = 11.92s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231112_200244\\\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "\n",
    "# Initialize the TabularPredictor\n",
    "best_model_a = TabularPredictor(label='pv_measurement', eval_metric='mean_absolute_error', problem_type=\"regression\").fit(\n",
    "    train_data=x_whole_a,\n",
    "    presets='best_quality',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231112_200256\\\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 10s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20231112_200256\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.0\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   29.67 GB / 1022.87 GB (2.9%)\n",
      "Train Data Rows:    28688\n",
      "Train Data Columns: 58\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2887.85 MB\n",
      "\tTrain Data (Original)  Memory Usage: 7.23 MB (0.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m', ...]\n",
      "\t\t('int', [])   :  5 | ['cloud_cover_over_30%', 'is_freezing', 'is_snow', 'is_rain', 'estimated_flag']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 53 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m', ...]\n",
      "\t\t('int', ['bool']) :  5 | ['cloud_cover_over_30%', 'is_freezing', 'is_snow', 'is_rain', 'estimated_flag']\n",
      "\t0.3s = Fit runtime\n",
      "\t58 features in original data used to generate 58 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 6.68 MB (0.2% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.32s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 6.45s of the 9.67s of remaining time.\n",
      "\t-35.9509\t = Validation score   (-mean_absolute_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.91s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 5.34s of the 8.56s of remaining time.\n",
      "\t-35.876\t = Validation score   (-mean_absolute_error)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.84s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 4.35s of the 7.57s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy\n",
      "\t-23.3516\t = Validation score   (-mean_absolute_error)\n",
      "\t3.85s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 9.68s of the 0.22s of remaining time.\n",
      "\t-23.3516\t = Validation score   (-mean_absolute_error)\n",
      "\t0.21s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "No base models to train on, skipping auxiliary stack level 3...\n",
      "AutoGluon training complete, total runtime = 10.09s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231112_200256\\\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# Initialize the TabularPredictor\n",
    "best_model_b = TabularPredictor(label='pv_measurement', eval_metric='mean_absolute_error', problem_type=\"regression\").fit(\n",
    "    train_data=x_whole_b,\n",
    "    presets='best_quality',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels\\ag-20231112_200306\\\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=20\n",
      "Beginning AutoGluon training ... Time limit = 1s\n",
      "AutoGluon will save models to \"AutogluonModels\\ag-20231112_200306\\\"\n",
      "AutoGluon Version:  0.8.2\n",
      "Python Version:     3.10.0\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19045\n",
      "Disk Space Avail:   29.64 GB / 1022.87 GB (2.9%)\n",
      "Train Data Rows:    25142\n",
      "Train Data Columns: 58\n",
      "Label Column: pv_measurement\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2216.12 MB\n",
      "\tTrain Data (Original)  Memory Usage: 6.34 MB (0.3% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 5 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m', ...]\n",
      "\t\t('int', [])   :  5 | ['cloud_cover_over_30%', 'is_freezing', 'is_snow', 'is_rain', 'estimated_flag']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])     : 53 | ['absolute_humidity_2m:gm3', 'air_density_2m:kgm3', 'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m', ...]\n",
      "\t\t('int', ['bool']) :  5 | ['cloud_cover_over_30%', 'is_freezing', 'is_snow', 'is_rain', 'estimated_flag']\n",
      "\t0.2s = Fit runtime\n",
      "\t58 features in original data used to generate 58 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.86 MB (0.3% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.22s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_absolute_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 0.52s of the 0.77s of remaining time.\n",
      "\t-27.1106\t = Validation score   (-mean_absolute_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.92s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 0.78s of the -0.36s of remaining time.\n",
      "\t-27.1106\t = Validation score   (-mean_absolute_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "No base models to train on, skipping auxiliary stack level 3...\n",
      "AutoGluon training complete, total runtime = 1.53s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels\\ag-20231112_200306\\\")\n"
     ]
    }
   ],
   "source": [
    "from autogluon.tabular import TabularPredictor\n",
    "\n",
    "\n",
    "# Initialize the TabularPredictor\n",
    "best_model_c = TabularPredictor(label='pv_measurement', eval_metric='mean_absolute_error', problem_type=\"regression\").fit(\n",
    "    train_data=x_whole_c,\n",
    "    presets='best_quality',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from src.features.preprocess_data import fetch_preprocessed_data\n",
    "\n",
    "find_time_sin = lambda hour: math.sin(2 * math.pi * (hour) / 24)\n",
    "find_time_cos = lambda hour: math.cos(2 * math.pi * (hour) / 24)\n",
    "\n",
    "def postprocess_data(x_test: pd.DataFrame, y_pred: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Postprocess the data to set the predicted values to 0 at the correct times.\"\"\"\n",
    "    \n",
    "    # Cap the min and max values for each location for each hour\n",
    "    y_pred = cap_min_max_values(x_test, y_pred)\n",
    "\n",
    "    # Set the predicted values to 0 at the correct times\n",
    "    y_pred = set_0_pv_at_times(x_test, y_pred, \"a\", [22, 23, 0])\n",
    "    y_pred = set_0_pv_at_times(x_test, y_pred, \"b\", [22, 23, 0])\n",
    "    y_pred = set_0_pv_at_times(x_test, y_pred, \"c\", [22, 23, 0])\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def cap_min_max_values(x_test: pd.DataFrame, y_pred: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Cap the min and max values for each location for each hour.\"\"\"\n",
    "    for hour in range(24):\n",
    "        # Get the min and max values for each location for each hour\n",
    "        min_value_a, max_value_a = get_min_max_values_for_location_at_hour(\"a\", hour)\n",
    "        min_value_b, max_value_b = get_min_max_values_for_location_at_hour(\"b\", hour)\n",
    "        min_value_c, max_value_c = get_min_max_values_for_location_at_hour(\"c\", hour)\n",
    "        print(f\"hour: {hour}, min_value_a: {min_value_a}, max_value_a: {max_value_a}, min_value_b: {min_value_b}, max_value_b: {max_value_b}, min_value_c: {min_value_c}, max_value_c: {max_value_c}\")\n",
    "        # Cap the values between min_value and max_value\n",
    "        y_pred = cap_min_max_values_for_hour(x_test, y_pred, \"a\", hour, min_value_a, max_value_a)\n",
    "        y_pred = cap_min_max_values_for_hour(x_test, y_pred, \"b\", hour, min_value_b, max_value_b)\n",
    "        y_pred = cap_min_max_values_for_hour(x_test, y_pred, \"c\", hour, min_value_c, max_value_c)\n",
    "    return y_pred\n",
    "\n",
    "X_train_obs_combined, X_val_obs_combined, y_train_obs_combined, y_val_obs_combined, X_train_est_combined, X_val_est_combined, y_train_est_combined, y_val_est_combined = fetch_preprocessed_data(drop_features=False)\n",
    "x_whole_with_time = pd.concat([X_train_obs_combined, X_val_obs_combined, X_train_est_combined, X_val_est_combined])\n",
    "\n",
    "def get_min_max_values_for_location_at_hour(location: str, hour: int) -> tuple[float, float]:\n",
    "    \"\"\"Get the min and max values for a specific location at a specific hour.\"\"\"\n",
    "    # Get the x and y for the given hour and location\n",
    "    hour_sin = find_time_sin(hour)\n",
    "    hour_cos = find_time_cos(hour)\n",
    "    # find the min and max values for the given hour and location\n",
    "    min_value = x_whole_with_time[(x_whole_with_time[\"location_\" + location] == 1) & (x_whole_with_time[\"sin_hour\"] == hour_sin) & (x_whole_with_time[\"cos_hour\"] == hour_cos)][\"pv_measurement\"].min()\n",
    "    max_value = x_whole_with_time[(x_whole_with_time[\"location_\" + location] == 1) & (x_whole_with_time[\"sin_hour\"] == hour_sin) & (x_whole_with_time[\"cos_hour\"] == hour_cos)][\"pv_measurement\"].max()\n",
    "    \n",
    "    return (min_value, max_value)\n",
    "\n",
    "def cap_min_max_values_for_hour(x_test: pd.DataFrame, y_pred: pd.DataFrame, location: str, hour: int, min_value: float, max_value: float) -> pd.DataFrame:\n",
    "    \"\"\"Cap the min and max values for a specific hour.\"\"\"\n",
    "    \n",
    "    # Calculate sin and cos values for the given hour\n",
    "    hour_sin = find_time_sin(hour)\n",
    "    hour_cos = find_time_cos(hour)\n",
    "    \n",
    "    # Find indices corresponding to the given hour at the given location\n",
    "    indices = x_test[(x_test[\"location_\" + location] == 1) & (x_test[\"sin_hour\"] == hour_sin) & (x_test[\"cos_hour\"] == hour_cos)].index\n",
    "    \n",
    "    # Cap the values between min_value and max_value\n",
    "    y_pred.loc[indices] = y_pred.loc[indices].clip(min_value, max_value)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def set_0_pv_at_times(x_test: pd.DataFrame, y_pred: pd.DataFrame, location: str, hours: list[int]) -> pd.DataFrame:\n",
    "    \"\"\"Find the correct predicted values at the given times and locaiton and set them to 0.\"\"\"\n",
    "    hours_to_set_0_sin = [find_time_sin(hour) for hour in hours]\n",
    "    hours_to_set_0_cos = [find_time_cos(hour) for hour in hours]\n",
    "\n",
    "\n",
    "    indices = x_test[(x_test[\"location_\" + location] == 1) & (x_test[\"sin_hour\"].isin(hours_to_set_0_sin) & (x_test[\"cos_hour\"].isin(hours_to_set_0_cos)))].index\n",
    "    for index in indices:\n",
    "        y_pred.loc[index] = 0\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole_a = x_test_whole[x_test_whole['location_a'] == 1]\n",
    "x_whole_b = x_test_whole[x_test_whole['location_b'] == 1]\n",
    "x_whole_c = x_test_whole[x_test_whole['location_c'] == 1]\n",
    "\n",
    "x_whole_a = x_whole_a.drop('location_a', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_b', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_c', axis = 1)\n",
    "\n",
    "x_whole_b = x_whole_b.drop('location_a', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_b', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_c', axis = 1)\n",
    "\n",
    "x_whole_c = x_whole_c.drop('location_a', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_b', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_c', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "RES_PATH = 'results/output/'\n",
    "\n",
    "\n",
    "def save_predictions(test: pd.DataFrame, filename: str) -> None:\n",
    "    \"\"\"\n",
    "    Save the 'id' and 'prediction' columns of the test DataFrame to a CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "        test (pd.DataFrame): A 1D DataFrame containing only the predictions.\n",
    "        filename (str): The name of the file where the predictions will be saved.\n",
    "    \"\"\"\n",
    "    model = pd.DataFrame()\n",
    "    \n",
    "    model[\"prediction\"] = test\n",
    "    model['id'] = model.index\n",
    "\n",
    "    model['prediction'] = model['prediction'].apply(lambda x: max(0, x))\n",
    "    \n",
    "    # Reorder the columns to ensure 'id' comes before 'prediction'\n",
    "    model = model[['id', 'prediction']]\n",
    "    \n",
    "\n",
    "    # Save the resulting DataFrame to a CSV file\n",
    "    model.to_csv(f'{RES_PATH}{filename}.csv', index=False)\n",
    "    \n",
    "    # Display the first few rows of the saved DataFrame\n",
    "    print(model.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hour: 0, min_value_a: 0.0, max_value_a: 3.3, min_value_b: -0.0, max_value_b: -0.0, min_value_c: 0.0, max_value_c: 0.0\n",
      "hour: 1, min_value_a: 0.0, max_value_a: 53.68, min_value_b: -0.0, max_value_b: 12.075, min_value_c: 0.0, max_value_c: 9.8\n",
      "hour: 2, min_value_a: 0.0, max_value_a: 233.64000000000001, min_value_b: -0.0, max_value_b: 68.1375, min_value_c: 0.0, max_value_c: 39.2\n",
      "hour: 3, min_value_a: 0.0, max_value_a: 439.12, min_value_b: -0.0, max_value_b: 138.0, min_value_c: 0.0, max_value_c: 88.2\n",
      "hour: 4, min_value_a: 0.0, max_value_a: 1046.98, min_value_b: -0.0, max_value_b: 307.05, min_value_c: 0.0, max_value_c: 176.4\n",
      "hour: 5, min_value_a: 0.0, max_value_a: 2049.08, min_value_b: -0.0, max_value_b: 452.8125, min_value_c: 0.0, max_value_c: 264.6\n",
      "hour: 6, min_value_a: 0.0, max_value_a: 3244.78, min_value_b: -0.0, max_value_b: 681.375, min_value_c: 0.0, max_value_c: 499.8\n",
      "hour: 7, min_value_a: 0.0, max_value_a: 4266.46, min_value_b: -0.0, max_value_b: 865.0875, min_value_c: 0.0, max_value_c: 705.6\n",
      "hour: 8, min_value_a: 0.0, max_value_a: 5048.780000000001, min_value_b: -0.0, max_value_b: 997.9125, min_value_c: 0.0, max_value_c: 784.0\n",
      "hour: 9, min_value_a: 0.0, max_value_a: 5477.339999999999, min_value_b: -0.0, max_value_b: 1091.925, min_value_c: 0.0, max_value_c: 882.0000000000001\n",
      "hour: 10, min_value_a: 0.0, max_value_a: 5733.42, min_value_b: -0.0, max_value_b: 1146.2625, min_value_c: 0.0, max_value_c: 980.0000000000001\n",
      "hour: 11, min_value_a: 0.0, max_value_a: 5651.8, min_value_b: -0.0, max_value_b: 1120.3875, min_value_c: 0.0, max_value_c: 999.6\n",
      "hour: 12, min_value_a: 0.0, max_value_a: 5344.46, min_value_b: 0.0, max_value_b: 1152.3, min_value_c: 0.0, max_value_c: 950.6\n",
      "hour: 13, min_value_a: 0.0, max_value_a: 5058.900000000001, min_value_b: -0.0, max_value_b: 1053.9750000000001, min_value_c: 0.0, max_value_c: 911.4000000000001\n",
      "hour: 14, min_value_a: 0.0, max_value_a: 4491.3, min_value_b: -0.0, max_value_b: 823.6875, min_value_c: 0.0, max_value_c: 803.6\n",
      "hour: 15, min_value_a: 0.0, max_value_a: 3633.96, min_value_b: -0.0, max_value_b: 723.6375, min_value_c: 0.0, max_value_c: 597.8000000000001\n",
      "hour: 16, min_value_a: 0.0, max_value_a: 2710.4, min_value_b: -0.0, max_value_b: 601.1625, min_value_c: 0.0, max_value_c: 494.90000000000003\n",
      "hour: 17, min_value_a: 0.0, max_value_a: 1606.0, min_value_b: -0.0, max_value_b: 363.1125, min_value_c: 0.0, max_value_c: 303.8\n",
      "hour: 18, min_value_a: 0.0, max_value_a: 849.64, min_value_b: -0.0, max_value_b: 242.3625, min_value_c: 0.0, max_value_c: 176.4\n",
      "hour: 19, min_value_a: 0.0, max_value_a: 554.4, min_value_b: -0.0, max_value_b: 145.7625, min_value_c: 0.0, max_value_c: 107.80000000000001\n",
      "hour: 20, min_value_a: 0.0, max_value_a: 197.12, min_value_b: -0.0, max_value_b: 33.637499999999996, min_value_c: 0.0, max_value_c: 29.400000000000002\n",
      "hour: 21, min_value_a: 0.0, max_value_a: 19.139999999999997, min_value_b: -0.0, max_value_b: 13.8, min_value_c: 0.0, max_value_c: 9.8\n",
      "hour: 22, min_value_a: 0.0, max_value_a: 0.66, min_value_b: -0.0, max_value_b: -0.0, min_value_c: -0.0, max_value_c: -0.0\n",
      "hour: 23, min_value_a: 0.0, max_value_a: 0.0, min_value_b: -0.0, max_value_b: -0.0, min_value_c: -0.0, max_value_c: -0.0\n",
      "   id  prediction\n",
      "0   0    0.000000\n",
      "1   1    0.000000\n",
      "2   2    1.377361\n",
      "3   3   40.407318\n",
      "4   4  168.083817\n"
     ]
    }
   ],
   "source": [
    "y_predictions_a = best_model_a.predict(x_whole_a)\n",
    "y_predictions_b = best_model_b.predict(x_whole_b)\n",
    "y_predictions_c = best_model_c.predict(x_whole_c)\n",
    "y_predictions = pd.concat([pd.Series(y_predictions_a), pd.Series(y_predictions_b), pd.Series(y_predictions_c)])\n",
    "y_predictions = y_predictions.reset_index(drop=True)\n",
    "# Save the model\n",
    "y_predictions_autogluon = postprocess_data(x_test_whole, pd.DataFrame(y_predictions))\n",
    "save_predictions(y_predictions, 'autogluon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catboost model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole[\"pv_measurement\"] = y_whole\n",
    "df_shuffled = x_whole.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "x_whole_a = df_shuffled[df_shuffled['location_a'] == 1]\n",
    "x_whole_b = df_shuffled[df_shuffled['location_b'] == 1]\n",
    "x_whole_c = df_shuffled[df_shuffled['location_c'] == 1]\n",
    "\n",
    "y_whole_a = x_whole_a[\"pv_measurement\"]\n",
    "x_whole_a = x_whole_a.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_a', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_b', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_c', axis = 1)\n",
    "\n",
    "y_whole_b = x_whole_b[\"pv_measurement\"]\n",
    "x_whole_b = x_whole_b.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_a', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_b', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_c', axis = 1)\n",
    "\n",
    "y_whole_c = x_whole_c[\"pv_measurement\"]\n",
    "x_whole_c = x_whole_c.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_a', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_b', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_c', axis = 1)\n",
    "cat_features = [\"estimated_flag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 615.2289223\ttotal: 44ms\tremaining: 44s\n",
      "100:\tlearn: 201.4072982\ttotal: 4.65s\tremaining: 41.4s\n",
      "200:\tlearn: 186.0497580\ttotal: 9.27s\tremaining: 36.9s\n",
      "300:\tlearn: 181.1303390\ttotal: 13.7s\tremaining: 31.8s\n",
      "400:\tlearn: 177.2485579\ttotal: 18.1s\tremaining: 27.1s\n",
      "500:\tlearn: 172.6623044\ttotal: 23s\tremaining: 23s\n",
      "600:\tlearn: 163.7204714\ttotal: 28.1s\tremaining: 18.7s\n",
      "700:\tlearn: 154.4498268\ttotal: 32.9s\tremaining: 14s\n",
      "800:\tlearn: 146.6204772\ttotal: 37.4s\tremaining: 9.29s\n",
      "900:\tlearn: 139.9470005\ttotal: 42.3s\tremaining: 4.65s\n",
      "999:\tlearn: 134.5556506\ttotal: 47.1s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x265da4fbfa0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_a = CatBoostRegressor(\n",
    "    max_depth=9,\n",
    "    cat_features=cat_features,\n",
    "    loss_function=\"MAE\",\n",
    "    verbose = 100\n",
    ")\n",
    "best_model_a.fit(x_whole_a, y_whole_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 94.2791670\ttotal: 42.8ms\tremaining: 42.8s\n",
      "100:\tlearn: 25.1394341\ttotal: 4.92s\tremaining: 43.8s\n",
      "200:\tlearn: 21.7299397\ttotal: 9.98s\tremaining: 39.7s\n",
      "300:\tlearn: 20.2992389\ttotal: 14.8s\tremaining: 34.4s\n",
      "400:\tlearn: 19.0627633\ttotal: 19.4s\tremaining: 29s\n",
      "500:\tlearn: 17.9527669\ttotal: 24.1s\tremaining: 24s\n",
      "600:\tlearn: 17.1682651\ttotal: 28.8s\tremaining: 19.1s\n",
      "700:\tlearn: 16.6932226\ttotal: 33.3s\tremaining: 14.2s\n",
      "800:\tlearn: 15.8845163\ttotal: 37.8s\tremaining: 9.38s\n",
      "900:\tlearn: 15.3506132\ttotal: 42.3s\tremaining: 4.65s\n",
      "999:\tlearn: 14.9515366\ttotal: 47.2s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x26585412260>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_b = CatBoostRegressor(\n",
    "    max_depth=9,\n",
    "    cat_features=cat_features,\n",
    "    loss_function=\"MAE\",\n",
    "    verbose = 100\n",
    ")\n",
    "best_model_b.fit(x_whole_b, y_whole_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 78.8208115\ttotal: 23.9ms\tremaining: 23.9s\n",
      "100:\tlearn: 21.4396391\ttotal: 4.52s\tremaining: 40.2s\n",
      "200:\tlearn: 18.5151414\ttotal: 9.49s\tremaining: 37.7s\n",
      "300:\tlearn: 17.0987782\ttotal: 14.6s\tremaining: 33.9s\n",
      "400:\tlearn: 16.0406782\ttotal: 19.3s\tremaining: 28.8s\n",
      "500:\tlearn: 15.0533016\ttotal: 23.8s\tremaining: 23.8s\n",
      "600:\tlearn: 14.2314945\ttotal: 29.4s\tremaining: 19.5s\n",
      "700:\tlearn: 13.6329705\ttotal: 33.9s\tremaining: 14.5s\n",
      "800:\tlearn: 13.1507093\ttotal: 38.2s\tremaining: 9.49s\n",
      "900:\tlearn: 12.7614489\ttotal: 42.3s\tremaining: 4.65s\n",
      "999:\tlearn: 12.4840959\ttotal: 46.4s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x265854bfac0>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_c = CatBoostRegressor(\n",
    "    max_depth=9,\n",
    "    cat_features=cat_features,\n",
    "    loss_function = \"MAE\",\n",
    "    verbose = 100\n",
    ")\n",
    "best_model_c.fit(x_whole_c, y_whole_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole_a = x_test_whole[x_test_whole['location_a'] == 1]\n",
    "x_whole_b = x_test_whole[x_test_whole['location_b'] == 1]\n",
    "x_whole_c = x_test_whole[x_test_whole['location_c'] == 1]\n",
    "\n",
    "x_whole_a = x_whole_a.drop('location_a', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_b', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_c', axis = 1)\n",
    "\n",
    "x_whole_b = x_whole_b.drop('location_a', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_b', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_c', axis = 1)\n",
    "\n",
    "x_whole_c = x_whole_c.drop('location_a', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_b', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_c', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hour: 0, min_value_a: 0.0, max_value_a: 3.3, min_value_b: -0.0, max_value_b: -0.0, min_value_c: 0.0, max_value_c: 0.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hour: 1, min_value_a: 0.0, max_value_a: 53.68, min_value_b: -0.0, max_value_b: 12.075, min_value_c: 0.0, max_value_c: 9.8\n",
      "hour: 2, min_value_a: 0.0, max_value_a: 233.64000000000001, min_value_b: -0.0, max_value_b: 68.1375, min_value_c: 0.0, max_value_c: 39.2\n",
      "hour: 3, min_value_a: 0.0, max_value_a: 439.12, min_value_b: -0.0, max_value_b: 138.0, min_value_c: 0.0, max_value_c: 88.2\n",
      "hour: 4, min_value_a: 0.0, max_value_a: 1046.98, min_value_b: -0.0, max_value_b: 307.05, min_value_c: 0.0, max_value_c: 176.4\n",
      "hour: 5, min_value_a: 0.0, max_value_a: 2049.08, min_value_b: -0.0, max_value_b: 452.8125, min_value_c: 0.0, max_value_c: 264.6\n",
      "hour: 6, min_value_a: 0.0, max_value_a: 3244.78, min_value_b: -0.0, max_value_b: 681.375, min_value_c: 0.0, max_value_c: 499.8\n",
      "hour: 7, min_value_a: 0.0, max_value_a: 4266.46, min_value_b: -0.0, max_value_b: 865.0875, min_value_c: 0.0, max_value_c: 705.6\n",
      "hour: 8, min_value_a: 0.0, max_value_a: 5048.780000000001, min_value_b: -0.0, max_value_b: 997.9125, min_value_c: 0.0, max_value_c: 784.0\n",
      "hour: 9, min_value_a: 0.0, max_value_a: 5477.339999999999, min_value_b: -0.0, max_value_b: 1091.925, min_value_c: 0.0, max_value_c: 882.0000000000001\n",
      "hour: 10, min_value_a: 0.0, max_value_a: 5733.42, min_value_b: -0.0, max_value_b: 1146.2625, min_value_c: 0.0, max_value_c: 980.0000000000001\n",
      "hour: 11, min_value_a: 0.0, max_value_a: 5651.8, min_value_b: -0.0, max_value_b: 1120.3875, min_value_c: 0.0, max_value_c: 999.6\n",
      "hour: 12, min_value_a: 0.0, max_value_a: 5344.46, min_value_b: 0.0, max_value_b: 1152.3, min_value_c: 0.0, max_value_c: 950.6\n",
      "hour: 13, min_value_a: 0.0, max_value_a: 5058.900000000001, min_value_b: -0.0, max_value_b: 1053.9750000000001, min_value_c: 0.0, max_value_c: 911.4000000000001\n",
      "hour: 14, min_value_a: 0.0, max_value_a: 4491.3, min_value_b: -0.0, max_value_b: 823.6875, min_value_c: 0.0, max_value_c: 803.6\n",
      "hour: 15, min_value_a: 0.0, max_value_a: 3633.96, min_value_b: -0.0, max_value_b: 723.6375, min_value_c: 0.0, max_value_c: 597.8000000000001\n",
      "hour: 16, min_value_a: 0.0, max_value_a: 2710.4, min_value_b: -0.0, max_value_b: 601.1625, min_value_c: 0.0, max_value_c: 494.90000000000003\n",
      "hour: 17, min_value_a: 0.0, max_value_a: 1606.0, min_value_b: -0.0, max_value_b: 363.1125, min_value_c: 0.0, max_value_c: 303.8\n",
      "hour: 18, min_value_a: 0.0, max_value_a: 849.64, min_value_b: -0.0, max_value_b: 242.3625, min_value_c: 0.0, max_value_c: 176.4\n",
      "hour: 19, min_value_a: 0.0, max_value_a: 554.4, min_value_b: -0.0, max_value_b: 145.7625, min_value_c: 0.0, max_value_c: 107.80000000000001\n",
      "hour: 20, min_value_a: 0.0, max_value_a: 197.12, min_value_b: -0.0, max_value_b: 33.637499999999996, min_value_c: 0.0, max_value_c: 29.400000000000002\n",
      "hour: 21, min_value_a: 0.0, max_value_a: 19.139999999999997, min_value_b: -0.0, max_value_b: 13.8, min_value_c: 0.0, max_value_c: 9.8\n",
      "hour: 22, min_value_a: 0.0, max_value_a: 0.66, min_value_b: -0.0, max_value_b: -0.0, min_value_c: -0.0, max_value_c: -0.0\n",
      "hour: 23, min_value_a: 0.0, max_value_a: 0.0, min_value_b: -0.0, max_value_b: -0.0, min_value_c: -0.0, max_value_c: -0.0\n"
     ]
    }
   ],
   "source": [
    "y_predictions_a = best_model_a.predict(x_whole_a)\n",
    "y_predictions_b = best_model_b.predict(x_whole_b)\n",
    "y_predictions_c = best_model_c.predict(x_whole_c)\n",
    "y_predictions = pd.concat([pd.Series(y_predictions_a), pd.Series(y_predictions_b), pd.Series(y_predictions_c)])\n",
    "y_predictions = y_predictions.reset_index(drop=True)\n",
    "\n",
    "# Save the model\n",
    "y_predictions_catboost_1 = postprocess_data(x_test_whole, pd.DataFrame(y_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catboost model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole = x_whole.drop(\"effective_solar_elevation_squared\", axis=1)\n",
    "x_test_whole = x_test_whole.drop(\"effective_solar_elevation_squared\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole[\"pv_measurement\"] = y_whole\n",
    "df_shuffled = x_whole.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "x_whole_a = df_shuffled[df_shuffled['location_a'] == 1]\n",
    "x_whole_b = df_shuffled[df_shuffled['location_b'] == 1]\n",
    "x_whole_c = df_shuffled[df_shuffled['location_c'] == 1]\n",
    "\n",
    "y_whole_a = x_whole_a[\"pv_measurement\"]\n",
    "x_whole_a = x_whole_a.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_a', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_b', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_c', axis = 1)\n",
    "\n",
    "y_whole_b = x_whole_b[\"pv_measurement\"]\n",
    "x_whole_b = x_whole_b.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_a', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_b', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_c', axis = 1)\n",
    "\n",
    "y_whole_c = x_whole_c[\"pv_measurement\"]\n",
    "x_whole_c = x_whole_c.drop(\"pv_measurement\", axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_a', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_b', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_c', axis = 1)\n",
    "cat_features = [\"estimated_flag\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 616.9363093\ttotal: 49.4ms\tremaining: 49.3s\n",
      "100:\tlearn: 199.9656401\ttotal: 4.36s\tremaining: 38.8s\n",
      "200:\tlearn: 185.3356686\ttotal: 8.66s\tremaining: 34.4s\n",
      "300:\tlearn: 180.5070612\ttotal: 12.6s\tremaining: 29.4s\n",
      "400:\tlearn: 176.7196537\ttotal: 16.9s\tremaining: 25.2s\n",
      "500:\tlearn: 172.6197441\ttotal: 21.2s\tremaining: 21.1s\n",
      "600:\tlearn: 163.7581476\ttotal: 25.6s\tremaining: 17s\n",
      "700:\tlearn: 153.8238135\ttotal: 29.9s\tremaining: 12.8s\n",
      "800:\tlearn: 147.6203702\ttotal: 34.2s\tremaining: 8.49s\n",
      "900:\tlearn: 141.6572310\ttotal: 38.5s\tremaining: 4.22s\n",
      "999:\tlearn: 136.7908527\ttotal: 42.7s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x26601b288e0>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_a = CatBoostRegressor(\n",
    "    max_depth=9,\n",
    "    cat_features=cat_features,\n",
    "    loss_function=\"MAE\",\n",
    "    verbose = 100\n",
    ")\n",
    "best_model_a.fit(x_whole_a, y_whole_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 94.1836286\ttotal: 42.2ms\tremaining: 42.2s\n",
      "100:\tlearn: 25.3949714\ttotal: 4.46s\tremaining: 39.7s\n",
      "200:\tlearn: 22.1209997\ttotal: 8.85s\tremaining: 35.2s\n",
      "300:\tlearn: 20.7702607\ttotal: 13.3s\tremaining: 30.8s\n",
      "400:\tlearn: 19.4111036\ttotal: 17.4s\tremaining: 26s\n",
      "500:\tlearn: 18.1584056\ttotal: 21.9s\tremaining: 21.8s\n",
      "600:\tlearn: 17.4038913\ttotal: 26.6s\tremaining: 17.7s\n",
      "700:\tlearn: 16.7939747\ttotal: 31.2s\tremaining: 13.3s\n",
      "800:\tlearn: 16.1720278\ttotal: 35.4s\tremaining: 8.79s\n",
      "900:\tlearn: 15.6136213\ttotal: 39.6s\tremaining: 4.35s\n",
      "999:\tlearn: 15.1423810\ttotal: 43.9s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x26601b5ec80>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_b = CatBoostRegressor(\n",
    "    max_depth=9,\n",
    "    cat_features=cat_features,\n",
    "    loss_function = \"MAE\",\n",
    "    verbose = 100\n",
    ")\n",
    "best_model_b.fit(x_whole_b, y_whole_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 78.7273693\ttotal: 66.6ms\tremaining: 1m 6s\n",
      "100:\tlearn: 21.3078061\ttotal: 4.86s\tremaining: 43.3s\n",
      "200:\tlearn: 18.2258783\ttotal: 9.32s\tremaining: 37.1s\n",
      "300:\tlearn: 17.0883950\ttotal: 13.4s\tremaining: 31.2s\n",
      "400:\tlearn: 16.1611729\ttotal: 17.7s\tremaining: 26.4s\n",
      "500:\tlearn: 15.2436549\ttotal: 21.8s\tremaining: 21.7s\n",
      "600:\tlearn: 14.4853484\ttotal: 26s\tremaining: 17.3s\n",
      "700:\tlearn: 13.7033024\ttotal: 30.3s\tremaining: 12.9s\n",
      "800:\tlearn: 13.1848967\ttotal: 34.7s\tremaining: 8.63s\n",
      "900:\tlearn: 12.7181220\ttotal: 39.3s\tremaining: 4.32s\n",
      "999:\tlearn: 12.3918690\ttotal: 43.6s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x26601b25450>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_c = CatBoostRegressor(\n",
    "    max_depth=9,\n",
    "    cat_features=cat_features,\n",
    "    loss_function = \"MAE\",\n",
    "    verbose = 100\n",
    ")\n",
    "best_model_c.fit(x_whole_c, y_whole_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_whole_a = x_test_whole[x_test_whole['location_a'] == 1]\n",
    "x_whole_b = x_test_whole[x_test_whole['location_b'] == 1]\n",
    "x_whole_c = x_test_whole[x_test_whole['location_c'] == 1]\n",
    "\n",
    "x_whole_a = x_whole_a.drop('location_a', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_b', axis = 1)\n",
    "x_whole_a = x_whole_a.drop('location_c', axis = 1)\n",
    "\n",
    "x_whole_b = x_whole_b.drop('location_a', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_b', axis = 1)\n",
    "x_whole_b = x_whole_b.drop('location_c', axis = 1)\n",
    "\n",
    "x_whole_c = x_whole_c.drop('location_a', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_b', axis = 1)\n",
    "x_whole_c = x_whole_c.drop('location_c', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hour: 0, min_value_a: 0.0, max_value_a: 3.3, min_value_b: -0.0, max_value_b: -0.0, min_value_c: 0.0, max_value_c: 0.0"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "hour: 1, min_value_a: 0.0, max_value_a: 53.68, min_value_b: -0.0, max_value_b: 12.075, min_value_c: 0.0, max_value_c: 9.8\n",
      "hour: 2, min_value_a: 0.0, max_value_a: 233.64000000000001, min_value_b: -0.0, max_value_b: 68.1375, min_value_c: 0.0, max_value_c: 39.2\n",
      "hour: 3, min_value_a: 0.0, max_value_a: 439.12, min_value_b: -0.0, max_value_b: 138.0, min_value_c: 0.0, max_value_c: 88.2\n",
      "hour: 4, min_value_a: 0.0, max_value_a: 1046.98, min_value_b: -0.0, max_value_b: 307.05, min_value_c: 0.0, max_value_c: 176.4\n",
      "hour: 5, min_value_a: 0.0, max_value_a: 2049.08, min_value_b: -0.0, max_value_b: 452.8125, min_value_c: 0.0, max_value_c: 264.6\n",
      "hour: 6, min_value_a: 0.0, max_value_a: 3244.78, min_value_b: -0.0, max_value_b: 681.375, min_value_c: 0.0, max_value_c: 499.8\n",
      "hour: 7, min_value_a: 0.0, max_value_a: 4266.46, min_value_b: -0.0, max_value_b: 865.0875, min_value_c: 0.0, max_value_c: 705.6\n",
      "hour: 8, min_value_a: 0.0, max_value_a: 5048.780000000001, min_value_b: -0.0, max_value_b: 997.9125, min_value_c: 0.0, max_value_c: 784.0\n",
      "hour: 9, min_value_a: 0.0, max_value_a: 5477.339999999999, min_value_b: -0.0, max_value_b: 1091.925, min_value_c: 0.0, max_value_c: 882.0000000000001\n",
      "hour: 10, min_value_a: 0.0, max_value_a: 5733.42, min_value_b: -0.0, max_value_b: 1146.2625, min_value_c: 0.0, max_value_c: 980.0000000000001\n",
      "hour: 11, min_value_a: 0.0, max_value_a: 5651.8, min_value_b: -0.0, max_value_b: 1120.3875, min_value_c: 0.0, max_value_c: 999.6\n",
      "hour: 12, min_value_a: 0.0, max_value_a: 5344.46, min_value_b: 0.0, max_value_b: 1152.3, min_value_c: 0.0, max_value_c: 950.6\n",
      "hour: 13, min_value_a: 0.0, max_value_a: 5058.900000000001, min_value_b: -0.0, max_value_b: 1053.9750000000001, min_value_c: 0.0, max_value_c: 911.4000000000001\n",
      "hour: 14, min_value_a: 0.0, max_value_a: 4491.3, min_value_b: -0.0, max_value_b: 823.6875, min_value_c: 0.0, max_value_c: 803.6\n",
      "hour: 15, min_value_a: 0.0, max_value_a: 3633.96, min_value_b: -0.0, max_value_b: 723.6375, min_value_c: 0.0, max_value_c: 597.8000000000001\n",
      "hour: 16, min_value_a: 0.0, max_value_a: 2710.4, min_value_b: -0.0, max_value_b: 601.1625, min_value_c: 0.0, max_value_c: 494.90000000000003\n",
      "hour: 17, min_value_a: 0.0, max_value_a: 1606.0, min_value_b: -0.0, max_value_b: 363.1125, min_value_c: 0.0, max_value_c: 303.8\n",
      "hour: 18, min_value_a: 0.0, max_value_a: 849.64, min_value_b: -0.0, max_value_b: 242.3625, min_value_c: 0.0, max_value_c: 176.4\n",
      "hour: 19, min_value_a: 0.0, max_value_a: 554.4, min_value_b: -0.0, max_value_b: 145.7625, min_value_c: 0.0, max_value_c: 107.80000000000001\n",
      "hour: 20, min_value_a: 0.0, max_value_a: 197.12, min_value_b: -0.0, max_value_b: 33.637499999999996, min_value_c: 0.0, max_value_c: 29.400000000000002\n",
      "hour: 21, min_value_a: 0.0, max_value_a: 19.139999999999997, min_value_b: -0.0, max_value_b: 13.8, min_value_c: 0.0, max_value_c: 9.8\n",
      "hour: 22, min_value_a: 0.0, max_value_a: 0.66, min_value_b: -0.0, max_value_b: -0.0, min_value_c: -0.0, max_value_c: -0.0\n",
      "hour: 23, min_value_a: 0.0, max_value_a: 0.0, min_value_b: -0.0, max_value_b: -0.0, min_value_c: -0.0, max_value_c: -0.0\n",
      "   id    prediction\n",
      "0   0  0.000000e+00\n",
      "1   1  5.292587e-07\n",
      "2   2  5.308785e-07\n",
      "3   3  4.747112e+01\n",
      "4   4  3.363666e+02\n"
     ]
    }
   ],
   "source": [
    "y_predictions_a = best_model_a.predict(x_whole_a)\n",
    "y_predictions_b = best_model_b.predict(x_whole_b)\n",
    "y_predictions_c = best_model_c.predict(x_whole_c)\n",
    "y_predictions = pd.concat([pd.Series(y_predictions_a), pd.Series(y_predictions_b), pd.Series(y_predictions_c)])\n",
    "y_predictions = y_predictions.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Save the model\n",
    "y_predictions_catboost_2 = postprocess_data(x_test_whole, pd.DataFrame(y_predictions))\n",
    "save_predictions(y_predictions, 'catboost without modified solar elevation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id    prediction\n",
      "0   0  0.000000e+00\n",
      "1   1  1.764196e-07\n",
      "2   2  4.591205e-01\n",
      "3   3  5.144618e+01\n",
      "4   4  2.752352e+02\n"
     ]
    }
   ],
   "source": [
    "y_predictions_autogluon.rename(columns={'existing_column_name': 'pred_autogluon'}, inplace=True)\n",
    "y_predictions_catboost_1.rename(columns={'existing_column_name': 'pred_catboost_1'}, inplace=True)\n",
    "y_predictions_catboost_2.rename(columns={'existing_column_name': 'pred_catboost_2'}, inplace=True)\n",
    "\n",
    "combined_df = pd.concat([y_predictions_autogluon, y_predictions_catboost_1, y_predictions_catboost_2], axis=1)\n",
    "\n",
    "\n",
    "combined_df['average_prediction'] = combined_df.mean(axis=1)\n",
    "\n",
    "save_predictions(combined_df[\"average_prediction\"], \"average autogluon cat stack 1435\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
