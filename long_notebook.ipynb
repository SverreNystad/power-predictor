{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team name: Group 6\n",
    "* Full name: Sverre Nystad, StudentNr: 56882,  Kandidatnr: 10003, Kaggle Name: Sverre Nystad\n",
    "* Full name: Gunnar Nystad, StudentNr: 527760, Kandidatnr: 10344, Kaggle Name: Gunnar Nystad\n",
    "* Full name: Peter Skoland, StudentNr: 528091, Kandidatnr 10307,  Kaggle Name: Peter Skoland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents:\n",
    "* Importing of external libraries \n",
    "* Importing of data\n",
    "* Importing of own libraries\n",
    "* Exploratory data analysis\n",
    "* Exploratory Feature Engineering\n",
    "* Exploratory PCA and CCA\n",
    "* Different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "%pip install statsmodels\n",
    "%pip install scikit-learn\n",
    "plt.style.use('ggplot')\n",
    "pd.set_option('display.max_columns', 200)\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.gofplots import qqplot\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_RAW_DATA_LOCATION = \"data/raw/\"\n",
    "\n",
    "def get_raw_data():\n",
    "    \"\"\"\n",
    "    Utility function to load the raw data from the data/raw folder.\n",
    "\n",
    "    Returns:\n",
    "        train_a (pd.DataFrame): The training targets for the A dataset.\n",
    "        train_b (pd.DataFrame): The training targets for the B dataset.\n",
    "        train_c (pd.DataFrame): The training targets for the C dataset.\n",
    "        X_train_estimated_a (pd.DataFrame): The estimated training features for the A dataset.\n",
    "        X_train_estimated_b (pd.DataFrame): The estimated training features for the B dataset.\n",
    "        X_train_estimated_c (pd.DataFrame): The estimated training features for the C dataset.\n",
    "        X_train_observed_a (pd.DataFrame): The observed training features for the A dataset.\n",
    "        X_train_observed_b (pd.DataFrame): The observed training features for the B dataset.\n",
    "        X_train_observed_c (pd.DataFrame): The observed training features for the C dataset.\n",
    "        X_test_estimated_a (pd.DataFrame): The estimated test features for the A dataset.\n",
    "        X_test_estimated_b (pd.DataFrame): The estimated test features for the B dataset.\n",
    "        X_test_estimated_c (pd.DataFrame): The estimated test features for the C dataset.\n",
    "    \"\"\"\n",
    "    train_a = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}A/train_targets.parquet')\n",
    "    train_b = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}B/train_targets.parquet')\n",
    "    train_c = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}C/train_targets.parquet')\n",
    "    X_train_estimated_a = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}A/X_train_estimated.parquet')\n",
    "    X_train_estimated_b = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}B/X_train_estimated.parquet')\n",
    "    X_train_estimated_c = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}C/X_train_estimated.parquet')\n",
    "    X_train_observed_a = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}A/X_train_observed.parquet')\n",
    "    X_train_observed_b = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}B/X_train_observed.parquet')\n",
    "    X_train_observed_c = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}C/X_train_observed.parquet')\n",
    "    X_test_estimated_a = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}A/X_test_estimated.parquet')\n",
    "    X_test_estimated_b = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}B/X_test_estimated.parquet')\n",
    "    X_test_estimated_c = pd.read_parquet(f'{PATH_RAW_DATA_LOCATION}C/X_test_estimated.parquet')\n",
    "\n",
    "    return train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c\n",
    "\n",
    "\n",
    "\n",
    "def get_all_features() -> list:\n",
    "    \"\"\"\n",
    "    Utility function to get all features from the raw data.\n",
    "\n",
    "    Returns:\n",
    "        features (list): A list of all features.\n",
    "    \"\"\"\n",
    "    _, _, _, X_train_estimated_a, _, _, _, _, _, _, _, _ = get_raw_data()\n",
    "    print(X_train_estimated_a.keys())\n",
    "    all_features = X_train_estimated_a.keys()[0]\n",
    "    all_featues = [feature for feature in all_features if feature != 'date_forecast']\n",
    "    return all_featues\n",
    "\n",
    "def get_tests() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Utility function to load the raw data from the data/raw folder.q\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(f'{PATH_RAW_DATA_LOCATION}test.csv')\n",
    "    return test\n",
    "\n",
    "def create_preprocessed_data():\n",
    "    _, _, _, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, _, _, _ = get_raw_data()\n",
    "    X_train_estimated_a = create_expected_pv_based_on_previous_years_same_day(X_train_estimated_a)\n",
    "    X_train_estimated_b = create_expected_pv_based_on_previous_years_same_day(X_train_estimated_b)\n",
    "    X_train_estimated_c = create_expected_pv_based_on_previous_years_same_day(X_train_estimated_c)\n",
    "    X_train_observed_a = create_expected_pv_based_on_previous_years_same_day(X_train_observed_a)\n",
    "    X_train_observed_b = create_expected_pv_based_on_previous_years_same_day(X_train_observed_b)\n",
    "    X_train_observed_c = create_expected_pv_based_on_previous_years_same_day(X_train_observed_c)\n",
    "\n",
    "def create_expected_pv_based_on_previous_years_same_day(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a mean pv_measurement for each data point based on the previous years same day and hour\n",
    "    Add this as a feature to the data frame\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing at least columns: \n",
    "                           location_a, location_b, location_c, date_forecast, \n",
    "                           pv_measurement, sin_day_of_year, cos_day_of_year, sin_hour, cos_hour\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with additional feature of mean pv_measurement based on historical data\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Identify the location from the binary flags\n",
    "    df['location'] = df[['location_a', 'location_b', 'location_c']].idxmax(axis=1)\n",
    "    \n",
    "    # Calculate mean pv_measurement for each location, sin_day_of_year, cos_day_of_year, sin_hour, and cos_hour\n",
    "    mean_pv = df.groupby(['location', 'sin_day_of_year', 'cos_day_of_year', 'sin_hour', 'cos_hour'])['pv_measurement'].mean().reset_index()\n",
    "    mean_pv.rename(columns={'pv_measurement': 'mean_pv_measurement'}, inplace=True)\n",
    "    \n",
    "    # Merge mean_pv_measurement back to the original DataFrame\n",
    "    df = pd.merge(df, mean_pv, on=['location', 'sin_day_of_year', 'cos_day_of_year', 'sin_hour', 'cos_hour'], how='left')\n",
    "    df.drop(columns=['location'], inplace=True)\n",
    "    return df\n",
    "if __name__ == \"__main__\":\n",
    "    data = get_raw_data()\n",
    "    print(data[0].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import skew\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\n",
    "import math\n",
    "\n",
    "\n",
    "def prepare_data(\n",
    "    train_observed: pd.DataFrame,\n",
    "    train_estimated: pd.DataFrame,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    drop_features: bool = True,\n",
    ") -> Tuple[\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.Series,\n",
    "    pd.Series,\n",
    "    pd.DataFrame,\n",
    "    pd.DataFrame,\n",
    "    pd.Series,\n",
    "    pd.Series,\n",
    "]:\n",
    "    \"\"\"\n",
    "    Prepares the data for modeling by handling missing values and splitting the data.\n",
    "\n",
    "    Args:\n",
    "    train_observed (pd.DataFrame): The aligned training DataFrame with observed features.\n",
    "    train_estimated (pd.DataFrame): The aligned training DataFrame with estimated features.\n",
    "    test_size (float): The proportion of the dataset to include in the test split.\n",
    "    random_state (int): Controls the shuffling applied to the data before applying the split.\n",
    "\n",
    "    Returns:\n",
    "    X_train_obs (pd.DataFrame): The training features with observed data.\n",
    "    X_val_obs (pd.DataFrame): The validation features with observed data.\n",
    "    y_train_obs (pd.Series): The training target with observed data.\n",
    "    y_val_obs (pd.Series): The validation target with observed data.\n",
    "    X_train_est (pd.DataFrame): The training features with estimated data.\n",
    "    X_val_est (pd.DataFrame): The validation features with estimated data.\n",
    "    y_train_est (pd.Series): The training target with estimated data.\n",
    "    y_val_est (pd.Series): The validation target with estimated data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove missing features\n",
    "    train_observed = remove_missing_features(train_observed)\n",
    "    train_estimated = remove_missing_features(train_estimated)\n",
    "\n",
    "    # Handle missing values (e.g., imputation, removal)\n",
    "    train_observed_clean = train_observed.dropna(\n",
    "        subset=[\"visibility:m\", \"pv_measurement\"]\n",
    "    )\n",
    "    train_estimated_clean = train_estimated.dropna(\n",
    "        subset=[\"visibility:m\", \"pv_measurement\"]\n",
    "    )\n",
    "\n",
    "    # Remove discrepancies\n",
    "    train_observed_clean = remove_discrepancies(train_observed_clean)\n",
    "    train_estimated_clean = remove_discrepancies(train_estimated_clean)\n",
    "\n",
    "    # Feature engineer\n",
    "    train_observed_clean = feature_engineer(train_observed_clean)\n",
    "    train_estimated_clean = feature_engineer(train_estimated_clean)\n",
    "\n",
    "    # Split the data into features (X) and target (y)\n",
    "\n",
    "    y_obs = train_observed_clean[\"pv_measurement\"]\n",
    "\n",
    "    if drop_features:\n",
    "        X_obs = train_observed_clean.drop(\n",
    "            columns=[\"time\", \"pv_measurement\", \"date_forecast\", \"date_calc\"],\n",
    "            errors=\"ignore\",\n",
    "        )\n",
    "    else:\n",
    "        X_obs = train_observed_clean\n",
    "\n",
    "    if drop_features:\n",
    "        X_est = train_estimated_clean.drop(\n",
    "            columns=[\"time\", \"pv_measurement\", \"date_calc\", \"date_forecast\"],\n",
    "            errors=\"ignore\",\n",
    "        )\n",
    "    else:\n",
    "        X_est = train_estimated_clean\n",
    "\n",
    "    y_est = train_estimated_clean[\"pv_measurement\"]\n",
    "\n",
    "    # Split the data into training and validation sets\n",
    "    X_train_obs, X_val_obs, y_train_obs, y_val_obs = train_test_split(\n",
    "        X_obs, y_obs, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    X_train_est, X_val_est, y_train_est, y_val_est = train_test_split(\n",
    "        X_est, y_est, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        X_train_obs,\n",
    "        X_val_obs,\n",
    "        y_train_obs,\n",
    "        y_val_obs,\n",
    "        X_train_est,\n",
    "        X_val_est,\n",
    "        y_train_est,\n",
    "        y_val_est,\n",
    "    )\n",
    "\n",
    "\n",
    "def get_location_datasets(\n",
    "    df: pd.DataFrame,\n",
    ") -> (pd.DataFrame, pd.DataFrame, pd.DataFrame):\n",
    "    locations = [\"location_a\", \"location_b\", \"location_c\"]\n",
    "    x_a = df[df[\"location_a\"] == 1]\n",
    "    x_a = x_a.drop(locations, axis=1)\n",
    "    y_a = x_a[\"pv_measurement\"]\n",
    "    if \"pv_measurement\" in x_a.columns:\n",
    "        x_a = x_a.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "    x_b = df[df[\"location_b\"] == 1]\n",
    "    x_b = x_b.drop(locations, axis=1)\n",
    "    y_b = x_b[\"pv_measurement\"]\n",
    "    if \"pv_measurement\" in x_b.columns:\n",
    "        x_b = x_b.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "    x_c = df[df[\"location_c\"] == 1]\n",
    "    x_c = x_c.drop(locations, axis=1)\n",
    "    y_c = x_c[\"pv_measurement\"]\n",
    "    if \"pv_measurement\" in x_b.columns:\n",
    "        x_b = x_b.drop(\"pv_measurement\", axis=1)\n",
    "\n",
    "    return (x_a, x_b, x_c, y_a, y_b, y_c)\n",
    "\n",
    "\n",
    "def remove_missing_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.drop(\"snow_density:kgm3\", axis=1)\n",
    "    df = df.drop(\"ceiling_height_agl:m\", axis=1)\n",
    "    df[\"cloud_base_agl:m\"] = df[\"cloud_base_agl:m\"].fillna(0)\n",
    "    df = df.drop(\"elevation:m\", axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_discrepancies(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = remove_positive_pv_in_night(df)\n",
    "    df = remove_night_light_discrepancies(df)\n",
    "    df = remove_zero_value_discrepancies(df)\n",
    "    df = remove_faulty_zero_measurements_for_direct_sun_light(df)\n",
    "    # df = remove_outliers(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_positive_pv_in_night(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove positive pv measurements when is_day is 0 and pv_measurement is positive and pv_measurement is the same next timestep\n",
    "    \"\"\"\n",
    "    # Remove positive pv measurements when is_day is 0 and pv_measurement is positive and pv_measurement is the same next timestep\n",
    "    df = df.drop(\n",
    "        df[\n",
    "            (df[\"is_day:idx\"] == 0)\n",
    "            & (df[\"pv_measurement\"] > 0)\n",
    "            & (df[\"pv_measurement\"] == df[\"pv_measurement\"].shift(1))\n",
    "        ].index\n",
    "    )\n",
    "\n",
    "    # Remove positive pv measurements when sun_elevation is negative\n",
    "    threshold = -10\n",
    "    df = df.drop(\n",
    "        df[(df[\"sun_elevation:d\"] < threshold) & (df[\"pv_measurement\"] > 0)].index\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_outliers(\n",
    "    df: pd.DataFrame, lower_bound: float = 0.1, upper_bound: float = 0.9\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Removing outliers using IQR method\n",
    "    \"\"\"\n",
    "\n",
    "    columns_to_check = [col for col in df.columns if col != \"pv_measurement\"]\n",
    "    for col in columns_to_check:\n",
    "        # Calculate IQR\n",
    "        Q1 = df[col].quantile(lower_bound)\n",
    "        Q3 = df[col].quantile(upper_bound)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        # Define outlier bounds\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Filter the data\n",
    "        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_night_light_discrepancies(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Remove all rows where pv_measurement has the same value for 6 timesteps and not is 0 remove them\n",
    "\n",
    "    # Step 1: Identify runs of equal, non-zero values\n",
    "    df[\"group\"] = (\n",
    "        (df[\"pv_measurement\"] != df[\"pv_measurement\"].shift())\n",
    "        | (df[\"pv_measurement\"] == 0)\n",
    "    ).cumsum()\n",
    "\n",
    "    # Step 2: Count occurrences in each run\n",
    "    counts = df.groupby(\"group\")[\"pv_measurement\"].transform(\"count\")\n",
    "\n",
    "    # Step 3: Identify groups to remove\n",
    "    to_remove = (counts >= 6) & (df[\"pv_measurement\"] != 0)\n",
    "\n",
    "    # Step 4: Remove those rows\n",
    "    df_cleaned = df[~to_remove].drop(columns=[\"group\"])\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "def remove_zero_value_discrepancies(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Remove all rows where pv_measurement has the same value for 100 timesteps and is 0 remove them\n",
    "\n",
    "    # Didn't do anything lol\n",
    "\n",
    "    # Step 1: Identify runs of equal, non-zero values\n",
    "    return df\n",
    "    df[\"group\"] = (\n",
    "        (df[\"pv_measurement\"] != df[\"pv_measurement\"].shift())\n",
    "        | (df[\"pv_measurement\"] == 0)\n",
    "    ).cumsum()\n",
    "\n",
    "    # Step 2: Count occurrences in each run\n",
    "    counts = df.groupby(\"group\")[\"pv_measurement\"].transform(\"count\")\n",
    "\n",
    "    # Step 3: Identify groups to remove\n",
    "    to_remove = (counts >= 50) & (df[\"pv_measurement\"] == 0) & (df[\"is_day:idx\"] == 1)\n",
    "\n",
    "    # Step 4: Remove those rows\n",
    "    df_cleaned = df[~to_remove].drop(columns=[\"group\"])\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "def remove_faulty_zero_measurements_for_direct_sun_light(\n",
    "    df: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\" \"\"\"\n",
    "    mask = ((df[\"diffuse_rad:W\"] + df[\"direct_rad:W\"]) >= 30) & (\n",
    "        df[\"pv_measurement\"] == 0\n",
    "    )\n",
    "    df = df[~mask]\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_engineer(data_frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    data_frame = create_time_features_from_date(data_frame)\n",
    "    # data_frame = create_expected_pv_based_on_previous_years_same_day(data_frame)\n",
    "    data_frame[\"sun_product\"] = data_frame[\"diffuse_rad:W\"] * data_frame[\"direct_rad:W\"]\n",
    "\n",
    "    data_frame[\"modified_solar_elevation\"] = np.where(\n",
    "        data_frame[\"sun_elevation:d\"] <= 0,\n",
    "        0,\n",
    "        np.sin(np.radians(data_frame[\"sun_elevation:d\"])),\n",
    "    )\n",
    "    data_frame = data_frame.drop(\"sun_elevation:d\", axis=1)\n",
    "\n",
    "    data_frame[\"effective_radiation\"] = np.where(\n",
    "        data_frame[\"clear_sky_energy_1h:J\"] == 0,\n",
    "        0,  # or your specified value\n",
    "        data_frame[\"direct_rad_1h:J\"] / data_frame[\"clear_sky_energy_1h:J\"],\n",
    "    )\n",
    "\n",
    "    # Check for the existence of date_calc column\n",
    "    if \"date_calc\" not in data_frame.columns:\n",
    "        data_frame[\"time_since_prediction\"] = 0\n",
    "    else:\n",
    "        data_frame[\"time_since_prediction\"] = 1\n",
    "\n",
    "    # data_frame[\"time_since_prediction\"] =\n",
    "\n",
    "    data_frame[\"residual_radiation\"] = (\n",
    "        data_frame[\"clear_sky_rad:W\"]\n",
    "        - data_frame[\"direct_rad:W\"]\n",
    "        - data_frame[\"diffuse_rad:W\"]\n",
    "    )\n",
    "\n",
    "    # WAS WORSE\n",
    "    # data_frame[\"effective_radiation2\"] = np.where(\n",
    "    #     data_frame[\"clear_sky_rad:W\"] == 0,\n",
    "    #     0,  # or your specified value\n",
    "    #     data_frame[\"direct_rad:W\"] / data_frame[\"clear_sky_rad:W\"],\n",
    "    # )\n",
    "\n",
    "    data_frame[\"cloud_ratio\"] = np.where(\n",
    "        data_frame[\"total_cloud_cover:p\"] == 0,\n",
    "        0,  # or your specified value\n",
    "        data_frame[\"effective_cloud_cover:p\"] / data_frame[\"total_cloud_cover:p\"],\n",
    "    )\n",
    "\n",
    "    data_frame[\"diffuse_cloud_conditional_interaction\"] = data_frame[\n",
    "        \"diffuse_rad:W\"\n",
    "    ].where(data_frame[\"effective_cloud_cover:p\"] < 0.3, 0)\n",
    "\n",
    "    data_frame[\"cloud_cover_over_30%\"] = np.where(\n",
    "        data_frame[\"effective_cloud_cover:p\"] > 30, 1, 0\n",
    "    )\n",
    "\n",
    "    data_frame[\"global_horizon_index\"] = (\n",
    "        data_frame[\"diffuse_rad:W\"] + data_frame[\"direct_rad:W\"]\n",
    "    )\n",
    "\n",
    "    data_frame[\"direct_rad_cloud_interaction\"] = data_frame[\"direct_rad:W\"] * (\n",
    "        100 - data_frame[\"effective_cloud_cover:p\"]\n",
    "    )\n",
    "\n",
    "    data_frame[\"modified_solar_elevation_squared\"] = (\n",
    "        data_frame[\"modified_solar_elevation\"] ** 0.5\n",
    "    )\n",
    "    \n",
    "    # data_frame[\"global_horizon_index_squared\"] = data_frame[\"global_horizon_index\"] ** 2\n",
    "\n",
    "    snow_columns = [\n",
    "        \"snow_depth:cm\",\n",
    "        \"fresh_snow_12h:cm\",\n",
    "        \"fresh_snow_1h:cm\",\n",
    "        \"fresh_snow_24h:cm\",\n",
    "        \"fresh_snow_3h:cm\",\n",
    "        \"fresh_snow_6h:cm\",\n",
    "    ]\n",
    "\n",
    "    # Was worse\n",
    "    # data_frame[\"surface_temperature\"] = data_frame.apply(calculate_surface_temp, axis=1)\n",
    "    # data_frame[\"50m_temperature\"] = data_frame.apply(calculate_50m_temp, axis=1)\n",
    "    # data_frame[\"100m_temperature\"] = data_frame.apply(calculate_100m_temp, axis=1)\n",
    "    # data_frame = data_frame.drop(\"t_1000hPa:K\", axis=1)\n",
    "\n",
    "    # data_frame[\"global_horizon_index\"] = (\n",
    "    #     data_frame[\"direct_rad_1h:J\"] + data_frame[\"diffuse_rad_1h:J\"]\n",
    "    # )\n",
    "\n",
    "    # Create a feature that is precip_5min:mm * precip_type_5min.idx\n",
    "    # data_frame[\"any_precip\"] = (\n",
    "    #     data_frame[\"precip_5min:mm\"] * data_frame[\"precip_type_5min:idx\"]\n",
    "    # )\n",
    "\n",
    "    # data_frame[\"sun addition W to 1h ratio\"] = (\n",
    "    #     data_frame[\"global_horizon_index\"]\n",
    "    #     * 3600\n",
    "    #     / (data_frame[\"direct_rad_1h:J\"] + data_frame[\"diffuse_rad_1h:J\"])\n",
    "    # ).fillna(1)\n",
    "\n",
    "    # data_frame[\"rime\"] = data_frame[\"prob_rime:p\"].apply(lambda x: 1 if x > 0 else 0)\n",
    "    # data_frame = data_frame.drop(\"prob_rime:p\", axis=1)\n",
    "\n",
    "    data_frame[\"is_freezing\"] = (data_frame[\"t_1000hPa:K\"] < 273).astype(int)\n",
    "\n",
    "    data_frame[\"is_snow\"] = (data_frame[snow_columns] > 0).any(axis=1).astype(int)\n",
    "    data_frame[\"is_rain\"] = (data_frame[\"precip_5min:mm\"] > 0).astype(int)\n",
    "\n",
    "    data_frame = data_frame.drop(\"snow_drift:idx\", axis=1)\n",
    "    # Was worse\n",
    "    # data_frame = data_frame.drop(\"snow_depth:cm\", axis=1)\n",
    "    # data_frame = data_frame.drop(\"snow_water:kgm2\", axis=1)\n",
    "    # data_frame = data_frame.drop(\"fresh_snow_12h:cm\", axis=1)\n",
    "    # data_frame = data_frame.drop(\"fresh_snow_1h:cm\", axis=1)\n",
    "    # data_frame = data_frame.drop(\"fresh_snow_24h:cm\", axis=1)\n",
    "    # data_frame = data_frame.drop(\"fresh_snow_3h:cm\", axis=1)\n",
    "    # data_frame = data_frame.drop(\"fresh_snow_6h:cm\", axis=1)\n",
    "    # data_frame = data_frame.drop(\"snow_melt_10min:mm\", axis=1)\n",
    "\n",
    "    # data_frame = data_frame.drop(\"msl_pressure:hPa\", axis=1)\n",
    "    # data_frame = data_frame.drop(\"pressure_100m:hPa\", axis=1)\n",
    "    # data_frame = data_frame.drop(\"sfc_pressure:hPa\", axis=1)\n",
    "\n",
    "    # Add maximum pv_measurement based on location\n",
    "    # max_pv_a = 5733.42\n",
    "    # max_pv_b = 1152.3\n",
    "    # max_pv_c = 999.6\n",
    "    # data_frame[\"max_pv_location\"] = np.where(\n",
    "    #     data_frame[\"location_a\"] == 1,\n",
    "    #     max_pv_a,\n",
    "    #     np.where(\n",
    "    #         data_frame[\"location_b\"] == 1,\n",
    "    #         max_pv_b,\n",
    "    #         np.where(data_frame[\"location_c\"] == 1, max_pv_c, np.nan),\n",
    "    #     ),\n",
    "    # )\n",
    "\n",
    "    # # Add average pv_measurement based on location\n",
    "    # average_pv_a = 814.88\n",
    "    # average_pv_b = 129.375\n",
    "    # average_pv_c = 117.6\n",
    "    # data_frame[\"average_pv_location\"] = np.where(\n",
    "    #     data_frame[\"location_a\"] == 1,\n",
    "    #     average_pv_a,\n",
    "    #     np.where(\n",
    "    #         data_frame[\"location_b\"] == 1,\n",
    "    #         average_pv_b,\n",
    "    #         np.where(data_frame[\"location_c\"] == 1, average_pv_c, np.nan),\n",
    "    #     ),\n",
    "    # )\n",
    "\n",
    "    # # Add Maximum_pv_location times global_horizon_index\n",
    "    # data_frame[\"max_pv_location_times_global_horizon_index\"] = (\n",
    "    #     data_frame[\"max_pv_location\"] * data_frame[\"global_horizon_index\"]\n",
    "    # )\n",
    "\n",
    "    # # Add Maximum_pv_location times global_horizon_index\n",
    "    # data_frame[\"average_pv_location_times_global_horizon_index\"] = (\n",
    "    #     data_frame[\"average_pv_location\"] * data_frame[\"global_horizon_index\"]\n",
    "    # )\n",
    "\n",
    "    # data_frame[\"global_horizon_index_temp_ratio\"] = (\n",
    "    #     data_frame[\"max_pv_location\"]\n",
    "    #     * data_frame[\"global_horizon_index\"]\n",
    "    #     / data_frame[\"t_1000hPa:K\"]\n",
    "    # )\n",
    "\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "def ratio(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    # Sample DataFrame (replace this with your actual DataFrame)\n",
    "\n",
    "    # Calculate total irradiance\n",
    "    df[\"total_radiance\"] = df[\"direct_radiance\"] + df[\"diffuse_radiance\"]\n",
    "\n",
    "    # Set up the linear regression problem\n",
    "    X = df[[\"total_radiance\", \"ambient_temp\"]]\n",
    "    y = df[\"pv_measurement\"]\n",
    "\n",
    "    # Train a linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Extract coefficients\n",
    "    a = model.coef_[0]  # Coefficient for total_radiance\n",
    "    b = model.coef_[1]  # Coefficient for ambient_temp\n",
    "    c = model.intercept_\n",
    "\n",
    "    # Estimate the parameters\n",
    "    T_ref = 25  # Reference temperature, you can set this based on your understanding\n",
    "    eta = a  # Assuming that a corresponds to efficiency\n",
    "    beta = (\n",
    "        -b / a\n",
    "    )  # Assuming linear relationship between power drop and temperature increase\n",
    "\n",
    "    # Estimate panel temperature\n",
    "    df[\"T_panel\"] = df[\"ambient_temp\"] + (df[\"total_radiance\"] / a) * (1 - eta)\n",
    "\n",
    "    # Calculate the desired ratio\n",
    "    df[\"ratio\"] = 1 - beta * (df[\"T_panel\"] - T_ref)\n",
    "\n",
    "    # Display the DataFrame\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_surface_temp(row):\n",
    "    # Constants\n",
    "    R = 287  # Specific gas constant for dry air, J kg^-1 K^-1\n",
    "    g = 9.81  # Acceleration due to gravity, m s^-2\n",
    "    lapse_rate = 0.0065  # Average lapse rate, K m^-1\n",
    "\n",
    "    P1 = 1000  # Initial pressure level, hPa\n",
    "    P2 = row[\"sfc_pressure:hPa\"]  # Final pressure level, hPa\n",
    "    T1 = row[\"t_1000hPa:K\"]  # Temperature at P1, K\n",
    "\n",
    "    # Altitude difference using barometric formula (approximation)\n",
    "    delta_h = (R * T1 / g) * np.log(P1 / P2)\n",
    "\n",
    "    # Temperature difference using constant lapse rate\n",
    "    delta_T = lapse_rate * delta_h\n",
    "\n",
    "    # Temperature at P2, converting from K to C\n",
    "    T2_C = T1 - delta_T - 273.15\n",
    "\n",
    "    return T2_C\n",
    "\n",
    "\n",
    "def calculate_50m_temp(row):\n",
    "    # Constants\n",
    "    R = 287  # Specific gas constant for dry air, J kg^-1 K^-1\n",
    "    g = 9.81  # Acceleration due to gravity, m s^-2\n",
    "    lapse_rate = 0.0065  # Average lapse rate, K m^-1\n",
    "\n",
    "    P1 = 1000  # Initial pressure level, hPa\n",
    "    P2 = row[\"pressure_50m:hPa\"]  # Final pressure level, hPa\n",
    "    T1 = row[\"t_1000hPa:K\"]  # Temperature at P1, K\n",
    "\n",
    "    # Altitude difference using barometric formula (approximation)\n",
    "    delta_h = (R * T1 / g) * np.log(P1 / P2)\n",
    "\n",
    "    # Temperature difference using constant lapse rate\n",
    "    delta_T = lapse_rate * delta_h\n",
    "\n",
    "    # Temperature at P2, converting from K to C\n",
    "    T2_C = T1 - delta_T - 273.15\n",
    "\n",
    "    return T2_C\n",
    "\n",
    "\n",
    "def calculate_100m_temp(row):\n",
    "    # Constants\n",
    "    R = 287  # Specific gas constant for dry air, J kg^-1 K^-1\n",
    "    g = 9.81  # Acceleration due to gravity, m s^-2\n",
    "    lapse_rate = 0.0065  # Average lapse rate, K m^-1\n",
    "\n",
    "    P1 = 1000  # Initial pressure level, hPa\n",
    "    P2 = row[\"pressure_100m:hPa\"]  # Final pressure level, hPa\n",
    "    T1 = row[\"t_1000hPa:K\"]  # Temperature at P1, K\n",
    "\n",
    "    # Altitude difference using barometric formula (approximation)\n",
    "    delta_h = (R * T1 / g) * np.log(P1 / P2)\n",
    "\n",
    "    # Temperature difference using constant lapse rate\n",
    "    delta_T = lapse_rate * delta_h\n",
    "\n",
    "    # Temperature at P2, converting from K to C\n",
    "    T2_C = T1 - delta_T - 273.15\n",
    "\n",
    "    return T2_C\n",
    "\n",
    "\n",
    "def create_time_features_from_date(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a new data frame with new features from date_forecast column.\n",
    "    This will create temporal features from date_forecast that are easier to learn by the model.\n",
    "    It creates the following features: month, season, year, day_of_year, day_segment.\n",
    "    All of the new features are int type.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Data frame with date_forecast column.\n",
    "    Returns:\n",
    "        pd.DataFrame: Data frame copy with new features.\n",
    "\n",
    "    \"\"\"\n",
    "    df[\"sin_day_of_year\"] = df[\"date_forecast\"].apply(get_sin_day)\n",
    "    df[\"cos_day_of_year\"] = df[\"date_forecast\"].apply(get_cos_day)\n",
    "    df[\"sin_hour\"] = df[\"date_forecast\"].apply(get_sin_hour)\n",
    "    df[\"cos_hour\"] = df[\"date_forecast\"].apply(get_cos_hour)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def get_sin_hour(date: datetime) -> float:\n",
    "    HOURS_OF_DAY = 24\n",
    "    return math.sin(2 * math.pi * (date.hour) / HOURS_OF_DAY)\n",
    "\n",
    "\n",
    "def get_cos_hour(date: datetime) -> float:\n",
    "    HOURS_OF_DAY = 24\n",
    "    return math.cos(2 * math.pi * (date.hour) / HOURS_OF_DAY)\n",
    "\n",
    "\n",
    "def get_sin_day(date: datetime) -> float:\n",
    "    DAY_OF_YEAR = 365.25  # Add 0.25 to account for leap years\n",
    "    return math.sin(2 * math.pi * (date.timetuple().tm_yday - 1) / DAY_OF_YEAR)\n",
    "\n",
    "\n",
    "def get_cos_day(date: datetime) -> float:\n",
    "    DAY_OF_YEAR = 365.25  # Add 0.25 to account for leap years\n",
    "\n",
    "    return math.cos(2 * math.pi * (date.timetuple().tm_yday - 1) / DAY_OF_YEAR)\n",
    "\n",
    "\n",
    "def create_polynomial_features(\n",
    "    df, columns, degree=2, include_bias=False, interaction_only=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Create polynomial features for specified columns in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "    - columns (list): List of column names for which to create polynomial features.\n",
    "    - degree (int): The degree of the polynomial features. Default is 2.\n",
    "    - include_bias (bool): Whether to include a bias column in the output. Default is False.\n",
    "    - interaction_only (bool): Whether to include only interaction features. Default is False.\n",
    "\n",
    "    Returns:\n",
    "    - df_poly (pd.DataFrame): DataFrame with original and new polynomial features.\n",
    "    \"\"\"\n",
    "    poly = PolynomialFeatures(\n",
    "        degree=degree, include_bias=include_bias, interaction_only=interaction_only\n",
    "    )\n",
    "    poly_features = poly.fit_transform(df[columns])\n",
    "    feature_names = poly.get_feature_names(input_features=columns)\n",
    "\n",
    "    df_poly = pd.DataFrame(poly_features, columns=feature_names, index=df.index)\n",
    "    df_poly = df_poly.drop(\n",
    "        columns=columns\n",
    "    )  # Drop original columns as they are included in the polynomial features\n",
    "\n",
    "    # Concatenate the original DataFrame with the new polynomial features DataFrame\n",
    "    df_poly = pd.concat([df.drop(columns=columns), df_poly], axis=1)\n",
    "\n",
    "    return df_poly\n",
    "\n",
    "\n",
    "def log_transform(df: pd.DataFrame, columns: List[str]):\n",
    "    df_transformed = df.copy()\n",
    "    for column in columns:\n",
    "        df_transformed[column] = np.log1p(df_transformed[column])\n",
    "    return df_transformed\n",
    "\n",
    "\n",
    "def scale_features(df: pd.DataFrame, columns: List[str], method=\"standard\"):\n",
    "    df_scaled = df.copy()\n",
    "    scaler = StandardScaler() if method == \"standard\" else MinMaxScaler()\n",
    "    df_scaled[columns] = scaler.fit_transform(df_scaled[columns])\n",
    "    return df_scaled\n",
    "\n",
    "\n",
    "def trig_transform(df: pd.DataFrame, column: str, period: int):\n",
    "    df_trig = df.copy()\n",
    "    df_trig[f\"{column}_sin\"] = np.sin(2 * math.pi * df_trig[column] / period)\n",
    "    df_trig[f\"{column}_cos\"] = np.cos(2 * math.pi * df_trig[column] / period)\n",
    "    return df_trig\n",
    "\n",
    "\n",
    "def create_expected_pv_based_on_previous_years_same_day(\n",
    "    df: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a mean pv_measurement for each data point based on the previous years same day and hour\n",
    "    Add this as a feature to the data frame\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing at least columns:\n",
    "                           location_a, location_b, location_c, date_forecast,\n",
    "                           pv_measurement, sin_day_of_year, cos_day_of_year, sin_hour, cos_hour\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with additional feature of mean pv_measurement based on historical data\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    # When the data does not contain needed columns, return the original df.\n",
    "    if not all(\n",
    "        col in df.columns\n",
    "        for col in [\n",
    "            \"location_a\",\n",
    "            \"location_b\",\n",
    "            \"location_c\",\n",
    "            \"pv_measurement\",\n",
    "            \"sin_day_of_year\",\n",
    "            \"cos_day_of_year\",\n",
    "            \"sin_hour\",\n",
    "            \"cos_hour\",\n",
    "        ]\n",
    "    ):\n",
    "        return df\n",
    "    # Identify the location from the binary flags\n",
    "    df[\"location\"] = df[[\"location_a\", \"location_b\", \"location_c\"]].idxmax(axis=1)\n",
    "\n",
    "    # Calculate mean pv_measurement for each location, sin_day_of_year, cos_day_of_year, sin_hour, and cos_hour\n",
    "    mean_pv = (\n",
    "        df.groupby(\n",
    "            [\"location\", \"sin_day_of_year\", \"cos_day_of_year\", \"sin_hour\", \"cos_hour\"]\n",
    "        )[\"pv_measurement\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    mean_pv.rename(columns={\"pv_measurement\": \"mean_pv_measurement\"}, inplace=True)\n",
    "\n",
    "    # Merge mean_pv_measurement back to the original DataFrame\n",
    "    df = pd.merge(\n",
    "        df,\n",
    "        mean_pv,\n",
    "        on=[\"location\", \"sin_day_of_year\", \"cos_day_of_year\", \"sin_hour\", \"cos_hour\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "    df.drop(columns=[\"location\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_simple_rolling_mean(\n",
    "    df: pd.DataFrame, column: str, window: int\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a simple rolling mean feature for a given column in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame containing your time-series data.\n",
    "        column: The name of the column for which you want to create lagged features.\n",
    "        window: The size of the window for calculating the rolling mean.\n",
    "                For example, if window=10, it will take the previous 10 days.\n",
    "    \"\"\"\n",
    "    # Ensure the DataFrame is sorted by date\n",
    "    df = df.sort_values(by=\"date_forecast\")\n",
    "\n",
    "    # Ensure 'date_forecast' is in datetime format\n",
    "    df[\"date_forecast\"] = pd.to_datetime(df[\"date_forecast\"])\n",
    "\n",
    "    # Calculate the rolling mean\n",
    "    df[\"rolling_mean_of_\" + column] = df[column].rolling(window=window).mean()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Define a function to identify positively skewed numerical features in a DataFrame\n",
    "def identify_skewed_features(df, skew_threshold=1):\n",
    "    # Select numerical features\n",
    "    num_features = df.select_dtypes(include=[\"float64\", \"int64\"]).columns.tolist()\n",
    "\n",
    "    # Calculate skewness for each numerical feature and filter those that are positively skewed\n",
    "    skewed_features = [\n",
    "        feature for feature in num_features if skew(df[feature]) > skew_threshold\n",
    "    ]\n",
    "\n",
    "    return skewed_features\n",
    "\n",
    "\n",
    "def create_domain_specific_features(df):\n",
    "    \"\"\"\n",
    "    Create domain-specific features for solar energy production.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - df_domain (pd.DataFrame): DataFrame with original and new domain-specific features.\n",
    "    \"\"\"\n",
    "    df_domain = df.copy()\n",
    "\n",
    "    # Create a binary feature representing whether the sky is clear\n",
    "    df_domain[\"is_clear_sky\"] = (df_domain[\"clear_sky_energy_1h:J\"] > 0).astype(int)\n",
    "\n",
    "    # Create a feature representing total sun exposure\n",
    "    df_domain[\"total_sun_exposure\"] = (\n",
    "        df_domain[\"direct_rad:W\"] + df_domain[\"diffuse_rad:W\"]\n",
    "    )\n",
    "\n",
    "    # Create a binary feature representing whether it is raining\n",
    "    df_domain[\"is_raining\"] = (df_domain[\"precip_5min:mm\"] > 0).astype(int)\n",
    "\n",
    "    # Create a binary feature representing whether there is snow cover\n",
    "    df_domain[\"is_snow_cover\"] = (df_domain[\"snow_depth:cm\"] > 0).astype(int)\n",
    "\n",
    "    return df_domain\n",
    "\n",
    "\n",
    "def clean_data(data_frame: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean data frame by removing outliers and NaN values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Data frame with date_forecast column.\n",
    "    Returns:\n",
    "        pd.DataFrame: Data frame copy without outliers and NaN values.\n",
    "\n",
    "    \"\"\"\n",
    "    df = data_frame.copy()\n",
    "    df = create_time_features_from_date(df)\n",
    "    df = df[df[\"target\"] > 0]\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_location(data_frame: pd.DataFrame, location: str):\n",
    "    if location.lower() == \"a\":\n",
    "        data_frame[\"location_a\"] = 1\n",
    "    else:\n",
    "        data_frame[\"location_a\"] = 0\n",
    "\n",
    "    if location.lower() == \"b\":\n",
    "        data_frame[\"location_b\"] = 1\n",
    "    else:\n",
    "        data_frame[\"location_b\"] = 0\n",
    "\n",
    "    if location.lower() == \"c\":\n",
    "        data_frame[\"location_c\"] = 1\n",
    "    else:\n",
    "        data_frame[\"location_c\"] = 0\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "# Define a function to align the temporal resolution of the datasets\n",
    "def temporal_alignment(\n",
    "    train: pd.DataFrame, observed: pd.DataFrame, estimated: pd.DataFrame\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Aligns the temporal resolution of the datasets by aggregating the 15-min interval weather data to hourly intervals.\n",
    "\n",
    "    Args:\n",
    "        train (pd.DataFrame): The training targets DataFrame.\n",
    "        observed (pd.DataFrame): The observed training features DataFrame.\n",
    "        estimated (pd.DataFrame): The estimated training features DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        train_observed (pd.DataFrame): The aligned training DataFrame with observed features.\n",
    "        train_estimated (pd.DataFrame): The aligned training DataFrame with estimated features.\n",
    "    \"\"\"\n",
    "    # Convert the time columns to datetime objects\n",
    "    train[\"time\"] = pd.to_datetime(train[\"time\"])\n",
    "    observed[\"date_forecast\"] = pd.to_datetime(observed[\"date_forecast\"])\n",
    "    estimated[\"date_forecast\"] = pd.to_datetime(estimated[\"date_forecast\"])\n",
    "\n",
    "    # Set the date_forecast column as index for resampling\n",
    "    observed.set_index(\"date_forecast\", inplace=True)\n",
    "    estimated.set_index(\"date_forecast\", inplace=True)\n",
    "\n",
    "    # Resample the weather data to hourly intervals and aggregate the values by mean\n",
    "    observed_resampled = observed.resample(\"1H\").mean()\n",
    "    estimated_resampled = estimated.resample(\"1H\").mean()\n",
    "\n",
    "    # Reset the index after resampling\n",
    "    observed_resampled.reset_index(inplace=True)\n",
    "    estimated_resampled.reset_index(inplace=True)\n",
    "\n",
    "    # Merge the aggregated weather data with the solar production data based on the timestamp\n",
    "    train_observed = pd.merge(\n",
    "        train, observed_resampled, how=\"left\", left_on=\"time\", right_on=\"date_forecast\"\n",
    "    )\n",
    "    train_estimated = pd.merge(\n",
    "        train, estimated_resampled, how=\"left\", left_on=\"time\", right_on=\"date_forecast\"\n",
    "    )\n",
    "\n",
    "    return train_observed, train_estimated\n",
    "\n",
    "\n",
    "def temporal_alignment_tests(test: pd.DataFrame) -> Tuple[pd.DataFrame]:\n",
    "    return aggregate_rows(test)\n",
    "\n",
    "\n",
    "def aggregate_rows(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # Create a 'group' column to group every 4 rows together\n",
    "    df[\"group\"] = df.index // 4\n",
    "\n",
    "    # Define the aggregation functions\n",
    "    aggregation = {col: \"mean\" for col in df.columns if col != \"date_forecast\"}\n",
    "    aggregation[\"date_forecast\"] = \"first\"\n",
    "\n",
    "    # Group by the 'group' column and aggregate\n",
    "    df_agg = df.groupby(\"group\").agg(aggregation).reset_index(drop=True)\n",
    "\n",
    "    # Drop the 'group' column from the original dataframe\n",
    "    df_agg.drop(\"group\", axis=1, inplace=True)\n",
    "\n",
    "    return df_agg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "from src.data.data_fetcher import get_raw_data, get_tests\n",
    "from src.features.feature_engineering import (\n",
    "    feature_engineer,\n",
    "    prepare_data,\n",
    "    remove_missing_features,\n",
    "    temporal_alignment,\n",
    "    add_location,\n",
    "    temporal_alignment_tests,\n",
    ")\n",
    "\n",
    "\n",
    "def fetch_preprocessed_data(drop_features: bool = True) -> (\n",
    "    Tuple[\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "        pd.DataFrame,\n",
    "    ]\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch the preprocessed data for training and validation.\n",
    "\n",
    "    Returns:\n",
    "        X_train_obs_combined: The observed data for training\n",
    "        X_val_obs_combined: The observed data for validation\n",
    "        y_train_obs_combined: The observed labels for training\n",
    "        y_val_obs_combined: The observed labels for validation\n",
    "        X_train_est_combined: The estimated data for training\n",
    "        X_val_est_combined: The estimated data for validation\n",
    "        y_train_est_combined: The estimated labels for training\n",
    "        y_val_est_combined: The estimated labels for validation\n",
    "    \"\"\"\n",
    "    (\n",
    "        train_a,\n",
    "        train_b,\n",
    "        train_c,\n",
    "        X_train_estimated_a,\n",
    "        X_train_estimated_b,\n",
    "        X_train_estimated_c,\n",
    "        X_train_observed_a,\n",
    "        X_train_observed_b,\n",
    "        X_train_observed_c,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "    ) = get_raw_data()\n",
    "\n",
    "    # Temporally align the data from all three locations to the same time.\n",
    "    train_observed_a, train_estimated_a = temporal_alignment(\n",
    "        train_a, X_train_observed_a, X_train_estimated_a\n",
    "    )\n",
    "    train_observed_b, train_estimated_b = temporal_alignment(\n",
    "        train_b, X_train_observed_b, X_train_estimated_b\n",
    "    )\n",
    "    train_observed_c, train_estimated_c = temporal_alignment(\n",
    "        train_c, X_train_observed_c, X_train_estimated_c\n",
    "    )\n",
    "\n",
    "    # Add location data\n",
    "    train_observed_a = add_location(train_observed_a, \"a\")\n",
    "    train_estimated_a = add_location(train_estimated_a, \"a\")\n",
    "\n",
    "    train_observed_b = add_location(train_observed_b, \"b\")\n",
    "    train_estimated_b = add_location(train_estimated_b, \"b\")\n",
    "\n",
    "    train_observed_c = add_location(train_observed_c, \"c\")\n",
    "    train_estimated_c = add_location(train_estimated_c, \"c\")\n",
    "\n",
    "    # Combine the temporally aligned datasets from all three locations\n",
    "    train_observed_combined = pd.concat(\n",
    "        [train_observed_a, train_observed_b, train_observed_c], ignore_index=True\n",
    "    )\n",
    "    train_estimated_combined = pd.concat(\n",
    "        [train_estimated_a, train_estimated_b, train_estimated_c], ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Prepare the combined dataset by handling missing values and splitting the data\n",
    "    (\n",
    "        X_train_obs_combined,\n",
    "        X_val_obs_combined,\n",
    "        y_train_obs_combined,\n",
    "        y_val_obs_combined,\n",
    "        X_train_est_combined,\n",
    "        X_val_est_combined,\n",
    "        y_train_est_combined,\n",
    "        y_val_est_combined,\n",
    "    ) = prepare_data(train_observed_combined, train_estimated_combined, drop_features=drop_features)\n",
    "\n",
    "    return (\n",
    "        X_train_obs_combined,\n",
    "        X_val_obs_combined,\n",
    "        y_train_obs_combined,\n",
    "        y_val_obs_combined,\n",
    "        X_train_est_combined,\n",
    "        X_val_est_combined,\n",
    "        y_train_est_combined,\n",
    "        y_val_est_combined,\n",
    "    )\n",
    "\n",
    "def get_preprocessed_test_data() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the preprocessed test data without the 'date_forecast' column.\n",
    "    \"\"\"\n",
    "    (\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        _,\n",
    "        X_test_estimated_a,\n",
    "        X_test_estimated_b,\n",
    "        X_test_estimated_c,\n",
    "    ) = get_raw_data()\n",
    "\n",
    "    # Align the test data to the same time as the training data\n",
    "    X_test_estimated_a = temporal_alignment_tests(X_test_estimated_a)\n",
    "    X_test_estimated_b = temporal_alignment_tests(X_test_estimated_b)\n",
    "    X_test_estimated_c = temporal_alignment_tests(X_test_estimated_c)\n",
    "    print(\"After temporal alignment\")\n",
    "    print(f\"X_test_estimated_a.shape = {X_test_estimated_a.shape}, X_test_estimated_b.shape = {X_test_estimated_b.shape}, X_test_estimated_c.shape = {X_test_estimated_c.shape}\")\n",
    "\n",
    "    X_test_estimated_a = remove_missing_features(X_test_estimated_a)\n",
    "    X_test_estimated_b = remove_missing_features(X_test_estimated_b)\n",
    "    X_test_estimated_c = remove_missing_features(X_test_estimated_c)\n",
    "\n",
    "    # Add location data\n",
    "    X_test_estimated_a = add_location(X_test_estimated_a, \"a\")\n",
    "    X_test_estimated_b = add_location(X_test_estimated_b, \"b\")\n",
    "    X_test_estimated_c = add_location(X_test_estimated_c, \"c\")\n",
    "\n",
    "    X_test_a_correct_features = feature_engineer(X_test_estimated_a)\n",
    "    X_test_b_correct_features = feature_engineer(X_test_estimated_b)\n",
    "    X_test_c_correct_features = feature_engineer(X_test_estimated_c)\n",
    "\n",
    "    # X_train_obs_combined, X_val_obs_combined, y_train_obs_combined, y_val_obs_combined, X_train_est_combined, X_val_est_combined, y_train_est_combined, y_val_est_combined = fetch_preprocessed_data()\n",
    "    \n",
    "    # # Add historical data so that the model can use it for prediction\n",
    "    # # Add mean_pv_measurement with same day and hour from previous years\n",
    "    # X_test_estimated_a_with_historical_data = add_expected_pv_to_test_data(X_test_a_correct_features, X_train_obs_combined)\n",
    "    # X_test_estimated_b_with_historical_data = add_expected_pv_to_test_data(X_test_b_correct_features, X_train_obs_combined)\n",
    "    # X_test_estimated_c_with_historical_data = add_expected_pv_to_test_data(X_test_c_correct_features, X_train_obs_combined)\n",
    "\n",
    "    # Drop the 'date_calc' and 'date_forecast' columns from the test data\n",
    "    X_test_estimated_a_processed = X_test_a_correct_features.drop(\n",
    "        columns=[\"date_calc\", \"date_forecast\"], errors='ignore'\n",
    "    )\n",
    "    X_test_estimated_b_processed = X_test_b_correct_features.drop(\n",
    "        columns=[\"date_calc\", \"date_forecast\"], errors='ignore'\n",
    "    )\n",
    "    X_test_estimated_c_processed = X_test_c_correct_features.drop(\n",
    "        columns=[\"date_calc\", \"date_forecast\"], errors='ignore'\n",
    "    )\n",
    "\n",
    "    # # # Handle NaN values in the test data by filling them with the mean value of the respective column from the training data\n",
    "    # X_test_estimated_a_processed.dropna()\n",
    "    # X_test_estimated_b_processed.dropna()\n",
    "    # X_test_estimated_c_processed.dropna()\n",
    "    print(f\"X_test_estimated_a_processed.shape = {X_test_estimated_a_processed.shape}, X_test_estimated_b_processed.shape = {X_test_estimated_b_processed.shape}, X_test_estimated_c_processed.shape = {X_test_estimated_c_processed.shape}\")\n",
    "    tests = pd.concat([X_test_estimated_a_processed, X_test_estimated_b_processed, X_test_estimated_c_processed], ignore_index=True)\n",
    "    return tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Plotting libriary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "import numpy as np\n",
    "import pywt\n",
    "\n",
    "import os\n",
    "# Create the directories for the figures\n",
    "if not os.path.exists('results/figures'):\n",
    "    os.makedirs('results/figures')\n",
    "if not os.path.exists('results/figures/time_plot'):\n",
    "    os.makedirs('results/figures/time_plot')\n",
    "if not os.path.exists('results/figures/scatter_plot'):\n",
    "    os.makedirs('results/figures/scatter_plot')\n",
    "if not os.path.exists('results/figures/moving_average'):\n",
    "    os.makedirs('results/figures/moving_average')\n",
    "if not os.path.exists('results/figures/seasonal_trends'):\n",
    "    os.makedirs('results/figures/seasonal_trends')\n",
    "if not os.path.exists('results/figures/spectral_analysis'):\n",
    "    os.makedirs('results/figures/spectral_analysis')\n",
    "if not os.path.exists('results/figures/wavelet_analysis'):\n",
    "    os.makedirs('results/figures/wavelet_analysis')\n",
    "if not os.path.exists('results/figures/acf'):\n",
    "    os.makedirs('results/figures/acf')\n",
    "if not os.path.exists('results/figures/feature_correlation'):\n",
    "    os.makedirs('results/figures/feature_correlation')\n",
    "if not os.path.exists('results/figures/boxplots'):\n",
    "    os.makedirs('results/figures/boxplots')\n",
    "if not os.path.exists('results/figures/pair_grid_plot'):\n",
    "    os.makedirs('results/figures/pair_grid_plot')\n",
    "if not os.path.exists('results/figures/prediction_vs_actuals'):\n",
    "    os.makedirs('results/figures/prediction_vs_actuals')\n",
    "if not os.path.exists('results/figures/residuals'):\n",
    "    os.makedirs('results/figures/residuals')\n",
    "    \n",
    "\n",
    "FIGURE_PATH = \"results/figures/\"\n",
    "\n",
    "\n",
    "(\n",
    "    train_a,\n",
    "    train_b,\n",
    "    train_c,\n",
    "    X_train_estimated_a,\n",
    "    X_train_estimated_b,\n",
    "    X_train_estimated_c,\n",
    "    X_train_observed_a,\n",
    "    X_train_observed_b,\n",
    "    X_train_observed_c,\n",
    "    X_test_estimated_a,\n",
    "    X_test_estimated_b,\n",
    "    X_test_estimated_c,\n",
    ") = get_raw_data()\n",
    "\n",
    "\n",
    "def parse_feature_name(feature_name: str) -> str:\n",
    "    return feature_name.replace(\":\", \"_\")\n",
    "\n",
    "\n",
    "def plot_pv_measurement(pv_a = train_a, pv_b = train_b, pv_c = train_c, show: bool = False) -> None:\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(20, 10), sharex=True)\n",
    "\n",
    "\n",
    "    pv_a.set_index(\"date_forecast\")[\"pv_measurement\"].plot(ax=axs[0], title=\"location A\", color=\"red\")\n",
    "    pv_b.set_index(\"date_forecast\")[\"pv_measurement\"].plot(ax=axs[1], title=\"location B\", color=\"red\")\n",
    "    pv_c.set_index(\"date_forecast\")[\"pv_measurement\"].plot(ax=axs[2], title=\"location C\", color=\"red\")\n",
    "\n",
    "    plt.ylabel(\"PV Measurement\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.savefig(f\"{FIGURE_PATH}time_plot/pv_measurement.png\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_single_feature(feature_name: str, show: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Plots a single feature for all three train/test sets.\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(20, 10), sharex=True)\n",
    "    X_train_observed_a[[\"date_forecast\", feature_name]].set_index(\"date_forecast\").plot(\n",
    "        ax=axs[0], title=\"Train/Test A\", color=\"red\"\n",
    "    )\n",
    "    X_train_estimated_a[[\"date_forecast\", feature_name]].set_index(\n",
    "        \"date_forecast\"\n",
    "    ).plot(ax=axs[0], title=\"Train/Test A\", color=\"blue\")\n",
    "    X_test_estimated_a[[\"date_forecast\", feature_name]].set_index(\"date_forecast\").plot(\n",
    "        ax=axs[0], title=\"Train/Test  A\", color=\"green\"\n",
    "    )\n",
    "\n",
    "    X_train_observed_b[[\"date_forecast\", feature_name]].set_index(\"date_forecast\").plot(\n",
    "        ax=axs[1], title=\"Train/Test  B\", color=\"red\"\n",
    "    )\n",
    "    X_train_estimated_b[[\"date_forecast\", feature_name]].set_index(\n",
    "        \"date_forecast\"\n",
    "    ).plot(ax=axs[1], title=\"Train/Test  B\", color=\"blue\")\n",
    "    X_test_estimated_b[[\"date_forecast\", feature_name]].set_index(\"date_forecast\").plot(\n",
    "        ax=axs[1], title=\"Train/Test  B\", color=\"green\"\n",
    "    )\n",
    "\n",
    "    X_train_observed_c[[\"date_forecast\", feature_name]].set_index(\"date_forecast\").plot(\n",
    "        ax=axs[2], title=\"Train/Test  C\", color=\"red\"\n",
    "    )\n",
    "    X_train_estimated_c[[\"date_forecast\", feature_name]].set_index(\n",
    "        \"date_forecast\"\n",
    "    ).plot(ax=axs[2], title=\"Train/Test  C\", color=\"blue\")\n",
    "    X_test_estimated_c[[\"date_forecast\", feature_name]].set_index(\"date_forecast\").plot(\n",
    "        ax=axs[2], title=\"Train/Test  C\", color=\"green\"\n",
    "    )\n",
    "\n",
    "    file_name = parse_feature_name(feature_name)\n",
    "\n",
    "    plt.ylabel(feature_name)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.savefig(f\"{FIGURE_PATH}time_plot/\" + file_name + \".png\")\n",
    "    plt.legend([\"Observed\", \"Estimated\", \"Test\"])\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_all_features() -> None:\n",
    "    \"\"\"\n",
    "    Plots all features for all three train/test sets.\n",
    "    \"\"\"\n",
    "    for feature in get_all_features():\n",
    "        print(f\"[INFO] Plotting {feature}\")\n",
    "        plot_single_feature(str(feature))\n",
    "\n",
    "\n",
    "def scatter_plot(feature_name: str, show: bool = False) -> None:\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(20, 10), sharex=True)\n",
    "\n",
    "    X_train_observed_a[[\"date_forecast\", feature_name]].set_index(\"date_forecast\").plot(\n",
    "        title=\"Train/Test A\",\n",
    "        kind=\"scatter\",\n",
    "        x=\"date_forecast\",\n",
    "        y=feature_name,\n",
    "        color=\"red\",\n",
    "    )\n",
    "    X_train_estimated_a[[\"date_forecast\", feature_name]].set_index(\n",
    "        \"date_forecast\"\n",
    "    ).plot(\n",
    "        title=\"Train/Test A\",\n",
    "        kind=\"scatter\",\n",
    "        x=\"date_forecast\",\n",
    "        y=feature_name,\n",
    "        color=\"blue\",\n",
    "    )\n",
    "    X_test_estimated_a[[\"date_forecast\", feature_name]].set_index(\"date_forecast\").plot(\n",
    "        title=\"Train/Test A\",\n",
    "        kind=\"scatter\",\n",
    "        x=\"date_forecast\",\n",
    "        y=feature_name,\n",
    "        color=\"green\",\n",
    "    )\n",
    "\n",
    "    X_train_observed_b[[\"date_forecast\", feature_name]].set_index(\"date_forecast\").plot(\n",
    "        kind=\"scatter\", x=\"date_forecast\", y=feature_name, color=\"red\"\n",
    "    )\n",
    "    X_train_estimated_b[[\"date_forecast\", feature_name]].set_index(\n",
    "        \"date_forecast\"\n",
    "    ).plot(kind=\"scatter\", x=\"date_forecast\", y=feature_name, color=\"blue\")\n",
    "    X_test_estimated_b[[\"date_forecast\", feature_name]].set_index(\"date_forecast\").plot(\n",
    "        kind=\"scatter\", x=\"date_forecast\", y=feature_name, color=\"green\"\n",
    "    )\n",
    "\n",
    "    X_train_observed_c[[\"date_forecast\", feature_name]].set_index(\"date_forecast\").plot(\n",
    "        kind=\"scatter\", x=\"date_forecast\", y=feature_name, color=\"red\"\n",
    "    )\n",
    "    X_train_estimated_c[[\"date_forecast\", feature_name]].set_index(\n",
    "        \"date_forecast\"\n",
    "    ).plot(kind=\"scatter\", x=\"date_forecast\", y=feature_name, color=\"blue\")\n",
    "    X_test_estimated_c[[\"date_forecast\", feature_name]].set_index(\"date_forecast\").plot(\n",
    "        kind=\"scatter\", x=\"date_forecast\", y=feature_name, color=\"green\"\n",
    "    )\n",
    "    file_name = parse_feature_name(feature_name)\n",
    "    plt.ylabel(feature_name)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.savefig(f\"{FIGURE_PATH}scatter_plot/\" + file_name + \".png\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_boxplots(df: pd.DataFrame, title: str, show: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Plot box plots for each feature in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the features.\n",
    "    \"\"\"\n",
    "    # Filter out non-numeric columns\n",
    "    df_numeric = df.select_dtypes(include=[\"number\"])\n",
    "\n",
    "    # Number of numeric features\n",
    "    num_features = df_numeric.shape[1]\n",
    "\n",
    "    if num_features == 0:\n",
    "        print(\"No numeric columns to plot\")\n",
    "        return\n",
    "\n",
    "    fig, axes = plt.subplots(num_features, 1, figsize=(10, 4 * num_features))\n",
    "\n",
    "    # Check if df has only one numeric feature, if so, axes is not an array and needs to be put into one\n",
    "    if num_features == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, col in enumerate(df_numeric.columns):\n",
    "        axes[i].boxplot(df_numeric[col].dropna(), vert=True)\n",
    "        axes[i].set_title(f\"Box plot of {col}\")\n",
    "        axes[i].set_ylabel(\"Values\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{FIGURE_PATH}boxplots/{title}_boxplots.png\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def pair_grid_plot(feature_name: str, show: bool = False) -> None:\n",
    "    # We can quickly make a boxplot with Pandas on each feature split out by species\n",
    "    # iris.drop(\"Id\", axis=1).boxplot(by=\"Species\", figsize=(12, 6))\n",
    "\n",
    "    sns.pairplot(\n",
    "        X_train_observed_a.drop(\"date_forecast\", axis=1), hue=\"date_forecast\", height=10\n",
    "    )\n",
    "    plt.savefig(f\"{FIGURE_PATH}pair_grid_plot/\" + feature_name + \".png\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_moving_average(\n",
    "    series, window, plot_intervals=False, scale=1.96, title=\"Moving Average Trend\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute and plot moving average for given series.\n",
    "    For the moving average, we take the average of the past few days of the time series.\n",
    "    This is good for smoothing out short-term flucuations and highlighting long-term trends.\n",
    "    And trend analysis is what we are after.\n",
    "\n",
    "    Args:\n",
    "        series - dataset with timeseries\n",
    "        window - rolling window size\n",
    "        plot_intervals - show confidence intervals\n",
    "        scale - scaling factor for confidence intervals\n",
    "    \"\"\"\n",
    "    rolling_mean = series.rolling(window=window).mean()\n",
    "\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.title(title)\n",
    "\n",
    "    # Plot confidence intervals for the moving average\n",
    "    if plot_intervals:\n",
    "        mae = series.rolling(window=window).std()\n",
    "        deviation = mae * scale\n",
    "        lower_bound = rolling_mean - deviation\n",
    "        upper_bound = rolling_mean + deviation\n",
    "        plt.fill_between(\n",
    "            x=series.index, y1=lower_bound, y2=upper_bound, color=\"b\", alpha=0.2\n",
    "        )\n",
    "\n",
    "    plt.plot(series, label=\"Actual values\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.plot(rolling_mean, \"g\", label=\"Rolling Mean Trend\")\n",
    "    plt.savefig(f\"{FIGURE_PATH}moving_average/\" + title + \".png\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def detect_outliers(target_series, title: str):\n",
    "    \"\"\"\n",
    "    Detects outliers in a time series using the IQR method and plots the time series with outliers highlighted.\n",
    "\n",
    "    Args:\n",
    "    - target_series: A pandas Series representing the time series data.\n",
    "\n",
    "    Returns:\n",
    "    - A plot of the time series with outliers highlighted in red.\n",
    "    \"\"\"\n",
    "\n",
    "    # IQR method for outlier detection\n",
    "    Q1 = target_series.quantile(0.25)\n",
    "    Q3 = target_series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = target_series[\n",
    "        (target_series < lower_bound) | (target_series > upper_bound)\n",
    "    ]\n",
    "\n",
    "    # Plotting the time series and outliers\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(target_series.index, target_series, label=\"Target Values\", color=\"blue\")\n",
    "    plt.scatter(outliers.index, outliers, color=\"red\", label=\"Outliers\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def seasonal_trends(targets: pd.DataFrame, title: str, show: bool = False):\n",
    "    \"\"\"\n",
    "    Decomposes a time series into its trend, seasonal, and residual components.\n",
    "\n",
    "    Args:\n",
    "        targets: A pandas DataFrame representing the time series data.\n",
    "        title: A string representing the title of the plot.\n",
    "        show: A boolean indicating whether or not to display the plot.\n",
    "    \"\"\"\n",
    "\n",
    "    result = seasonal_decompose(targets[\"pv_measurement\"], model=\"additive\", period=24)\n",
    "    result.plot()\n",
    "    # Save\n",
    "    plt.savefig(f\"{FIGURE_PATH}seasonal_trends/\" + title + \".png\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    # Seasonal plot for daily patterns\n",
    "    daily_seasonal = result.seasonal[\n",
    "        \"2022-01-01\":\"2022-01-02\"\n",
    "    ]  # Adjust dates to pick a representative 2-day period\n",
    "    daily_seasonal.plot(figsize=(15, 6))\n",
    "    plt.title(\"Daily Seasonal Pattern\")\n",
    "    plt.savefig(\n",
    "        f\"{FIGURE_PATH}seasonal_trends/\" + title + \"from_2022-01-01_to_2022-01-02.png\"\n",
    "    )\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Autocorrelation plot to identify seasonality\n",
    "    plot_acf(targets[\"pv_measurement\"], lags=168)  # 168 hours for a weekly pattern\n",
    "    plt.title(\"Autocorrelation Plot\")\n",
    "    plt.savefig(f\"{FIGURE_PATH}seasonal_trends/\" + title + \"autocorrelation.png\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def spectral_analysis(signal: pd.Series, title: str, show: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Spectral analysis of a time series using Fourier Transform.\n",
    "\n",
    "    Args:\n",
    "        signal: A pandas Series representing the time series data.\n",
    "        title: A string representing the title of the plot.\n",
    "        show: A boolean indicating whether or not to display the plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply Fourier Transform using numpy\n",
    "    spectral_density = np.abs(np.fft.fft(signal))\n",
    "\n",
    "    # Frequency values for the x-axis\n",
    "    frequencies = np.fft.fftfreq(len(spectral_density))\n",
    "\n",
    "    # Plot Spectral Density\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(frequencies, spectral_density)\n",
    "    plt.title(\"Spectral Density\")\n",
    "    plt.xlabel(\"Frequency\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.savefig(f\"{FIGURE_PATH}spectral_analysis/spectral_density_{title}.png\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def wavelet_analysis(\n",
    "    signal: pd.Series, title: str, wavelet: str = \"cmor\", show: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Wavelet analysis of a time series using Continuous Wavelet Transform.\n",
    "    Continuous Wavelet Transform is a mathematical tool used to analyze non-stationary signals.\n",
    "        * Morlet Wavelet ('cmor'): Often used for analyzing oscillatory patterns and is suitable for most applications due to its good frequency localization.\n",
    "        * Mexican Hat Wavelet ('mexh'): Suitable for detecting sharp changes in signals.\n",
    "\n",
    "    \"\"\"\n",
    "    # Perform Continuous Wavelet Transform\n",
    "    coefficients, frequencies = pywt.cwt(\n",
    "        signal, scales=np.arange(1, 128), wavelet=wavelet\n",
    "    )\n",
    "\n",
    "    # Plot Wavelet Transform result\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(\n",
    "        np.abs(coefficients),\n",
    "        aspect=\"auto\",\n",
    "        extent=[0, len(signal), 1, 128],\n",
    "        cmap=\"jet\",\n",
    "        interpolation=\"bilinear\",\n",
    "    )\n",
    "    plt.colorbar(label=\"Magnitude\")\n",
    "    plt.title(title)\n",
    "    plt.ylabel(\"Scale\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.savefig(\n",
    "        f\"{FIGURE_PATH}wavelet_analysis/wavelet_{wavelet}_transform_{title}.png\"\n",
    "    )\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_correlation_matrix(df: pd.DataFrame, title: str, show: bool = False) -> None:\n",
    "    \"\"\"\n",
    "    Plots a heatmap of the correlation between the features in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the features.\n",
    "        title (str): The title of the plot.\n",
    "        show (bool): A boolean indicating whether or not to display the plot.\n",
    "    \"\"\"\n",
    "    # Drop or impute missing values if any\n",
    "    df = df.fillna(0)\n",
    "    correlation_matrix = df.corr()\n",
    "    # Compute the correlation matrix\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap=\"RdYlGn\", vmin=-1, vmax=1)\n",
    "    plt.title(title)\n",
    "    plt.savefig(f\"{FIGURE_PATH}feature_correlation/{title}_correlation.png\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_acf_daily_weekly_monthly_yearly(\n",
    "    date_frame: pd.DataFrame, feature: str, title: str, show: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot the Autocorrelation Function (ACF) for a given feature in a DataFrame.\n",
    "    The ACF is a measure of the correlation between the time series and a lagged version of itself.\n",
    "    This is useful for identifying patterns in the time series data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plot_acf(date_frame[feature], lags=24)  # 24 hours to check for daily patterns\n",
    "    plt.title(f\"Autocorrelation Function (ACF) Plot for {title} (Daily)\")\n",
    "    plt.savefig(f\"{FIGURE_PATH}acf/{feature}_daily_for_{title}.png\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    # Plot the Autocorrelation Function\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plot_acf(date_frame[feature], lags=168)  # 168 hours to check for weekly patterns\n",
    "    plt.title(f\"Autocorrelation Function (ACF) Plot for {title} (Weekly)\")\n",
    "    plt.savefig(f\"{FIGURE_PATH}acf/{feature}_weekly_for_{title}.png\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plot_acf(\n",
    "        date_frame[feature], lags=168 * 4\n",
    "    )  # 168*4 hours to check for monthly patterns\n",
    "    plt.title(f\"Autocorrelation Function (ACF) Plot for {title} (Monthly)\")\n",
    "    plt.savefig(f\"{FIGURE_PATH}acf/{feature}_monthly_for_{title}.png\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plot_acf(\n",
    "        date_frame[feature], lags=168 * 4 * 12\n",
    "    )  # 168*4*12 hours to check for yearly patterns\n",
    "    plt.title(f\"Autocorrelation Function (ACF) Plot for {title} (Yearly)\")\n",
    "    plt.savefig(f\"{FIGURE_PATH}acf/{feature}_yearly_for_{title}.png\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "\n",
    "def plot_actual_vs_prediction(actual, prediction) -> None:\n",
    "    \"\"\"\n",
    "    Plot the actual values against the predicted values.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Line plot of Actual values\n",
    "    plt.plot(actual.reset_index(drop=True), label='Actual', linestyle='-', marker='o', markersize=5, alpha=0.7, color='blue')\n",
    "    # Line plot of Predicted values\n",
    "    plt.plot(prediction, label='Predicted', linestyle='--', marker='x', markersize=5, alpha=0.7, color='orange')\n",
    "    # Titles and labels\n",
    "    plt.title('Actual vs Predicted - Observed Data', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{FIGURE_PATH}prediction_vs_actuals/actual_vs_predicted.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Visualise the monthly predictions\n",
    "    # Observed Data\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Line plot of Actual values\n",
    "    plt.plot(actual.reset_index(drop=True)[:24*7*4], label='Actual', linestyle='-', marker='o', markersize=5, alpha=0.7, color='blue')\n",
    "    # Line plot of Predicted values\n",
    "    plt.plot(prediction[:24*7*4], label='Predicted', linestyle='--', marker='x', markersize=5, alpha=0.7, color='orange')\n",
    "    # Titles and labels\n",
    "    plt.title('Actual vs Predicted - Observed Data', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{FIGURE_PATH}prediction_vs_actuals/actual_vs_predicted.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "PREDICTION_PATH = \"results/output/\"\n",
    "def plot_residual_predictions(submission_1_name: str, submission_2_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot the residuals of two submissions. The residuals are the difference between the two submissions.\n",
    "    \"\"\"\n",
    "    prediction_1 = pd.read_csv(PREDICTION_PATH + submission_1_name + \".csv\")\n",
    "    prediction_2 = pd.read_csv(PREDICTION_PATH + submission_2_name + \".csv\")\n",
    "    residual = prediction_1 - prediction_2\n",
    "    amount_of_predictions = residual.shape[0]\n",
    "    # Create lines two separate location A, B and C By taking 1/3 of the total amount of predictions as index line\n",
    "    line_a_b = 1/3 * amount_of_predictions\n",
    "    line_b_c = 2/3 * amount_of_predictions\n",
    "    \n",
    "    # Plot the residuals\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # Line plot of Residuals\n",
    "    plt.plot(residual, label='Residuals', linestyle='-', marker='o', markersize=5, alpha=0.7, color='blue')\n",
    "    \n",
    "    # Create lines to separate location A, B and C\n",
    "    plt.axvline(x=line_a_b, color='black', linestyle='--')\n",
    "    plt.axvline(x=line_b_c, color='black', linestyle='--')\n",
    "    \n",
    "    # Titles and labels\n",
    "    plt.title(f'Residuals: {submission_1_name} - {submission_2_name}', fontsize=16)\n",
    "    plt.ylabel(\"Pv Measurement \")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{FIGURE_PATH}residuals/{submission_1_name}_vs_{submission_2_name}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical libriary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "\n",
    "def analyze_normality(target_series):\n",
    "    \"\"\"\n",
    "    Analyzes the normality of a time series using the Shapiro-Wilk test.\n",
    "    \n",
    "    Parameters:\n",
    "    - target_series: A pandas Series representing the time series data.\n",
    "    \n",
    "    Returns:\n",
    "    - Result of the Shapiro-Wilk test and insights on the distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Perform Shapiro-Wilk test\n",
    "    shapiro_stat, shapiro_p = stats.shapiro(target_series)\n",
    "    \n",
    "    # Check for normality based on the p-value\n",
    "    alpha = 0.05\n",
    "    if shapiro_p > alpha:\n",
    "        print(\"Shapiro-Wilk Test: Data seems to be normally distributed (fail to reject H0).\")\n",
    "    else:\n",
    "        print(\"Shapiro-Wilk Test: Data does not seem to be normally distributed (reject H0).\")\n",
    "    \n",
    "    # Return test statistic and p-value\n",
    "    return shapiro_stat, shapiro_p\n",
    "\n",
    "def anderson_darling_test(target_series: pd.Series):\n",
    "    \"\"\"\n",
    "    Analyzes the normality of a time series using the Anderson-Darling test.\n",
    "    \n",
    "    Args:\n",
    "        target_series (pandas.Series): A pandas Series representing the time series data.\n",
    "    \n",
    "    Returns:\n",
    "    - Result of the Anderson-Darling test and insights on the distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Perform Anderson-Darling test\n",
    "    anderson_stat, anderson_crit_vals, anderson_sig_levels = stats.anderson(target_series)\n",
    "    \n",
    "    # Check for normality based on the critical values\n",
    "    alpha = 0.05\n",
    "    if anderson_stat < anderson_crit_vals[2]:\n",
    "        print(\"Anderson-Darling Test: Data seems to be normally distributed (fail to reject H0).\")\n",
    "    else:\n",
    "        print(\"Anderson-Darling Test: Data does not seem to be normally distributed (reject H0).\")\n",
    "    \n",
    "    # Return test statistic and critical values\n",
    "    return anderson_stat, anderson_crit_vals\n",
    "\n",
    "def lilliefors_test(target_series):\n",
    "    \"\"\"\n",
    "    Analyzes the normality of a time series using the Lilliefors test.\n",
    "    It is better for large samples.\n",
    "\n",
    "    Args:\n",
    "        target_series (pandas.Series): A pandas Series representing the time series data.\n",
    "    Returns: \n",
    "        Result of the Lilliefors test and insights on the distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    result = sm.stats.diagnostic.lilliefors(target_series, dist='norm')\n",
    "\n",
    "    statistic, p_value = result\n",
    "    print(f\"Test Statistic: {statistic}\")\n",
    "    print(f\"P-value: {p_value}\")\n",
    "\n",
    "    alpha = 0.05\n",
    "    if p_value < alpha:\n",
    "        print(\"Data does not look normal (Reject H0)\")\n",
    "    else:\n",
    "        print(\"Data looks normal (Fail to reject H0)\")\n",
    "\n",
    "        return statistic, p_value\n",
    "    \n",
    "\n",
    "def feature_correlation(df: pd.DataFrame, threshold: float = 0.8):\n",
    "    \"\"\"\n",
    "    Finds pairs of features with correlation above a given threshold.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): A pandas DataFrame representing the data.\n",
    "        threshold (float, optional): The threshold for the correlation. Defaults to 0.8.\n",
    "    \"\"\"\n",
    "    correlation_matrix = df.corr()\n",
    "\n",
    "    # Find pairs of features with correlation above threshold\n",
    "    correlated_features = set()\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > threshold:\n",
    "                colname = correlation_matrix.columns[i]\n",
    "                correlated_features.add(colname)\n",
    "\n",
    "    \n",
    "    return correlated_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0.5: Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_A, targets_B, targets_C, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c = get_raw_data()\n",
    "all_features = get_raw_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset for Location A\n",
    "targets_A['time'] = pd.to_datetime(targets_A['time'])\n",
    "targets_A.set_index('time', inplace=True)\n",
    "\n",
    "targets_B['time'] = pd.to_datetime(targets_B['time'])\n",
    "targets_B.set_index('time', inplace=True)\n",
    "\n",
    "targets_C['time'] = pd.to_datetime(targets_C['time'])\n",
    "targets_C.set_index('time', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute statistics for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics for each location\n",
    "stats_A = targets_A['pv_measurement'].describe()\n",
    "stats_B = targets_B['pv_measurement'].describe()\n",
    "stats_C = targets_C['pv_measurement'].describe()\n",
    "print(\"Statistics for Location A:\\n\", stats_A)\n",
    "print(\"\\nStatistics for Location B:\\n\", stats_B)\n",
    "print(\"\\nStatistics for Location C:\\n\", stats_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_boxplots(X_train_estimated_a, title = \"X_train_estimated_a\", show=True)\n",
    "plot_boxplots(X_train_estimated_b, title = \"X_train_estimated_b\", show=True)\n",
    "plot_boxplots(X_train_estimated_c, title = \"X_train_estimated_c\", show=True)\n",
    "plot_boxplots(X_train_observed_a, title = \"X_train_observed_a\", show=True)\n",
    "plot_boxplots(X_train_observed_b, title = \"X_train_observed_b\", show=True)\n",
    "plot_boxplots(X_train_observed_c, title = \"X_train_observed_c\", show=True)\n",
    "plot_boxplots(X_test_estimated_a, title = \"X_test_estimated_a\", show=True)\n",
    "plot_boxplots(X_test_estimated_b, title = \"X_test_estimated_b\", show=True)\n",
    "plot_boxplots(X_test_estimated_c, title = \"X_test_estimated_c\", show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend Analysis\n",
    "This code will plot the actual pv_measurement values along with their rolling mean trend (computed over a 7-day window) for Location A. The shaded region represents the confidence intervals for the moving average. You can repeat similar plots for Locations B and C by calling the plot_moving_average function with their respective datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute moving averages for trend analysis for Location A, B and C\n",
    "plot_moving_average(targets_A['pv_measurement'], window=24*7, plot_intervals=True, title=\"Moving Average Trend for Location A\")\n",
    "plot_moving_average(targets_B['pv_measurement'], window=24*7, plot_intervals=True, title=\"Moving Average Trend for Location B\")\n",
    "plot_moving_average(targets_C['pv_measurement'], window=24*7, plot_intervals=True, title=\"Moving Average Trend for Location C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonality analysis\n",
    "Identifying recurring patterns or cycles in the data. For hourly data, you might find daily or monthly seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_targets_A = targets_A.copy()\n",
    "cleaned_targets_B = targets_B.copy()\n",
    "cleaned_targets_C = targets_C.copy()\n",
    "# Remove nan values\n",
    "cleaned_targets_A = cleaned_targets_A.dropna()\n",
    "cleaned_targets_B = cleaned_targets_B.dropna()\n",
    "cleaned_targets_C = cleaned_targets_C.dropna()\n",
    "\n",
    "print(\"Seasonal Trends for Location A\")\n",
    "seasonal_trends(cleaned_targets_A, title=\"Seasonal Trends for Location A\", show=True)\n",
    "print(\"Seasonal Trends for Location B\")\n",
    "seasonal_trends(cleaned_targets_B, title=\"Seasonal Trends for Location B\", show=True)\n",
    "print(\"Seasonal Trends for Location C\")\n",
    "seasonal_trends(cleaned_targets_C, title=\"Seasonal Trends for Location C\", show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cyclicity: \n",
    "Unlike seasonality, which happens at fixed known periods, cycles are fluctuations that are not of a fixed frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Inspection\n",
    "plt.figure(figsize=(15,6))\n",
    "targets_A['pv_measurement'].plot()\n",
    "plt.title('Time Series Plot for Visual Inspection of Cyclicity')\n",
    "plt.show()\n",
    "\n",
    "# Autocorrelation plot\n",
    "plot_acf(targets_A['pv_measurement'], lags=500)  # Adjust lags as needed to inspect longer periods\n",
    "plt.title('Autocorrelation Plot')\n",
    "plt.show()\n",
    "\n",
    "# Partial autocorrelation plot\n",
    "plot_pacf(targets_A['pv_measurement'], lags=500)  # Adjust lags as needed\n",
    "plt.title('Partial Autocorrelation Plot')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectral analysis\n",
    "Spectral analysis, specifically Fourier Transform, can be used to identify cyclic patterns in the data by decomposing the time series into its frequency components. The frequency components with the highest power are the ones that contribute most to the time series. The following code plots the power spectrum of the pv_measurement data for Location A. You can repeat similar plots for Locations B and C by calling the plot_power_spectrum function with their respective datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do spectral analysis for each location\n",
    "A = targets_A.copy()\n",
    "B = targets_B.copy()\n",
    "C = targets_C.copy()\n",
    "A = A.dropna()\n",
    "B = B.dropna()\n",
    "C = C.dropna()\n",
    "spectral_analysis(A['pv_measurement'], title=\"Spectral Analysis for Location A\", show=True)\n",
    "spectral_analysis(B['pv_measurement'], title=\"Spectral Analysis for Location B\", show=True)\n",
    "spectral_analysis(C['pv_measurement'], title=\"Spectral Analysis for Location C\", show=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wavelet analysis\n",
    "Wavelet analysis can be used to study the localized frequency behavior of the time series and is particularly useful when the time series has non-stationary or time-varying cyclic behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CMOR Wavelet Analysis for Locations\")\n",
    "print(\"Wavelet Analysis for Location A\")\n",
    "wavelet_analysis(targets_A['pv_measurement'], wavelet='cmor', title=\"Wavelet Analysis for Location A\", show=True)\n",
    "print(\"Wavelet Analysis for Location B\")\n",
    "wavelet_analysis(targets_B['pv_measurement'], wavelet='cmor', title=\"Wavelet Analysis for Location B\", show=True)\n",
    "print(\"Wavelet Analysis for Location C\")\n",
    "wavelet_analysis(targets_C['pv_measurement'], wavelet='cmor', title=\"Wavelet Analysis for Location C\", show=True)\n",
    "\n",
    "print(\"MEXH Wavelet Analysis for Locations\")\n",
    "\n",
    "wavelet_analysis(targets_A['pv_measurement'], wavelet='mexh', title=\"Wavelet Analysis for Location A\", show=True)\n",
    "print(\"Wavelet Analysis for Location B\")\n",
    "wavelet_analysis(targets_B['pv_measurement'], wavelet='mexh', title=\"Wavelet Analysis for Location B\", show=True)\n",
    "print(\"Wavelet Analysis for Location C\")\n",
    "wavelet_analysis(targets_C['pv_measurement'], wavelet='mexh', title=\"Wavelet Analysis for Location C\", show=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation: \n",
    "It measures the relationship between a variable's current value and its past values. A lag plot or an autocorrelation function (ACF) plot can help in understanding this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Autocorrelation Function\n",
    "\n",
    "B = targets_B.copy()\n",
    "C = targets_C.copy()\n",
    "B.dropna(inplace=True)\n",
    "C.dropna(inplace=True)\n",
    "\n",
    "plot_acf_daily_weekly_monthly_yearly(targets_A, \"pv_measurement\", \"target_a\", show=True)\n",
    "plot_acf_daily_weekly_monthly_yearly(B, \"pv_measurement\", \"target_b\", show=True)\n",
    "plot_acf_daily_weekly_monthly_yearly(C, \"pv_measurement\", \"target_c\", show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection: \n",
    "Identifying unusual data points that might be errors or rare events. This can be done visually or with statistical methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers\n",
    "outliers_a = detect_outliers(targets_A['pv_measurement'], \"Outliers for Location A\")\n",
    "outliers_b = detect_outliers(targets_B['pv_measurement'], \"Outliers for Location B\")\n",
    "outliers_c = detect_outliers(targets_C['pv_measurement'], \"Outliers for Location C\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution Analysis: \n",
    "Understanding the distribution of data can provide insights into its nature (e.g., normal vs. skewed, presence of heavy tails)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example:\n",
    "# Load the dataset for Location A\n",
    "stat, p = analyze_normality(targets_A['pv_measurement'])\n",
    "\n",
    "# Print the test statistic and p-value\n",
    "print(f\"Shapiro-Wilk Test Statistic: {stat}\")\n",
    "print(f\"P-value: {p}\")\n",
    "\n",
    "# Anderson-Darling Test\n",
    "anderson_darling_test(targets_A['pv_measurement'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing the code, you'll receive the result of the Shapiro-Wilk test. If the test indicates non-normality, you can then consider other distributions like log-normal, exponential, or even more domain-specific distributions like the Weibull distribution (often used for lifespan analyses). The choice of distribution should also be influenced by domain knowledge and the nature of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Correlation\n",
    "Correlation analysis can be used to identify highly correlated features. This can be done visually or with statistical methods. The following code plots the correlation matrix for the pv_measurement data for Location A. You can repeat similar plots for Locations B and C by calling the plot_correlation_matrix function with their respective datasets.\n",
    "* Correlation measures linear relationships. Two variables might be related in non-linear ways that the correlation coefficient would not capture. Consider using other methods like mutual information for non-linear relationships.\n",
    "* Correlation does not imply causation. Just because two variables are correlated doesn't mean one causes the other.\n",
    "* Before removing any features, consider the impact on the model and the interpretability. Sometimes, even if two features are correlated, they might have different interpretations in a domain-specific context, and you might want to keep both.\n",
    "* For tree-based models like decision trees and random forests, multicollinearity is less of an issue, as these models can handle correlated features well. However, for linear models and neural networks, it's essential to address highly correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append target to features\n",
    "X_train_observed_a_with_target = X_train_observed_a.copy()\n",
    "X_train_observed_b_with_target = X_train_observed_b.copy()\n",
    "X_train_observed_c_with_target = X_train_observed_c.copy()\n",
    "X_test_estimated_a_with_target = X_test_estimated_a.copy()\n",
    "X_test_estimated_b_with_target = X_test_estimated_b.copy()\n",
    "X_test_estimated_c_with_target = X_test_estimated_c.copy()\n",
    "X_train_estimated_a_with_target = X_train_estimated_a.copy()\n",
    "X_train_estimated_b_with_target = X_train_estimated_b.copy()\n",
    "X_train_estimated_c_with_target = X_train_estimated_c.copy()\n",
    "\n",
    "\n",
    "X_train_observed_a_with_target.set_index('date_forecast', inplace=True)\n",
    "X_train_observed_b_with_target.set_index('date_forecast', inplace=True)\n",
    "X_train_observed_c_with_target.set_index('date_forecast', inplace=True)\n",
    "X_test_estimated_a_with_target.set_index('date_forecast', inplace=True)\n",
    "X_test_estimated_b_with_target.set_index('date_forecast', inplace=True)\n",
    "X_test_estimated_c_with_target.set_index('date_forecast', inplace=True)\n",
    "X_train_estimated_a_with_target.set_index('date_forecast', inplace=True)\n",
    "X_train_estimated_b_with_target.set_index('date_forecast', inplace=True)\n",
    "X_train_estimated_c_with_target.set_index('date_forecast', inplace=True)\n",
    "\n",
    "X_train_observed_a_with_target['pv_measurement'] = targets_A['pv_measurement']\n",
    "X_train_observed_b_with_target['pv_measurement'] = targets_B['pv_measurement']\n",
    "X_train_observed_c_with_target['pv_measurement'] = targets_C['pv_measurement']  \n",
    "X_test_estimated_a_with_target['pv_measurement'] = targets_A['pv_measurement']\n",
    "X_test_estimated_b_with_target['pv_measurement'] = targets_B['pv_measurement']\n",
    "X_test_estimated_c_with_target['pv_measurement'] = targets_C['pv_measurement']\n",
    "X_train_estimated_a_with_target['pv_measurement'] = targets_A['pv_measurement']\n",
    "X_train_estimated_b_with_target['pv_measurement'] = targets_B['pv_measurement']\n",
    "X_train_estimated_c_with_target['pv_measurement'] = targets_C['pv_measurement']\n",
    "\n",
    "plot_correlation_matrix(X_train_observed_a_with_target, title=\"Correlation Matrix X_train_observed_a_with_target for Location A\", show=True)\n",
    "plot_correlation_matrix(X_train_observed_b_with_target, title=\"Correlation Matrix X_train_observed_b_with_target for Location B\", show=True)\n",
    "plot_correlation_matrix(X_train_observed_c_with_target, title=\"Correlation Matrix X_train_observed_c_with_target for Location C\", show=True)\n",
    "plot_correlation_matrix(X_test_estimated_a_with_target, title=\"Correlation Matrix X_test_estimated_a_with_target for Location A\", show=True)\n",
    "plot_correlation_matrix(X_test_estimated_b_with_target, title=\"Correlation Matrix X_test_estimated_b_with_target for Location B\", show=True)\n",
    "plot_correlation_matrix(X_test_estimated_c_with_target, title=\"Correlation Matrix X_test_estimated_c_with_target for Location C\", show=True)\n",
    "plot_correlation_matrix(X_train_estimated_a_with_target, title=\"Correlation Matrix X_train_estimated_a_with_target for Location A\", show=True)\n",
    "plot_correlation_matrix(X_train_estimated_b_with_target, title=\"Correlation Matrix X_train_estimated_b_with_target for Location B\", show=True)\n",
    "plot_correlation_matrix(X_train_estimated_c_with_target, title=\"Correlation Matrix X_train_estimated_c_with_target for Location C\", show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance: \n",
    "If using machine learning models, understanding which features (in this case, weather parameters) are most influential in predicting solar energy production.\n",
    "Lasso Regression (L1 Regularization):\n",
    "* Lasso regression can be used to perform feature selection by shrinking some coefficients to zero, effectively excluding them from the model. The features with non-zero coefficients are considered important.\n",
    "Recursive Feature Elimination (RFE):\n",
    "RFE is a method that fits the model multiple times on iteratively reduced sets of features. It ranks features based on when they were eliminated.\n",
    "Permutation Feature Importance:\n",
    "This method involves randomly shuffling individual features and measuring how much the model's performance deteriorates. Important features will result in a significant drop in model performance when they're shuffled.\n",
    "Feature Importance from Gradient Boosted Trees:\n",
    "Models like XGBoost, LightGBM, and CatBoost provide feature importance scores similar to random forests but based on the structure of the boosted trees.\n",
    "\n",
    "* SHAP (SHapley Additive exPlanations):\n",
    "  SHAP values provide a unified measure of feature importance by averaging the marginal contributions of features across all possible combinations. It is model-agnostic and provides insights into how features influence predictions for individual observations.\n",
    "* Correlation Coefficients:\n",
    "  Features that have a higher correlation with the target variable might be considered more important. However, this method doesn't account for interactions between features.\n",
    "* ANOVA F-values (for regression tasks):\n",
    "  The F-values from a one-way ANOVA test can be used to rank features based on their variance with respect to the target variable. Higher F-values indicate more significant differences in the means and, hence, potentially more important features.\n",
    "* Chi-Squared Test (for classification tasks):\n",
    "  For categorical input features and a categorical target, the chi-squared test can assess the independence between each feature and the target. Larger chi-squared statistics indicate more dependency and, thus, potentially more importance.\n",
    "  \n",
    "The method or combination of methods you choose depends on the specific requirements of the task, the nature of your data, and the model you're working with. It's also beneficial to cross-reference multiple methods to get a more robust understanding of feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train_obs_combined, X_val_obs_combined, y_train_obs_combined, y_val_obs_combined, X_train_est_combined, X_val_est_combined, y_train_est_combined, y_val_est_combined = fetch_preprocessed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Feature Importance using Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_obs_combined, y_train_obs_combined)\n",
    "coefficients = lr.coef_\n",
    "\n",
    "# Plotting feature importance for Linear Regression\n",
    "plt.figure(figsize=(30,10))\n",
    "plt.bar(X_train_obs_combined.columns, coefficients)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Feature Importance on observed using Linear Regression')\n",
    "plt.savefig('results/figures/feature_importance/feature_importance_obs_using_linear_regression.png')\n",
    "plt.show()\n",
    "\n",
    "# 1. Feature Importance using Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_est_combined, y_train_est_combined)\n",
    "coefficients = lr.coef_\n",
    "\n",
    "# Plotting feature importance for Linear Regression\n",
    "plt.figure(figsize=(30,10))\n",
    "\n",
    "plt.bar(X_train_est_combined.columns, coefficients)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Feature Importance on estimate using Linear Regression')\n",
    "plt.savefig('results/figures/feature_importance/feature_importance_obs_using_linear_regression.png')\n",
    "plt.show()\n",
    "\n",
    "# 2. Feature Importance using Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_obs_combined, y_train_obs_combined)\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Plotting feature importance for Random Forest\n",
    "plt.figure(figsize=(30,10))\n",
    "\n",
    "plt.bar(X_train_obs_combined.columns, importances)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Feature Importance on observed using Random Forest')\n",
    "plt.savefig('results/figures/feature_importance/feature_importance_obs_using_random_forest.png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_est_combined, y_train_est_combined)\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Plotting feature importance for Random Forest\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.bar(X_train_est_combined.columns, importances)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Feature Importance on estimate using Random Forest')\n",
    "plt.savefig('results/figures/feature_importance/feature_importance_est_using_random_forest.png')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Analysis: \n",
    "After fitting a model, analyzing the residuals (difference between predictions and actual values) can give insights into the model's accuracy and potential areas of improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Residual analysis is an essential aspect of evaluating the performance and validity of a regression model. Residuals are the differences between the observed values and the values predicted by the model. Analyzing residuals helps in diagnosing the assumptions of linear regression, identifying outliers, and checking the fit of the model.\n",
    "\n",
    "Here's a breakdown of the steps we'll take for residual analysis:\n",
    "\n",
    "* Compute Residuals: We'll first fit a model (for this example, I'll use linear regression) to the data and compute the residuals.\n",
    "* Residual Plots:\n",
    "  * Residual vs. Fitted Plot: This plot helps in checking the assumption of linearity and equal variance (homoscedasticity). If there's a clear pattern or if the spread is not consistent, it indicates a problem.\n",
    "  * Histogram and QQ-Plot: These plots help in checking the assumption of normally distributed residuals.\n",
    "* Scale-Location Plot: Helps in checking the assumption of homoscedasticity.\n",
    "* Residual vs. Leverage Plot: Helps in identifying influential cases that might be affecting the regression model disproportionately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fit the linear regression model\n",
    "X = X_train_est_combined\n",
    "y = y_train_est_combined\n",
    "def residuals(X, y, show=True, title: str = \"on estimate\"): \n",
    "    X = X.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "\n",
    "    X = sm.add_constant(X)  # Adding a constant to the model (intercept)\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    predictions = model.predict(X)\n",
    "    residuals = y - predictions\n",
    "\n",
    "    # Residual vs. Fitted Plot\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.residplot(x=predictions, y=residuals, lowess=True, line_kws={'color': 'red', 'lw': 1})\n",
    "    plt.xlabel('Fitted values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title(f'Residual vs. Fitted Plot {title}')\n",
    "    plt.savefig(f'results/figures/residuals/residual_vs_fitted_plot_on_{title}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Histogram of residuals\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.histplot(residuals, kde=True)\n",
    "    plt.title(f'Histogram of Residuals {title}')\n",
    "    plt.savefig(f'results/figures/residuals/histogram_of_residuals_on_{title}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # QQ-Plot\n",
    "    qqplot(residuals, line='s')\n",
    "    plt.title('QQ-Plot')\n",
    "    plt.show()\n",
    "\n",
    "    # Scale-Location Plot\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.regplot(x=predictions, y=np.sqrt(np.abs(residuals)), lowess=True, line_kws={'color': 'red', 'lw': 1})\n",
    "    plt.ylabel('Standardized Residuals')\n",
    "    plt.xlabel('Fitted values')\n",
    "    plt.title(f'Scale-Location Plot {title}')\n",
    "    plt.savefig(f'results/figures/residuals/scale_location_plot_on_{title}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Residuals vs. Leverage Plot\n",
    "    from statsmodels.graphics.regressionplots import plot_leverage_resid2\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    fig = plot_leverage_resid2(model, ax=ax)\n",
    "    plt.title(f'Residuals vs. Leverage Plot {title}')\n",
    "    plt.savefig(f'results/figures/residuals/residuals_vs_leverage_plot_on_{title}.png')\n",
    "    plt.show()\n",
    "\n",
    "residuals(X, y, show=True, title=\"on estimate\")\n",
    "X = X_train_obs_combined\n",
    "y = y_train_obs_combined\n",
    "residuals(X, y, show=True, title=\"on observed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain-Specific Insights: \n",
    "Since this data deals with solar energy production, domain knowledge about factors affecting solar panel efficiency, degradation over time, and other domain-specific considerations can be invaluable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some domain-specific insights related to solar energy:\n",
    "\n",
    "Sunlight Intensity: Solar panel output is directly proportional to the sunlight's intensity. Hence, periods with more intense sunlight (e.g., mid-day) will typically have higher solar production.\n",
    "\n",
    "Weather Conditions:\n",
    "\n",
    "* Cloud Cover: Overcast days can significantly reduce solar panel output.\n",
    "* Temperature: While sunlight is the primary driver of solar panel output, temperature can also play a role. Solar panel efficiency can decrease as temperature rises, meaning on very hot days, the panels might produce slightly less energy than on cooler sunny days.\n",
    "* Rain and Snow: Rain can reduce solar panel efficiency by blocking sunlight. Snow, especially if it covers the panels, can halt production entirely.\n",
    "* Seasonal Variations: Due to the tilt of the Earth's axis, the angle and duration of sunlight vary throughout the year. This leads to seasonal variations in solar production.\n",
    "\n",
    "Solar Panel Degradation: Over time, solar panels degrade and become less efficient. Typically, solar panels degrade at a rate of about 0.5% to 1% per year. This means that the same panel will produce slightly less energy each year.\n",
    "\n",
    "Orientation and Tilt of Solar Panels: The direction that solar panels face (often towards the equator) and their tilt can have a significant effect on their energy production.\n",
    "\n",
    "Dirt and Debris: Dirt, dust, and other debris can accumulate on solar panels and reduce their efficiency. Regular cleaning can help maintain optimal production.\n",
    "\n",
    "Shadowing: Shadows from nearby structures or trees can reduce solar panel output. Even a small amount of shade on a panel can reduce its output significantly.\n",
    "\n",
    "System Issues: Technical issues, such as inverter problems, can reduce the efficiency of solar energy systems.\n",
    "\n",
    "Policy and Grid Factors: Factors external to the physical system can also influence solar energy production data. For instance, if a grid is at capacity and cannot take on additional solar energy, solar installations might be curtailed.\n",
    "\n",
    "Economic and Behavioral Factors: Economic incentives, such as feed-in tariffs, can influence the amount of solar energy fed into the grid. Similarly, energy storage solutions, like batteries, can influence when solar energy is used versus stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory PCA and CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_obs_combined, X_val_obs_combined, y_train_obs_combined, y_val_obs_combined, X_train_est_combined, X_val_est_combined, y_train_est_combined, y_val_est_combined = fetch_preprocessed_data()\n",
    "\n",
    "x_whole = pd.concat([X_train_obs_combined, X_val_obs_combined])\n",
    "y_whole = pd.concat([y_train_obs_combined, y_val_obs_combined])\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate some sample data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 3)  # first set of variables\n",
    "Y = np.random.rand(100, 3)  # second set of variables\n",
    "\n",
    "# Step 1: Standardize the data\n",
    "scaler_X = StandardScaler()\n",
    "scaler_Y = StandardScaler()\n",
    "\n",
    "X_std = scaler_X.fit_transform(X)\n",
    "Y_std = scaler_Y.fit_transform(Y)\n",
    "\n",
    "# Step 2-5: Apply CCA\n",
    "n_components = 2\n",
    "cca = CCA(n_components=n_components)\n",
    "X_c, Y_c = cca.fit_transform(X_std, Y_std)\n",
    "\n",
    "print(\"Transformed X with CCA:\\n\", X_c)\n",
    "print(\"Transformed Y with CCA:\\n\", Y_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Standardize the data\n",
    "scaler_X = StandardScaler()\n",
    "data_std = scaler_X.fit_transform(x_whole)\n",
    "\n",
    "# Step 2-6: Apply PCA\n",
    "n_components = 25\n",
    "pca = PCA(n_components=n_components)\n",
    "data_pca = pca.fit_transform(data_std)\n",
    "\n",
    "print(\"Transformed data with PCA:\\n\", data_pca)\n",
    "print(\"Explained variance by each component:\", pca.explained_variance_ratio_)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(range(1, len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_)\n",
    "plt.xlabel('Principal Component Number')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot')\n",
    "plt.show()\n",
    "\n",
    "print(\"Component Loadings:\\n\", pca.components_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Generate some sample data\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 3)  # first set of variables\n",
    "Y = np.random.rand(100, 3)  # second set of variables\n",
    "\n",
    "# Step 1: Standardize the data\n",
    "scaler_X = StandardScaler()\n",
    "scaler_Y = StandardScaler()\n",
    "\n",
    "X_std = scaler_X.fit_transform(X)\n",
    "Y_std = scaler_Y.fit_transform(Y)\n",
    "\n",
    "# Step 2-5: Apply CCA\n",
    "n_components = 2\n",
    "cca = CCA(n_components=n_components)\n",
    "X_c, Y_c = cca.fit_transform(X_std, Y_std)\n",
    "\n",
    "print(\"Transformed X with CCA:\\n\", X_c)\n",
    "print(\"Transformed Y with CCA:\\n\", Y_c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Canonical Correlations: These are the correlations between the canonical variables obtained from the two datasets. They give you a sense of how well the canonical variables from each dataset are related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Canonical loadings for X:\\n\", cca.x_loadings_)\n",
    "print(\"Canonical loadings for Y:\\n\", cca.y_loadings_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X_c[:,0], Y_c[:,0], label='First Canonical Variable')\n",
    "plt.scatter(X_c[:,1], Y_c[:,1], label='Second Canonical Variable')\n",
    "plt.xlabel('Canonical X')\n",
    "plt.ylabel('Canonical Y')\n",
    "plt.title('Scatter plot of Canonical Variables')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all data both preprocessed and raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a, train_b, train_c, X_train_estimated_a, X_train_estimated_b, X_train_estimated_c, X_train_observed_a, X_train_observed_b, X_train_observed_c, X_test_estimated_a, X_test_estimated_b, X_test_estimated_c = get_raw_data()\n",
    "X_train_obs_combined, X_val_obs_combined, y_train_obs_combined, y_val_obs_combined, X_train_est_combined, X_val_est_combined, y_train_est_combined, y_val_est_combined = fetch_preprocessed_data()\n",
    "\n",
    "x_whole = pd.concat([X_train_obs_combined, X_val_obs_combined])\n",
    "y_whole = pd.concat([y_train_obs_combined, y_val_obs_combined])\n",
    "estimated_data = pd.concat([X_train_est_combined, X_val_est_combined, y_train_est_combined, y_val_est_combined])\n",
    "# x_whole.reset_index(drop=True, inplace=True)\n",
    "# y_whole.reset_index(drop=True, inplace=True)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "x_whole[\"time_since_prediction\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_est_combined[\"time_since_prediction\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_whole.sort_index(inplace=True)\n",
    "y_whole.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50 = y_whole.value_counts().head(50)\n",
    "\n",
    "# Print the results\n",
    "print(top_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_pv_measurement_by_dataframe(dataframes: list, labels: list, show: bool = False) -> None:\n",
    "    segment_size = 24*7*4\n",
    "    \n",
    "    # Loop through each dataframe and its label\n",
    "    for df, label in zip(dataframes, labels):\n",
    "\n",
    "        # Determine the number of segments for the current dataframe\n",
    "        num_segments = len(df) // segment_size + (len(df) % segment_size > 0)\n",
    "\n",
    "        # Loop through each segment\n",
    "        for i in range(num_segments):\n",
    "            # Select the segment of data\n",
    "            start_idx = i * segment_size\n",
    "            end_idx = start_idx + segment_size\n",
    "            segment = df.iloc[start_idx:end_idx]\n",
    "\n",
    "            # Create a new figure for each segment\n",
    "            plt.figure(figsize=(20, 10))\n",
    "\n",
    "            # Plot a standard line graph\n",
    "            plt.plot(segment.index, segment['pv_measurement'], color='red', linewidth=2)\n",
    "\n",
    "            # Add black dots for each data point using scatter\n",
    "            plt.scatter(segment.index, segment['pv_measurement'], color='black', s=30)\n",
    "\n",
    "            plt.title(f'{label} Segment: {i+1}')\n",
    "            plt.ylabel('PV Measurement')\n",
    "            plt.xlabel('Date')\n",
    "\n",
    "            if show:\n",
    "                plt.show()\n",
    "\n",
    "            # Consider saving each figure with a unique name\n",
    "            # plt.savefig(f'results/figures/{label}/new_pv_measurement_segment_{i+1}.png')\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "# Example Usage\n",
    "dataframes = [train_a, train_b, train_c]\n",
    "labels = ['location_a', 'location_b', 'location_c']\n",
    "plot_pv_measurement_by_dataframe(dataframes, labels, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_with_target = pd.concat([x_whole, y_whole], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "plot_correlation_matrix(data_with_target, \"data_with_target\", show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data_with_target_a = data_with_target[data_with_target['location_a'] == 1]\n",
    "# data_with_target_a = data_with_target_a[(data_with_target_a['sun_azimuth:d'] >= 230) & (data_with_target_a['sun_azimuth:d'] <= 300)]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(5, 10, figsize=(18, 10))  # Create a grid of 5x9 subplots\n",
    "\n",
    "# Flatten the axis array and iterate over both the flattened array and the column names\n",
    "# Assuming df is your DataFrame and you have stored all feature names in a list called feature_names\n",
    "for ax, feature in zip(axs.flatten(), data_with_target_a.columns):\n",
    "    ax.scatter(data_with_target_a[feature], data_with_target_a['pv_measurement'], s=0.01)  # Replace with your column names\n",
    "    ax.set_title(feature)\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('pv_measurement')\n",
    "\n",
    "# Adjust the layout so that plots do not overlap\n",
    "plt.tight_layout()\n",
    "plt.title(\"Location A\")\n",
    "plt.savefig(\"results/figures/feature_engineering/location_a_correlation_with_pv.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = data_with_target[data_with_target['prob_rime:p'] > 10]\n",
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = data_with_target[data_with_target['prob_rime:p'] == 0]\n",
    "filtered_df = filtered_df[filtered_df[\"dew_or_rime:idx\"] != 0]\n",
    "filtered_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_target_a[\"sun_azimuth:d\"].describe()\n",
    "sorted_df = data_with_target_a.sort_values(by='pv_measurement', ascending=False)\n",
    "mean_azimuth = sorted_df.iloc[:1000]['sun_azimuth:d'].mean()\n",
    "print(mean_azimuth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_target_b = data_with_target[data_with_target['location_b'] == 1]\n",
    "fig, axs = plt.subplots(5, 10, figsize=(18, 10))  # Create a grid of 5x9 subplots\n",
    "\n",
    "# Flatten the axis array and iterate over both the flattened array and the column names\n",
    "# Assuming df is your DataFrame and you have stored all feature names in a list called feature_names\n",
    "for ax, feature in zip(axs.flatten(), data_with_target_b.columns):\n",
    "    ax.scatter(data_with_target_b[feature], data_with_target_b['pv_measurement'], s=0.01)  # Replace with your column names\n",
    "    ax.set_title(feature)\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('pv_measurement')\n",
    "\n",
    "# Adjust the layout so that plots do not overlap\n",
    "plt.tight_layout()\n",
    "plt.title(\"Location B\")\n",
    "plt.savefig(\"results/figures/feature_engineering/location_b_correlation_with_pv.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_target_c = data_with_target[data_with_target['location_c'] == 1]\n",
    "fig, axs = plt.subplots(5, 10, figsize=(18, 10))  # Create a grid of 5x9 subplots\n",
    "\n",
    "# Flatten the axis array and iterate over both the flattened array and the column names\n",
    "# Assuming df is your DataFrame and you have stored all feature names in a list called feature_names\n",
    "for ax, feature in zip(axs.flatten(), data_with_target_c.columns):\n",
    "    ax.scatter(data_with_target_c[feature], data_with_target_c['pv_measurement'], s=0.01)  # Replace with your column names\n",
    "    ax.set_title(feature)\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('pv_measurement')\n",
    "\n",
    "# Adjust the layout so that plots do not overlap\n",
    "plt.tight_layout()\n",
    "plt.title(\"Location C\")\n",
    "plt.savefig(\"results/figures/feature_engineering/location_c_correlation_with_pv.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'df' is your DataFrame and it has columns 'effective_radiation' and 'pv_measurement'\n",
    "\n",
    "plt.figure(figsize=(10,6))  # You can adjust the size as needed\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(data_with_target['effective_radiation'], data_with_target['pv_measurement'], color='blue')  # you can choose the color\n",
    "\n",
    "# Adding a vertical line at x=0.9\n",
    "plt.axvline(x=0.85, color='pink')  # Here, 'x' is the position of the vertical line on the x-axis\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('PV Measurement vs Effective Radiation')\n",
    "plt.xlabel('Effective Radiation')\n",
    "plt.ylabel('PV Measurement')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pd.set_option('display.max_rows', 200)\n",
    "\n",
    "mask = (data_with_target[\"modified_solar_elevation\"] < -0.1) & (data_with_target['pv_measurement'] >0)\n",
    "\n",
    "# Apply the mask to get the desired rows\n",
    "filtered_df = data_with_target[mask]\n",
    "\n",
    "\n",
    "filtered_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mask = (data_with_target['sun_addition'] == 0) & (data_with_target['pv_measurement'] >0)\n",
    "\n",
    "# Apply the mask to get the desired rows\n",
    "filtered_df = data_with_target[mask]\n",
    "\n",
    "\n",
    "filtered_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_target\n",
    "\n",
    "mask = (data_with_target['prob_rime:p'] > 10)\n",
    "\n",
    "# Apply the mask to get the desired rows\n",
    "filtered_df = data_with_target[mask]\n",
    "\n",
    "\n",
    "filtered_df.head(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different models\n",
    "In this section we will show different models and how they perform on the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking of models into two layers\n",
    "1. First layer: train models on the whole training set\n",
    "2. Second layer: train a model on the first layer's predictions and the rest of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xgboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_xgb: bool = False\n",
    "load_lgb: bool = False\n",
    "load_cat: bool = False\n",
    "load_rf:  bool = False\n",
    "load_svr: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_obs_combined, X_val_obs_combined, y_train_obs_combined, y_val_obs_combined, X_train_est_combined, X_val_est_combined, y_train_est_combined, y_val_est_combined = fetch_preprocessed_data()\n",
    "x_test_whole = get_preprocessed_test_data()\n",
    "\n",
    "x_whole = pd.concat([X_train_obs_combined, X_val_obs_combined, X_train_est_combined, X_val_est_combined])\n",
    "y_whole = pd.concat([y_train_obs_combined, y_val_obs_combined, y_train_est_combined, y_val_est_combined])\n",
    "x_whole.reset_index(drop=True, inplace=True)\n",
    "y_whole.reset_index(drop=True, inplace=True)\n",
    "\n",
    "x_whole.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split for the base layer and meta layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base layer gets 80% of the data\n",
    "# The meta layer gets 20% of the data\n",
    "base_to_meta_layer_split = 0.8\n",
    "\n",
    "base_x_train = x_whole.sample(frac=base_to_meta_layer_split)\n",
    "meta_x_train = x_whole.sample(frac=1-base_to_meta_layer_split)\n",
    "\n",
    "# Get the corresponding y values\n",
    "base_y_train = y_whole[base_x_train.index]\n",
    "meta_y_train = y_whole[meta_x_train.index]\n",
    "base_x_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Base level models\n",
    "It is important to use a variety of models to get a diverse set of predictions.\n",
    "\n",
    "I want a model to check if there is a linear relationship between the location features and the target. I will use a linear regression model for this.\n",
    "I want to check if the different irradiation values are correlated with the target. I will use xgboost for this.\n",
    "I want to check if the different temperature values are correlated with the target. I will use \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "The XGBoost model is a gradient boosting model. It is a tree based model that uses the gradient descent algorithm to minimize the loss function. The loss function is a combination of the training loss and a regularization term. The regularization term is used to prevent overfitting. The model is trained on the training set and the validation set is used to check if the model is overfitting. The model is then trained on the whole training set and the test set is used to check the model's performance.\n",
    "\n",
    "Utilizing k-fold cross validation to train the model on different subsets of the training set and validate on the rest of the training set. This is done to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold cross validation\n",
    "if not load_xgb:\n",
    "    num_folds = 10\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    total_mae = 0\n",
    "\n",
    "    xgboost_models = []\n",
    "\n",
    "\n",
    "    for train_index, test_index in kf.split(base_x_train):\n",
    "        reg = xgb.XGBRegressor(n_estimators=10000000,\n",
    "                        early_stopping_rounds=50,\n",
    "                        learning_rate= 0.001,\n",
    "                        objective=\"reg:linear\",\n",
    "                        eval_metric=\"mae\",\n",
    "                        sub_sample = 0.9,\n",
    "                        colsample_bytree = 1.0,\n",
    "                        gamma = 0,\n",
    "                        min_child_weight=0,\n",
    "                        max_depth=9)\n",
    "\n",
    "        X_train, X_test = base_x_train.iloc[train_index], base_x_train.iloc[test_index]\n",
    "        y_train, y_test = base_y_train.iloc[train_index], base_y_train.iloc[test_index]\n",
    "        \n",
    "        # Create sample weights for training data\n",
    "        sample_weight_train = np.where(X_train['time_since_prediction'] == 0, 1, 2)\n",
    "        # Create sample weights for testing data\n",
    "        sample_weight_test = np.where(X_test['time_since_prediction'] == 0, 1, 2)\n",
    "        \n",
    "        reg.fit(X_train, y_train,\n",
    "                eval_set=[(X_test, y_test)],\n",
    "                sample_weight=sample_weight_train,\n",
    "                sample_weight_eval_set=[sample_weight_test],  # Here's how you pass the eval weights\n",
    "                verbose=100)\n",
    "        \n",
    "        xgboost_models.append(reg)\n",
    "        predictions = reg.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, predictions, sample_weight=sample_weight_test)\n",
    "        total_mae += mae\n",
    "        \n",
    "        print(f\"Fold {total_mae}, Mean Absolute Error: {mae}\")\n",
    "\n",
    "    average_mse = total_mae / num_folds\n",
    "    print(f\"Average Mean Squared Error: {average_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost - first attempt\n",
    "Tried using cross validation to early stop, however we went away from that after plateuing and suspecting overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_cat:\n",
    "    num_folds = 10\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    total_mae = 0\n",
    "    catboost_models = []\n",
    "\n",
    "    def compute_sample_weight(data):\n",
    "        # Assign weight of 2 for estimated data and 1 for observed data\n",
    "        return np.where(data['time_since_prediction'] > 0, 2, 1)\n",
    "\n",
    "    for train_index, test_index in kf.split(base_x_train):\n",
    "        reg = CatBoostRegressor(\n",
    "            iterations=10000000,\n",
    "            depth=8,\n",
    "            learning_rate=0.001,\n",
    "            loss_function='MAE',\n",
    "            verbose=200\n",
    "        )\n",
    "        \n",
    "        X_train, X_test = base_x_train.iloc[train_index], base_x_train.iloc[test_index]\n",
    "        y_train, y_test = base_y_train.iloc[train_index], base_y_train.iloc[test_index]\n",
    "        \n",
    "        # Compute sample weights for training and testing data\n",
    "        train_weight = compute_sample_weight(X_train)\n",
    "        test_weight = compute_sample_weight(X_test)\n",
    "\n",
    "        # Create Pool for training and testing\n",
    "        train_pool = Pool(data=X_train, label=y_train, weight=train_weight)\n",
    "        test_pool = Pool(data=X_test, label=y_test, weight=test_weight)\n",
    "\n",
    "        # Fit the model using the sample weights\n",
    "        reg.fit(train_pool, eval_set=test_pool, early_stopping_rounds=100)\n",
    "\n",
    "        catboost_models.append(reg)\n",
    "        predictions = reg.predict(test_pool)\n",
    "        \n",
    "        # Compute weighted MAE manually\n",
    "        weighted_mae = np.sum(test_weight * np.abs(y_test - predictions)) / np.sum(test_weight)\n",
    "        total_mae += weighted_mae\n",
    "        \n",
    "        print(f\"Fold {len(catboost_models)}, Weighted Mean Absolute Error: {weighted_mae}\")\n",
    "\n",
    "    average_mae = total_mae / num_folds\n",
    "    print(f\"Average Weighted Mean Absolute Error: {average_mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lightgbm\n",
    "Never got any good results with this model and stopped trying\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_lgb:\n",
    "    num_folds = 10\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    total_mae = 0\n",
    "\n",
    "    lightgbm_models = []\n",
    "\n",
    "    params = {\n",
    "        'objective': 'regression_l1',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'max_depth': -1,\n",
    "        'metric': 'mae',\n",
    "        'num_leaves': 128,\n",
    "        'learning_rate': 0.001,\n",
    "        'feature_fraction': 1.0,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 5,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'early_stopping_rounds': 100,\n",
    "        'verbosity': -1,  # 0 for verbose, -1 for silent\n",
    "    }\n",
    "\n",
    "    num_round = 10000000 # number of training iterations\n",
    "\n",
    "    # Ensure column names are compatible with LightGBM\n",
    "    base_x_train.columns = base_x_train.columns.str.replace('[^A-Za-z0-9_]', '_', regex=True)\n",
    "\n",
    "    for train_index, test_index in kf.split(base_x_train):\n",
    "        X_train, X_test = base_x_train.iloc[train_index], base_x_train.iloc[test_index]\n",
    "        y_train, y_test = base_y_train.iloc[train_index], base_y_train.iloc[test_index]\n",
    "\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        valid_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "        reg = lgb.train(params, train_data, num_round, valid_sets=[valid_data])\n",
    "        lightgbm_models.append(reg)\n",
    "        predictions = reg.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        total_mae += mae\n",
    "        \n",
    "        print(f\"Fold {total_mae}, Mean Absolute Error: {mae}\")\n",
    "\n",
    "    average_mse = total_mae / num_folds\n",
    "    print(f\"Average Mean Squared Error: {average_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Simple random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_rf:\n",
    "    # K-fold cross validation\n",
    "    num_folds = 5\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    total_mae = 0\n",
    "\n",
    "    random_forest_models = []\n",
    "\n",
    "\n",
    "    for train_index, test_index in kf.split(base_x_train):\n",
    "\n",
    "        rf_model = RandomForestRegressor(n_estimators=200, max_depth=25, random_state=42)\n",
    "\n",
    "        X_train, X_test = base_x_train.iloc[train_index], base_x_train.iloc[test_index]\n",
    "        y_train, y_test = base_y_train.iloc[train_index], base_y_train.iloc[test_index]\n",
    "\n",
    "        # Train the Random Forest model on the cleaned training data\n",
    "        \n",
    "        rf_model.fit(X_train, y_train)\n",
    "        \n",
    "        random_forest_models.append(rf_model)\n",
    "        predictions = rf_model.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        total_mae += mae\n",
    "        \n",
    "        print(f\"Fold {total_mae}, Mean Absolute Error: {mae}\")\n",
    "\n",
    "    average_mse = total_mae / num_folds\n",
    "    print(f\"Average Mean Squared Error: {average_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swarm Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_svr:\n",
    "    # K-fold cross validation\n",
    "    num_folds = 5\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    total_mae = 0\n",
    "    svr_models = []\n",
    "\n",
    "    for train_index, test_index in kf.split(base_x_train):\n",
    "        # Train an SVR model\n",
    "        svr = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n",
    "\n",
    "        X_train, X_test = base_x_train.iloc[train_index], base_x_train.iloc[test_index]\n",
    "        y_train, y_test = base_y_train.iloc[train_index], base_y_train.iloc[test_index]\n",
    "\n",
    "        # Train the Swarm Vector Regression model on the cleaned training data\n",
    "        svr.fit(X_train, y_train)\n",
    "\n",
    "        svr_models.append(svr)\n",
    "        predictions = rf_model.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        total_mae += mae\n",
    "        \n",
    "        print(f\"Fold {total_mae}, Mean Absolute Error: {mae}\")\n",
    "\n",
    "    average_mse = total_mae / num_folds\n",
    "    print(f\"Average Mean Squared Error: {average_mse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the models and save the newly trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what models should be loaded\n",
    "# Load XGBoost models\n",
    "if load_xgb:\n",
    "    with open(\"xgboost_models_stack.pkl\", \"rb\") as file:\n",
    "        xgboost_models = pickle.load(file)\n",
    "        print(f\"[LOADED] xgboost_models {len(xgboost_models)} has successfully been loaded\")\n",
    "else:\n",
    "    with open(\"xgboost_models_stack.pkl\", \"wb\") as file:\n",
    "        pickle.dump(xgboost_models, file)\n",
    "        print(f\"[SAVED] xgboost_models has successfully been saved.\")\n",
    "\n",
    "# Load CatBoost models\n",
    "if load_cat:\n",
    "    with open(\"catboost_models_stack.pkl\", \"rb\") as file:\n",
    "        catboost_models = pickle.load(file)\n",
    "        print(f\"[LOADED] catboost_models {len(catboost_models)} has successfully been loaded\")\n",
    "else:\n",
    "    with open(\"catboost_models_stack.pkl\", \"wb\") as file:\n",
    "        pickle.dump(catboost_models, file)\n",
    "        print(f\"[SAVED] catboost_models has successfully been saved.\")\n",
    "\n",
    "# Load lightGBM models\n",
    "if load_lgb:\n",
    "    with open(\"lightgbm_models_stack.pkl\", \"rb\") as file:\n",
    "        lightgbm_models = pickle.load(file)\n",
    "        print(f\"[LOADED] lightgbm_models {len(lightgbm_models)} has successfully been loaded\")\n",
    "else:\n",
    "    with open(\"lightgbm_models_stack.pkl\", \"wb\") as file:\n",
    "        pickle.dump(lightgbm_models, file)\n",
    "        print(f\"[SAVED] lightgbm_models has successfully been saved.\")\n",
    "\n",
    "# Load random forest models\n",
    "if load_rf:\n",
    "    with open(\"random_forest_models_stack.pkl\", \"rb\") as file:\n",
    "        random_forest_models = pickle.load(file)\n",
    "        print(f\"[LOADED] random_forest_models {len(random_forest_models)} has successfully been loaded\")\n",
    "else:\n",
    "    with open(\"random_forest_models_stack.pkl\", \"wb\") as file:\n",
    "        pickle.dump(random_forest_models, file)\n",
    "        print(f\"[SAVED] random_forest_models has successfully been saved.\")\n",
    "\n",
    "# Load SVR models\n",
    "if load_svr:\n",
    "    with open(\"svr_models_stack.pkl\", \"rb\") as file:\n",
    "        svr_models = pickle.load(file)\n",
    "        print(f\"[LOADED] svr_models {len(svr_models)} has successfully been loaded\")\n",
    "else:\n",
    "    with open(\"svr_models_stack.pkl\", \"wb\") as file:\n",
    "        pickle.dump(svr_models, file)\n",
    "        print(f\"[SAVED] svr_models has successfully been saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_prediction(x_values :pd.DataFrame, models) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function for predicting on multiple models and averaging the results\n",
    "    \"\"\"\n",
    "    results = models[0].predict(x_values)\n",
    "    for model in models[1:]:\n",
    "        model: xgb.XGBRegressor\n",
    "        prediction = model.predict(x_values)\n",
    "        results += prediction\n",
    "    \n",
    "    results = results / len(models)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train meta learner model\n",
    "Create dataset for meta learner model by using models to predict on the meta layer training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the base layer on meta_x_train\n",
    "base_xgboost_predictions  = average_prediction(meta_x_train, xgboost_models)\n",
    "base_catboost_predictions = average_prediction(meta_x_train, catboost_models)\n",
    "base_lightgbm_predictions = average_prediction(meta_x_train, lightgbm_models)\n",
    "base_random_forest_predictions = average_prediction(meta_x_train, random_forest_models)\n",
    "base_swarm_vector_regression_predictions = average_prediction(meta_x_train, svr_models)\n",
    "\n",
    "# Add the predictions to the meta_x_train\n",
    "meta_base_x_train = pd.DataFrame()\n",
    "meta_base_x_train[\"xgboost\"] = base_xgboost_predictions\n",
    "meta_base_x_train[\"catboost\"] = base_catboost_predictions\n",
    "meta_base_x_train[\"lightgbm\"] = base_lightgbm_predictions\n",
    "meta_base_x_train[\"random_forest\"] = base_random_forest_predictions\n",
    "meta_base_x_train[\"swarm_vector_regression\"] = base_swarm_vector_regression_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train meta learner model on new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold cross validation\n",
    "\n",
    "num_folds = 10\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "total_mae = 0\n",
    "\n",
    "meta_models = []\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(meta_base_x_train):\n",
    "\n",
    "    reg = xgb.XGBRegressor(n_estimators=100000,\n",
    "                       early_stopping_rounds=50,\n",
    "                       learning_rate= 0.01,\n",
    "                       objective=\"reg:linear\",\n",
    "                       eval_metric=\"mae\",\n",
    "                       sub_sample = 0.9,\n",
    "                       colsample_bytree = 0.8,\n",
    "                       gamma = 0,\n",
    "                       alpha = 0.001,\n",
    "                       min_child_weight=0,\n",
    "                       max_depth=4)\n",
    "\n",
    "    X_train, X_test = meta_base_x_train.iloc[train_index], meta_base_x_train.iloc[test_index]\n",
    "    y_train, y_test = meta_y_train.iloc[train_index], meta_y_train.iloc[test_index]\n",
    "\n",
    "    reg.fit(X_train, y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            verbose=100)\n",
    "    \n",
    "    meta_models.append(reg)\n",
    "    predictions = reg.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    total_mae += mae\n",
    "    \n",
    "    print(f\"Fold {total_mae}, Mean Absolute Error: {mae}\")\n",
    "\n",
    "average_mse = total_mae / num_folds\n",
    "print(f\"Average Mean Squared Error: {average_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.DataFrame(data=reg.feature_importances_,\n",
    "             index=reg.feature_names_in_,\n",
    "             columns=[\"importance\"])\n",
    "\n",
    "plt.figure(figsize=(100,100))\n",
    "plt.tight_layout()\n",
    "fi.sort_values(\"importance\").plot(kind=\"barh\", title=\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val_obs_combined = average_prediction(X_val_obs_combined, meta_models)\n",
    "y_pred_val_est_combined = average_prediction(X_val_est_combined, meta_models)\n",
    "\n",
    "# Evaluate the model's performance using Mean Absolute Error (MAE) on the combined validation observed data\n",
    "mae_obs_combined = mean_absolute_error(y_val_obs_combined, y_pred_val_obs_combined)\n",
    "mae_est_combined = mean_absolute_error(y_val_est_combined, y_pred_val_est_combined)\n",
    "print('MAE on validation observed data: ', mae_obs_combined)\n",
    "print('MAE on validation estimated data: ', mae_est_combined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions for meta learner model test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the base layer on meta_x_train\n",
    "base_xgboost_predictions  = average_prediction(x_test_whole, xgboost_models)\n",
    "base_catboost_predictions = average_prediction(x_test_whole, catboost_models)\n",
    "base_lightgbm_predictions = average_prediction(x_test_whole, lightgbm_models)\n",
    "base_random_forest_predictions = average_prediction(x_test_whole, random_forest_models)\n",
    "base_swarm_vector_regression_predictions = average_prediction(x_test_whole, svr_models)\n",
    "\n",
    "# Add the predictions to the meta_x_train\n",
    "meta_base_x_train = pd.DataFrame()\n",
    "meta_base_x_train[\"xgboost\"] = base_xgboost_predictions\n",
    "meta_base_x_train[\"catboost\"] = base_catboost_predictions\n",
    "meta_base_x_train[\"lightgbm\"] = base_lightgbm_predictions\n",
    "meta_base_x_train[\"random_forest\"] = base_random_forest_predictions\n",
    "meta_base_x_train[\"swarm_vector_regression\"] = base_swarm_vector_regression_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the cleaned validation set\n",
    "y_predictions = average_prediction(meta_base_x_train, meta_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = postprocess_data(x_test_whole, pd.Series(y_predictions))\n",
    "submission_name = 'stacking with xgboost catboost lightgbm random_forest svr '\n",
    "save_predictions(y_predictions, submission_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost\n",
    "Similar to catboost we used early stopping. However we moved away from it. Used feature importance to interpret the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "\n",
    "num_folds = 5\n",
    "\n",
    "# Combine key factors to create a stratification variable without modifying the original DataFrame\n",
    "location = np.where(x_whole['location_a'], 'A',\n",
    "                    np.where(x_whole['location_b'], 'B',\n",
    "                             np.where(x_whole['location_c'], 'C', '')))\n",
    "weather_type = np.where(x_whole['time_since_prediction'] == 0, 'Observed', 'Estimated')\n",
    "stratification_var = np.core.defchararray.add(location, weather_type)\n",
    "\n",
    "# Initialize StratifiedKFold with 5 folds\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "total_mae = 0\n",
    "\n",
    "reg_models = []\n",
    "\n",
    "for train_index, test_index in skf.split(x_whole, stratification_var):\n",
    "\n",
    "    reg = xgb.XGBRegressor(n_estimators=10000000,\n",
    "                       early_stopping_rounds=50,\n",
    "                       learning_rate= 0.001,\n",
    "                       objective=\"reg:linear\",\n",
    "                       eval_metric=\"mae\",\n",
    "                       sub_sample = 0.9,\n",
    "                       colsample_bytree = 1.0,\n",
    "                       gamma = 0,\n",
    "                       min_child_weight=0,\n",
    "                       max_depth=9)\n",
    "\n",
    "    X_train, X_test = x_whole.iloc[train_index], x_whole.iloc[test_index]\n",
    "    y_train, y_test = y_whole.iloc[train_index], y_whole.iloc[test_index]\n",
    "    \n",
    "    # Create sample weights for training data\n",
    "    sample_weight_train = np.where(X_train['time_since_prediction'] == 0, 1, 2)\n",
    "    # Create sample weights for testing data\n",
    "    sample_weight_test = np.where(X_test['time_since_prediction'] == 0, 1, 2)\n",
    "    \n",
    "    reg.fit(X_train, y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            sample_weight=sample_weight_train,\n",
    "            sample_weight_eval_set=[sample_weight_test],  # Here's how you pass the eval weights\n",
    "            verbose=100)\n",
    "    \n",
    "    reg_models.append(reg)\n",
    "    predictions = reg.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, predictions, sample_weight=sample_weight_test)\n",
    "    total_mae += mae\n",
    "    \n",
    "    print(f\"Fold {total_mae}, Mean Absolute Error: {mae}\")\n",
    "\n",
    "average_mse = total_mae / num_folds\n",
    "print(f\"Average Mean Squared Error: {average_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.DataFrame(data=reg.feature_importances_,\n",
    "             index=reg.feature_names_in_,\n",
    "             columns=[\"importance\"])\n",
    "\n",
    "plt.figure(figsize=(200,100))\n",
    "plt.tight_layout()\n",
    "fi.sort_values(\"importance\").plot(kind=\"barh\", title=\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet model\n",
    "Didnt work - we moved away from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Prophet()\n",
    "model.fit(X_train_obs_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamater optimizing\n",
    "Didnt really work that well - we manually adjusted through trial and error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "\n",
    "\n",
    "def compute_sample_weight(data):\n",
    "    # Assign weight of 2 for estimated data and 1 for observed data\n",
    "    return np.where(data[\"time_since_prediction\"] > 0, 2, 1)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        'depth': trial.suggest_int('depth', 8, 11),  # Optimizing the number of iterations\n",
    "        'eval_metric': 'MAE',\n",
    "        'random_seed': 42,\n",
    "        'verbose': 200,\n",
    "        'loss_function': 'MAE',\n",
    "        # Add more parameters here if you want\n",
    "        \"iterations\": 1000,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 8, 11),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.05, 1.0),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1, 100),\n",
    "    }\n",
    "    \n",
    "    fold_mae = []\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_index, test_index in kf.split(x_whole):\n",
    "        X_train_fold, X_val_fold = x_whole.iloc[train_index], x_whole.iloc[test_index]\n",
    "        y_train_fold, y_val_fold = y_whole.iloc[train_index], y_whole.iloc[test_index]\n",
    "\n",
    "\n",
    "        # Compute sample weights for training and testing data\n",
    "        train_weight = compute_sample_weight(X_train_fold)\n",
    "        test_weight = compute_sample_weight(X_val_fold)\n",
    "\n",
    "        # Create Pool for training and testing\n",
    "        train_pool = Pool(data=X_train_fold, label=y_train_fold, weight=train_weight)\n",
    "        test_pool = Pool(data=X_train_fold, label=y_val_fold, weight=test_weight)\n",
    "        \n",
    "        model = CatBoostRegressor(**param)\n",
    "        model.fit(train_pool, eval_set=test_pool, early_stopping_rounds=50)\n",
    "        \n",
    "        y_pred_fold = model.predict(X_val_fold)\n",
    "        fold_mae.append(mean_absolute_error(y_val_fold, y_pred_fold))\n",
    "    \n",
    "    return np.mean(fold_mae)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30)  # You can increase n_trials to try more combinations\n",
    "\n",
    "# Best hyperparameters\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)\n",
    "\n",
    "# Retrain with best parameters\n",
    "best_params = study.best_trial.params\n",
    "best_model = CatBoostRegressor(**best_params)\n",
    "best_model.fit(x_whole, y_whole)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best models - Short notebooks\n",
    "\n",
    "# Catboost\n",
    "Plateued a lot using early stopping, trying to optimize validation sets and methods, but couldnt achieve a high score. Switched over to train models without validation, started with a simple as possible model, worked very great. Tryed changing paramaters but didnt improve, so spent more time focusing on feature engineering than on model optimization. Notebook 1 is a stack of the same model with different features\n",
    "\n",
    "# Autogluon\n",
    "Tried various settings, found out through trial and error that best quality presets was optimal. Furthermore tried dictating what models it uses, but that didnt work. Ended up on a very standardized implementation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
