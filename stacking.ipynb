{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking of models into two layers\n",
    "1. First layer: train models on the whole training set\n",
    "2. Second layer: train a model on the first layer's predictions and the rest of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from src.features.preprocess_data import get_preprocessed_test_data, fetch_preprocessed_data\n",
    "pd.set_option('display.max_columns', 200)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_obs_combined, X_val_obs_combined, y_train_obs_combined, y_val_obs_combined, X_train_est_combined, X_val_est_combined, y_train_est_combined, y_val_est_combined = fetch_preprocessed_data()\n",
    "x_test_whole = get_preprocessed_test_data()\n",
    "\n",
    "x_whole = pd.concat([X_train_obs_combined, X_val_obs_combined])\n",
    "y_whole = pd.concat([y_train_obs_combined, y_val_obs_combined])\n",
    "x_whole.reset_index(drop=True, inplace=True)\n",
    "y_whole.reset_index(drop=True, inplace=True)\n",
    "\n",
    "x_whole.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split for the base layer and meta layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base layer gets 80% of the data\n",
    "# The meta layer gets 20% of the data\n",
    "base_to_meta_layer_split = 0.8\n",
    "\n",
    "base_x_train = x_whole.sample(frac=base_to_meta_layer_split)\n",
    "meta_x_train = x_whole.sample(frac=1-base_to_meta_layer_split)\n",
    "\n",
    "# Get the corresponding y values\n",
    "base_y_train = y_whole[base_x_train.index]\n",
    "meta_y_train = y_whole[meta_x_train.index]\n",
    "base_x_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Base level models\n",
    "It is important to use a variety of models to get a diverse set of predictions.\n",
    "\n",
    "I want a model to check if there is a linear relationship between the location features and the target. I will use a linear regression model for this.\n",
    "I want to check if the different irradiation values are correlated with the target. I will use xgboost for this.\n",
    "I want to check if the different temperature values are correlated with the target. I will use \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold cross validation\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "total_mae = 0\n",
    "\n",
    "xgboost_models = []\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(base_x_train):\n",
    "\n",
    "    reg = xgb.XGBRegressor(n_estimators=100000,\n",
    "                       early_stopping_rounds=50,\n",
    "                       learning_rate= 0.01,\n",
    "                       objective=\"reg:linear\",\n",
    "                       eval_metric=\"mae\",\n",
    "                       sub_sample = 0.9,\n",
    "                       colsample_bytree = 0.8,\n",
    "                       gamma = 0,\n",
    "                       alpha = 0.001,\n",
    "                       min_child_weight=0,\n",
    "                       max_depth=9)\n",
    "\n",
    "    X_train, X_test = base_x_train.iloc[train_index], base_x_train.iloc[test_index]\n",
    "    y_train, y_test = base_y_train.iloc[train_index], base_y_train.iloc[test_index]\n",
    "\n",
    "    reg.fit(X_train, y_train,\n",
    "            eval_set=[(X_val_est_combined, y_val_est_combined)],\n",
    "            verbose=100)\n",
    "    \n",
    "    xgboost_models.append(reg)\n",
    "    predictions = reg.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    total_mae += mae\n",
    "    \n",
    "    print(f\"Fold {total_mae}, Mean Absolute Error: {mae}\")\n",
    "\n",
    "average_mse = total_mae / num_folds\n",
    "print(f\"Average Mean Squared Error: {average_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "total_mae = 0\n",
    "catboost_models = []\n",
    "\n",
    "def compute_sample_weight(data):\n",
    "    # Assign weight of 2 for estimated data and 1 for observed data\n",
    "    return np.where(data['time_since_prediction'] > 0, 2, 1)\n",
    "\n",
    "for train_index, test_index in kf.split(base_x_train):\n",
    "    reg = CatBoostRegressor(\n",
    "        iterations=10000000,\n",
    "        depth=8,\n",
    "        learning_rate=0.001,\n",
    "        loss_function='MAE',\n",
    "        verbose=200\n",
    "    )\n",
    "    \n",
    "    X_train, X_test = base_x_train.iloc[train_index], base_x_train.iloc[test_index]\n",
    "    y_train, y_test = base_y_train.iloc[train_index], base_y_train.iloc[test_index]\n",
    "    \n",
    "    # Compute sample weights for training and testing data\n",
    "    train_weight = compute_sample_weight(X_train)\n",
    "    test_weight = compute_sample_weight(X_test)\n",
    "\n",
    "    # Create Pool for training and testing\n",
    "    train_pool = Pool(data=X_train, label=y_train, weight=train_weight)\n",
    "    test_pool = Pool(data=X_test, label=y_test, weight=test_weight)\n",
    "\n",
    "    # Fit the model using the sample weights\n",
    "    reg.fit(train_pool, eval_set=test_pool, early_stopping_rounds=100)\n",
    "\n",
    "    catboost_models.append(reg)\n",
    "    predictions = reg.predict(test_pool)\n",
    "    \n",
    "    # Compute weighted MAE manually\n",
    "    weighted_mae = np.sum(test_weight * np.abs(y_test - predictions)) / np.sum(test_weight)\n",
    "    total_mae += weighted_mae\n",
    "    \n",
    "    print(f\"Fold {len(catboost_models)}, Weighted Mean Absolute Error: {weighted_mae}\")\n",
    "\n",
    "average_mae = total_mae / num_folds\n",
    "print(f\"Average Weighted Mean Absolute Error: {average_mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming x_whole and y_whole are defined and are pandas DataFrames\n",
    "\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "total_mae = 0\n",
    "\n",
    "lightgbm_models = []\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression_l1',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_depth': 8,\n",
    "    'metric': 'mae',\n",
    "    'num_leaves': 256,\n",
    "    'learning_rate': 0.001,\n",
    "    'feature_fraction': 1.0,\n",
    "    'bagging_fraction': 0.9,\n",
    "    'bagging_freq': 5,\n",
    "    'min_data_in_leaf': 20,\n",
    "    'early_stopping_rounds': 100,\n",
    "    'verbosity': 100,  # 0 for verbose, -1 for silent\n",
    "}\n",
    "\n",
    "num_round = 100000 # number of training iterations\n",
    "\n",
    "# Ensure column names are compatible with LightGBM\n",
    "base_x_train.columns = base_x_train.columns.str.replace('[^A-Za-z0-9_]', '_', regex=True)\n",
    "\n",
    "for train_index, test_index in kf.split(base_x_train):\n",
    "    X_train, X_test = base_x_train.iloc[train_index], base_x_train.iloc[test_index]\n",
    "    y_train, y_test = base_y_train.iloc[train_index], base_y_train.iloc[test_index]\n",
    "\n",
    "    train_data = lgb.Dataset(X_train, label=y_train)\n",
    "    valid_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "    reg = lgb.train(params, train_data, num_round, valid_sets=[valid_data])\n",
    "    lightgbm_models.append(reg)\n",
    "    predictions = reg.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    total_mae += mae\n",
    "    \n",
    "    print(f\"Fold {total_mae}, Mean Absolute Error: {mae}\")\n",
    "\n",
    "average_mse = total_mae / num_folds\n",
    "print(f\"Average Mean Squared Error: {average_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_prediction(x_values :pd.DataFrame, models) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function for predicting on multiple models and averaging the results\n",
    "    \"\"\"\n",
    "    results = models[0].predict(x_values)\n",
    "    for model in models[1:]:\n",
    "        model: xgb.XGBRegressor\n",
    "        prediction = model.predict(x_values)\n",
    "        results += prediction\n",
    "    \n",
    "    results = results / len(models)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train meta learner model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset for meta learner model by using models to predict on the meta layer training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the base layer on meta_x_train\n",
    "base_xgboost_predictions = average_prediction(meta_x_train, xgboost_models)\n",
    "base_catboost_predictions = average_prediction(meta_x_train, catboost_models)\n",
    "base_lightgbm_predictions = average_prediction(meta_x_train, lightgbm_models)\n",
    "# base_random_forest_predictions = average_prediction(meta_x_train, reg_models)\n",
    "\n",
    "# Add the predictions to the meta_x_train\n",
    "meta_x_train[\"xgboost\"] = base_xgboost_predictions\n",
    "meta_x_train[\"catboost\"] = base_catboost_predictions\n",
    "meta_x_train[\"lightgbm\"] = base_lightgbm_predictions\n",
    "# meta_x_train[\"random_forest\"] = base_random_forest_predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train meta learner model on new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold cross validation\n",
    "\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "total_mae = 0\n",
    "\n",
    "meta_models = []\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(meta_x_train):\n",
    "\n",
    "    reg = xgb.XGBRegressor(n_estimators=100000,\n",
    "                       early_stopping_rounds=50,\n",
    "                       learning_rate= 0.01,\n",
    "                       objective=\"reg:linear\",\n",
    "                       eval_metric=\"mae\",\n",
    "                       sub_sample = 0.9,\n",
    "                       colsample_bytree = 0.8,\n",
    "                       gamma = 0,\n",
    "                       alpha = 0.001,\n",
    "                       min_child_weight=0,\n",
    "                       max_depth=9)\n",
    "\n",
    "    X_train, X_test = meta_x_train.iloc[train_index], meta_x_train.iloc[test_index]\n",
    "    y_train, y_test = y_whole.iloc[train_index], y_whole.iloc[test_index]\n",
    "\n",
    "    reg.fit(X_train, y_train,\n",
    "            eval_set=[(X_val_est_combined, y_val_est_combined)],\n",
    "            verbose=100)\n",
    "    \n",
    "    meta_models.append(reg)\n",
    "    predictions = reg.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    total_mae += mae\n",
    "    \n",
    "    print(f\"Fold {total_mae}, Mean Absolute Error: {mae}\")\n",
    "\n",
    "average_mse = total_mae / num_folds\n",
    "print(f\"Average Mean Squared Error: {average_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.DataFrame(data=reg.feature_importances_,\n",
    "             index=reg.feature_names_in_,\n",
    "             columns=[\"importance\"])\n",
    "\n",
    "plt.figure(figsize=(100,100))\n",
    "plt.tight_layout()\n",
    "fi.sort_values(\"importance\").plot(kind=\"barh\", title=\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val_obs_combined = average_prediction(X_val_obs_combined, meta_models)\n",
    "y_pred_val_est_combined = average_prediction(X_val_est_combined, meta_models)\n",
    "\n",
    "# Evaluate the model's performance using Mean Absolute Error (MAE) on the combined validation observed data\n",
    "mae_obs_combined = mean_absolute_error(y_val_obs_combined, y_pred_val_obs_combined)\n",
    "mae_est_combined = mean_absolute_error(y_val_est_combined, y_pred_val_est_combined)\n",
    "print('MAE on validation observed data: ', mae_obs_combined)\n",
    "print('MAE on validation estimated data: ', mae_est_combined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions for meta learner model test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the cleaned validation set\n",
    "from src.features.preprocess_data import get_final_prediction\n",
    "\n",
    "y_predictions = average_prediction(x_test_whole, meta_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.saving import save_predictions\n",
    "\n",
    "\n",
    "save_predictions(y_predictions, 'stacking with possible data leakage')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
