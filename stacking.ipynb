{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking of models into two layers\n",
    "1. First layer: train models on the whole training set\n",
    "2. Second layer: train a model on the first layer's predictions and the rest of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from xgboost) (1.23.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages (from xgboost) (1.9.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\bruker\\onedrive\\ntnu semester 05\\tdt4173 maskinlæring\\ml_power_predictor\\venv\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "from src.features.preprocess_data import get_preprocessed_test_data, fetch_preprocessed_data\n",
    "pd.set_option('display.max_columns', 200)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_xgb: bool = False\n",
    "load_lgb: bool = False\n",
    "load_cat: bool = False\n",
    "load_rf: bool = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_estimated_a.shape = (2880, 47), X_test_estimated_b.shape = (2880, 47), X_test_estimated_c.shape = (2880, 47)\n",
      "After temporal alignment\n",
      "X_test_estimated_a.shape = (720, 47), X_test_estimated_b.shape = (720, 47), X_test_estimated_c.shape = (720, 47)\n",
      "X_test_estimated_a_processed.shape = (720, 46), X_test_estimated_b_processed.shape = (720, 46), X_test_estimated_c_processed.shape = (720, 46)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>absolute_humidity_2m:gm3</th>\n",
       "      <th>air_density_2m:kgm3</th>\n",
       "      <th>clear_sky_energy_1h:J</th>\n",
       "      <th>clear_sky_rad:W</th>\n",
       "      <th>cloud_base_agl:m</th>\n",
       "      <th>dew_or_rime:idx</th>\n",
       "      <th>dew_point_2m:K</th>\n",
       "      <th>diffuse_rad:W</th>\n",
       "      <th>diffuse_rad_1h:J</th>\n",
       "      <th>direct_rad:W</th>\n",
       "      <th>direct_rad_1h:J</th>\n",
       "      <th>effective_cloud_cover:p</th>\n",
       "      <th>is_day:idx</th>\n",
       "      <th>is_in_shadow:idx</th>\n",
       "      <th>precip_5min:mm</th>\n",
       "      <th>precip_type_5min:idx</th>\n",
       "      <th>pressure_50m:hPa</th>\n",
       "      <th>prob_rime:p</th>\n",
       "      <th>rain_water:kgm2</th>\n",
       "      <th>relative_humidity_1000hPa:p</th>\n",
       "      <th>sun_azimuth:d</th>\n",
       "      <th>super_cooled_liquid_water:kgm2</th>\n",
       "      <th>t_1000hPa:K</th>\n",
       "      <th>total_cloud_cover:p</th>\n",
       "      <th>visibility:m</th>\n",
       "      <th>wind_speed_10m:ms</th>\n",
       "      <th>wind_speed_u_10m:ms</th>\n",
       "      <th>wind_speed_v_10m:ms</th>\n",
       "      <th>wind_speed_w_1000hPa:ms</th>\n",
       "      <th>location_a</th>\n",
       "      <th>location_b</th>\n",
       "      <th>location_c</th>\n",
       "      <th>sin_day_of_year</th>\n",
       "      <th>cos_day_of_year</th>\n",
       "      <th>sin_hour</th>\n",
       "      <th>cos_hour</th>\n",
       "      <th>sun_product</th>\n",
       "      <th>modified_solar_elevation</th>\n",
       "      <th>effective_radiation</th>\n",
       "      <th>time_since_prediction</th>\n",
       "      <th>cloud_ratio</th>\n",
       "      <th>cloud_cover_over_30%</th>\n",
       "      <th>sun_addition</th>\n",
       "      <th>is_freezing</th>\n",
       "      <th>is_snow</th>\n",
       "      <th>is_rain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.2</td>\n",
       "      <td>1.18175</td>\n",
       "      <td>1.351672e+06</td>\n",
       "      <td>440.600006</td>\n",
       "      <td>7231.174805</td>\n",
       "      <td>0.0</td>\n",
       "      <td>291.250000</td>\n",
       "      <td>95.500000</td>\n",
       "      <td>294605.53125</td>\n",
       "      <td>271.225006</td>\n",
       "      <td>872357.56250</td>\n",
       "      <td>54.224998</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1007.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>72.375000</td>\n",
       "      <td>92.172501</td>\n",
       "      <td>0.00</td>\n",
       "      <td>294.625000</td>\n",
       "      <td>98.925003</td>\n",
       "      <td>48152.375000</td>\n",
       "      <td>2.125</td>\n",
       "      <td>-2.000</td>\n",
       "      <td>-0.750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.027950</td>\n",
       "      <td>-0.999609</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>25901.988281</td>\n",
       "      <td>0.455669</td>\n",
       "      <td>0.645392</td>\n",
       "      <td>0</td>\n",
       "      <td>0.548142</td>\n",
       "      <td>1</td>\n",
       "      <td>366.725006</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.7</td>\n",
       "      <td>1.19250</td>\n",
       "      <td>2.456032e+06</td>\n",
       "      <td>637.599976</td>\n",
       "      <td>1410.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>289.399994</td>\n",
       "      <td>145.525009</td>\n",
       "      <td>447652.68750</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>11749.87500</td>\n",
       "      <td>99.675003</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1001.400024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>80.699997</td>\n",
       "      <td>236.257751</td>\n",
       "      <td>0.10</td>\n",
       "      <td>292.100006</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>5903.875000</td>\n",
       "      <td>1.350</td>\n",
       "      <td>-1.225</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.310857</td>\n",
       "      <td>-0.950457</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-8.660254e-01</td>\n",
       "      <td>844.045105</td>\n",
       "      <td>0.611489</td>\n",
       "      <td>0.004784</td>\n",
       "      <td>0</td>\n",
       "      <td>0.996750</td>\n",
       "      <td>1</td>\n",
       "      <td>151.325012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.5</td>\n",
       "      <td>1.20150</td>\n",
       "      <td>2.172293e+05</td>\n",
       "      <td>107.400002</td>\n",
       "      <td>2315.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>285.299988</td>\n",
       "      <td>42.200001</td>\n",
       "      <td>100314.65625</td>\n",
       "      <td>65.099998</td>\n",
       "      <td>135992.90625</td>\n",
       "      <td>23.299999</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1006.849976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.049999</td>\n",
       "      <td>85.390503</td>\n",
       "      <td>0.00</td>\n",
       "      <td>291.700012</td>\n",
       "      <td>43.549999</td>\n",
       "      <td>49974.699219</td>\n",
       "      <td>3.375</td>\n",
       "      <td>-0.400</td>\n",
       "      <td>3.350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.804826</td>\n",
       "      <td>-0.593511</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>2.588190e-01</td>\n",
       "      <td>2747.219971</td>\n",
       "      <td>0.164161</td>\n",
       "      <td>0.626034</td>\n",
       "      <td>0</td>\n",
       "      <td>0.535017</td>\n",
       "      <td>0</td>\n",
       "      <td>107.300003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.3</td>\n",
       "      <td>1.24800</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>131.375000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>277.100006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>99.574997</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>992.949951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.650002</td>\n",
       "      <td>127.701752</td>\n",
       "      <td>0.35</td>\n",
       "      <td>278.100006</td>\n",
       "      <td>99.974998</td>\n",
       "      <td>3399.375000</td>\n",
       "      <td>6.000</td>\n",
       "      <td>5.950</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.141444</td>\n",
       "      <td>0.989946</td>\n",
       "      <td>0.965926</td>\n",
       "      <td>-2.588190e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.995999</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.3</td>\n",
       "      <td>1.27175</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8580.474609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>277.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>49.400002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1012.200012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.649994</td>\n",
       "      <td>346.724243</td>\n",
       "      <td>0.00</td>\n",
       "      <td>278.600006</td>\n",
       "      <td>98.824997</td>\n",
       "      <td>27873.150391</td>\n",
       "      <td>1.450</td>\n",
       "      <td>1.450</td>\n",
       "      <td>-0.275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.931210</td>\n",
       "      <td>-0.364483</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>8.660254e-01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.499874</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   absolute_humidity_2m:gm3  air_density_2m:kgm3  clear_sky_energy_1h:J  \\\n",
       "0                      15.2              1.18175           1.351672e+06   \n",
       "1                      13.7              1.19250           2.456032e+06   \n",
       "2                      10.5              1.20150           2.172293e+05   \n",
       "3                       6.3              1.24800           0.000000e+00   \n",
       "4                       6.3              1.27175           0.000000e+00   \n",
       "\n",
       "   clear_sky_rad:W  cloud_base_agl:m  dew_or_rime:idx  dew_point_2m:K  \\\n",
       "0       440.600006       7231.174805              0.0      291.250000   \n",
       "1       637.599976       1410.500000              0.0      289.399994   \n",
       "2       107.400002       2315.000000              0.0      285.299988   \n",
       "3         0.000000        131.375000              0.0      277.100006   \n",
       "4         0.000000       8580.474609              0.0      277.000000   \n",
       "\n",
       "   diffuse_rad:W  diffuse_rad_1h:J  direct_rad:W  direct_rad_1h:J  \\\n",
       "0      95.500000      294605.53125    271.225006     872357.56250   \n",
       "1     145.525009      447652.68750      5.800000      11749.87500   \n",
       "2      42.200001      100314.65625     65.099998     135992.90625   \n",
       "3       0.000000           0.00000      0.000000          0.00000   \n",
       "4       0.000000           0.00000      0.000000          0.00000   \n",
       "\n",
       "   effective_cloud_cover:p  is_day:idx  is_in_shadow:idx  precip_5min:mm  \\\n",
       "0                54.224998         1.0               0.0             0.0   \n",
       "1                99.675003         1.0               0.0             0.0   \n",
       "2                23.299999         1.0               0.0             0.0   \n",
       "3                99.574997         0.0               1.0             0.0   \n",
       "4                49.400002         0.0               1.0             0.0   \n",
       "\n",
       "   precip_type_5min:idx  pressure_50m:hPa  prob_rime:p  rain_water:kgm2  \\\n",
       "0                   0.0       1007.500000          0.0              0.0   \n",
       "1                   0.0       1001.400024          0.0              0.1   \n",
       "2                   0.0       1006.849976          0.0              0.0   \n",
       "3                   0.0        992.949951          0.0              0.0   \n",
       "4                   0.0       1012.200012          0.0              0.0   \n",
       "\n",
       "   relative_humidity_1000hPa:p  sun_azimuth:d  super_cooled_liquid_water:kgm2  \\\n",
       "0                    72.375000      92.172501                            0.00   \n",
       "1                    80.699997     236.257751                            0.10   \n",
       "2                    59.049999      85.390503                            0.00   \n",
       "3                    93.650002     127.701752                            0.35   \n",
       "4                    81.649994     346.724243                            0.00   \n",
       "\n",
       "   t_1000hPa:K  total_cloud_cover:p  visibility:m  wind_speed_10m:ms  \\\n",
       "0   294.625000            98.925003  48152.375000              2.125   \n",
       "1   292.100006           100.000000   5903.875000              1.350   \n",
       "2   291.700012            43.549999  49974.699219              3.375   \n",
       "3   278.100006            99.974998   3399.375000              6.000   \n",
       "4   278.600006            98.824997  27873.150391              1.450   \n",
       "\n",
       "   wind_speed_u_10m:ms  wind_speed_v_10m:ms  wind_speed_w_1000hPa:ms  \\\n",
       "0               -2.000               -0.750                      0.0   \n",
       "1               -1.225                0.525                      0.0   \n",
       "2               -0.400                3.350                      0.0   \n",
       "3                5.950               -0.500                      0.0   \n",
       "4                1.450               -0.275                      0.0   \n",
       "\n",
       "   location_a  location_b  location_c  sin_day_of_year  cos_day_of_year  \\\n",
       "0           1           0           0         0.027950        -0.999609   \n",
       "1           0           1           0        -0.310857        -0.950457   \n",
       "2           1           0           0        -0.804826        -0.593511   \n",
       "3           1           0           0        -0.141444         0.989946   \n",
       "4           1           0           0         0.931210        -0.364483   \n",
       "\n",
       "   sin_hour      cos_hour   sun_product  modified_solar_elevation  \\\n",
       "0  1.000000  6.123234e-17  25901.988281                  0.455669   \n",
       "1 -0.500000 -8.660254e-01    844.045105                  0.611489   \n",
       "2  0.965926  2.588190e-01   2747.219971                  0.164161   \n",
       "3  0.965926 -2.588190e-01      0.000000                  0.000000   \n",
       "4 -0.500000  8.660254e-01      0.000000                  0.000000   \n",
       "\n",
       "   effective_radiation  time_since_prediction  cloud_ratio  \\\n",
       "0             0.645392                      0     0.548142   \n",
       "1             0.004784                      0     0.996750   \n",
       "2             0.626034                      0     0.535017   \n",
       "3             0.000000                      0     0.995999   \n",
       "4             0.000000                      0     0.499874   \n",
       "\n",
       "   cloud_cover_over_30%  sun_addition  is_freezing  is_snow  is_rain  \n",
       "0                     1    366.725006            0        0        0  \n",
       "1                     1    151.325012            0        0        0  \n",
       "2                     0    107.300003            0        0        0  \n",
       "3                     1      0.000000            0        0        0  \n",
       "4                     1      0.000000            0        0        0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_obs_combined, X_val_obs_combined, y_train_obs_combined, y_val_obs_combined, X_train_est_combined, X_val_est_combined, y_train_est_combined, y_val_est_combined = fetch_preprocessed_data()\n",
    "x_test_whole = get_preprocessed_test_data()\n",
    "\n",
    "x_whole = pd.concat([X_train_obs_combined, X_val_obs_combined])\n",
    "y_whole = pd.concat([y_train_obs_combined, y_val_obs_combined])\n",
    "x_whole.reset_index(drop=True, inplace=True)\n",
    "y_whole.reset_index(drop=True, inplace=True)\n",
    "\n",
    "x_whole.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split for the base layer and meta layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['absolute_humidity_2m:gm3', 'air_density_2m:kgm3',\n",
       "       'clear_sky_energy_1h:J', 'clear_sky_rad:W', 'cloud_base_agl:m',\n",
       "       'dew_or_rime:idx', 'dew_point_2m:K', 'diffuse_rad:W',\n",
       "       'diffuse_rad_1h:J', 'direct_rad:W', 'direct_rad_1h:J',\n",
       "       'effective_cloud_cover:p', 'is_day:idx', 'is_in_shadow:idx',\n",
       "       'precip_5min:mm', 'precip_type_5min:idx', 'pressure_50m:hPa',\n",
       "       'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
       "       'sun_azimuth:d', 'super_cooled_liquid_water:kgm2', 't_1000hPa:K',\n",
       "       'total_cloud_cover:p', 'visibility:m', 'wind_speed_10m:ms',\n",
       "       'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms', 'wind_speed_w_1000hPa:ms',\n",
       "       'location_a', 'location_b', 'location_c', 'sin_day_of_year',\n",
       "       'cos_day_of_year', 'sin_hour', 'cos_hour', 'sun_product',\n",
       "       'modified_solar_elevation', 'effective_radiation',\n",
       "       'time_since_prediction', 'cloud_ratio', 'cloud_cover_over_30%',\n",
       "       'sun_addition', 'is_freezing', 'is_snow', 'is_rain'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The base layer gets 80% of the data\n",
    "# The meta layer gets 20% of the data\n",
    "base_to_meta_layer_split = 0.8\n",
    "\n",
    "base_x_train = x_whole.sample(frac=base_to_meta_layer_split)\n",
    "meta_x_train = x_whole.sample(frac=1-base_to_meta_layer_split)\n",
    "\n",
    "# Get the corresponding y values\n",
    "base_y_train = y_whole[base_x_train.index]\n",
    "meta_y_train = y_whole[meta_x_train.index]\n",
    "base_x_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Base level models\n",
    "It is important to use a variety of models to get a diverse set of predictions.\n",
    "\n",
    "I want a model to check if there is a linear relationship between the location features and the target. I will use a linear regression model for this.\n",
    "I want to check if the different irradiation values are correlated with the target. I will use xgboost for this.\n",
    "I want to check if the different temperature values are correlated with the target. I will use \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mae:456.59072\n",
      "[100]\tvalidation_0-mae:415.60173\n",
      "[200]\tvalidation_0-mae:379.08133\n",
      "[300]\tvalidation_0-mae:346.59557\n",
      "[400]\tvalidation_0-mae:317.88171\n",
      "[500]\tvalidation_0-mae:292.54487\n",
      "[600]\tvalidation_0-mae:270.10937\n",
      "[700]\tvalidation_0-mae:250.30079\n",
      "[800]\tvalidation_0-mae:232.65276\n",
      "[900]\tvalidation_0-mae:216.92511\n",
      "[1000]\tvalidation_0-mae:202.88906\n",
      "[1100]\tvalidation_0-mae:190.31136\n",
      "[1200]\tvalidation_0-mae:179.16760\n",
      "[1300]\tvalidation_0-mae:169.36381\n",
      "[1400]\tvalidation_0-mae:160.62671\n",
      "[1500]\tvalidation_0-mae:152.91798\n",
      "[1600]\tvalidation_0-mae:146.08973\n",
      "[1700]\tvalidation_0-mae:140.00030\n",
      "[1800]\tvalidation_0-mae:134.49784\n",
      "[1900]\tvalidation_0-mae:129.56687\n",
      "[2000]\tvalidation_0-mae:125.22671\n",
      "[2100]\tvalidation_0-mae:121.36332\n",
      "[2200]\tvalidation_0-mae:117.93642\n"
     ]
    }
   ],
   "source": [
    "# K-fold cross validation\n",
    "if not load_xgb:\n",
    "    num_folds = 5\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    total_mae = 0\n",
    "\n",
    "    xgboost_models = []\n",
    "\n",
    "\n",
    "    for train_index, test_index in kf.split(base_x_train):\n",
    "        reg = xgb.XGBRegressor(n_estimators=10000000,\n",
    "                        early_stopping_rounds=50,\n",
    "                        learning_rate= 0.001,\n",
    "                        objective=\"reg:linear\",\n",
    "                        eval_metric=\"mae\",\n",
    "                        sub_sample = 0.9,\n",
    "                        colsample_bytree = 1.0,\n",
    "                        gamma = 0,\n",
    "                        min_child_weight=0,\n",
    "                        max_depth=9)\n",
    "\n",
    "        X_train, X_test = base_x_train.iloc[train_index], base_x_train.iloc[test_index]\n",
    "        y_train, y_test = base_y_train.iloc[train_index], base_y_train.iloc[test_index]\n",
    "        \n",
    "        # Create sample weights for training data\n",
    "        sample_weight_train = np.where(X_train['time_since_prediction'] == 0, 1, 2)\n",
    "        # Create sample weights for testing data\n",
    "        sample_weight_test = np.where(X_test['time_since_prediction'] == 0, 1, 2)\n",
    "        \n",
    "        reg.fit(X_train, y_train,\n",
    "                eval_set=[(X_test, y_test)],\n",
    "                sample_weight=sample_weight_train,\n",
    "                sample_weight_eval_set=[sample_weight_test],  # Here's how you pass the eval weights\n",
    "                verbose=100)\n",
    "        \n",
    "        xgboost_models.append(reg)\n",
    "        predictions = reg.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, predictions, sample_weight=sample_weight_test)\n",
    "        total_mae += mae\n",
    "        \n",
    "        print(f\"Fold {total_mae}, Mean Absolute Error: {mae}\")\n",
    "\n",
    "    average_mse = total_mae / num_folds\n",
    "    print(f\"Average Mean Squared Error: {average_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_cat:\n",
    "    num_folds = 5\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    total_mae = 0\n",
    "    catboost_models = []\n",
    "\n",
    "    def compute_sample_weight(data):\n",
    "        # Assign weight of 2 for estimated data and 1 for observed data\n",
    "        return np.where(data['time_since_prediction'] > 0, 2, 1)\n",
    "\n",
    "    for train_index, test_index in kf.split(base_x_train):\n",
    "        reg = CatBoostRegressor(\n",
    "            iterations=10000000,\n",
    "            depth=8,\n",
    "            learning_rate=0.001,\n",
    "            loss_function='MAE',\n",
    "            verbose=200\n",
    "        )\n",
    "        \n",
    "        X_train, X_test = base_x_train.iloc[train_index], base_x_train.iloc[test_index]\n",
    "        y_train, y_test = base_y_train.iloc[train_index], base_y_train.iloc[test_index]\n",
    "        \n",
    "        # Compute sample weights for training and testing data\n",
    "        train_weight = compute_sample_weight(X_train)\n",
    "        test_weight = compute_sample_weight(X_test)\n",
    "\n",
    "        # Create Pool for training and testing\n",
    "        train_pool = Pool(data=X_train, label=y_train, weight=train_weight)\n",
    "        test_pool = Pool(data=X_test, label=y_test, weight=test_weight)\n",
    "\n",
    "        # Fit the model using the sample weights\n",
    "        reg.fit(train_pool, eval_set=test_pool, early_stopping_rounds=100)\n",
    "\n",
    "        catboost_models.append(reg)\n",
    "        predictions = reg.predict(test_pool)\n",
    "        \n",
    "        # Compute weighted MAE manually\n",
    "        weighted_mae = np.sum(test_weight * np.abs(y_test - predictions)) / np.sum(test_weight)\n",
    "        total_mae += weighted_mae\n",
    "        \n",
    "        print(f\"Fold {len(catboost_models)}, Weighted Mean Absolute Error: {weighted_mae}\")\n",
    "\n",
    "    average_mae = total_mae / num_folds\n",
    "    print(f\"Average Weighted Mean Absolute Error: {average_mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_lgb:\n",
    "    num_folds = 5\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    total_mae = 0\n",
    "\n",
    "    lightgbm_models = []\n",
    "\n",
    "    params = {\n",
    "        'objective': 'regression_l1',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'max_depth': 8,\n",
    "        'metric': 'mae',\n",
    "        'num_leaves': 256,\n",
    "        'learning_rate': 0.001,\n",
    "        'feature_fraction': 1.0,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 5,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'early_stopping_rounds': 100,\n",
    "        'verbosity': 100,  # 0 for verbose, -1 for silent\n",
    "    }\n",
    "\n",
    "    num_round = 10000000 # number of training iterations\n",
    "\n",
    "    # Ensure column names are compatible with LightGBM\n",
    "    base_x_train.columns = base_x_train.columns.str.replace('[^A-Za-z0-9_]', '_', regex=True)\n",
    "\n",
    "    for train_index, test_index in kf.split(base_x_train):\n",
    "        X_train, X_test = base_x_train.iloc[train_index], base_x_train.iloc[test_index]\n",
    "        y_train, y_test = base_y_train.iloc[train_index], base_y_train.iloc[test_index]\n",
    "\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        valid_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "        reg = lgb.train(params, train_data, num_round, valid_sets=[valid_data])\n",
    "        lightgbm_models.append(reg)\n",
    "        predictions = reg.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        total_mae += mae\n",
    "        \n",
    "        print(f\"Fold {total_mae}, Mean Absolute Error: {mae}\")\n",
    "\n",
    "    average_mse = total_mae / num_folds\n",
    "    print(f\"Average Mean Squared Error: {average_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_rf:\n",
    "    # K-fold cross validation\n",
    "    num_folds = 5\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    total_mae = 0\n",
    "\n",
    "    random_forest_models = []\n",
    "\n",
    "\n",
    "    for train_index, test_index in kf.split(base_x_train):\n",
    "\n",
    "        rf_model = RandomForestRegressor(n_estimators=1, random_state=42)\n",
    "\n",
    "        X_train, X_test = base_x_train.iloc[train_index], base_x_train.iloc[test_index]\n",
    "        y_train, y_test = base_y_train.iloc[train_index], base_y_train.iloc[test_index]\n",
    "\n",
    "        # Train the Random Forest model on the cleaned training data\n",
    "        \n",
    "        reg.fit(X_train, y_train,\n",
    "                eval_set=[(X_val_est_combined, y_val_est_combined)],\n",
    "                verbose=100)\n",
    "        \n",
    "        random_forest_models.append(reg)\n",
    "        predictions = reg.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        total_mae += mae\n",
    "        \n",
    "        print(f\"Fold {total_mae}, Mean Absolute Error: {mae}\")\n",
    "\n",
    "    average_mse = total_mae / num_folds\n",
    "    print(f\"Average Mean Squared Error: {average_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the models and save the newly trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what models should be loaded\n",
    "# Load XGBoost models\n",
    "if load_xgb:\n",
    "    with open(\"xgboost_models.pkl\", \"rb\") as file:\n",
    "        xgboost_models = pickle.load(file)\n",
    "else:\n",
    "    with open(\"xgboost_models.pkl\", \"wb\") as file:\n",
    "        pickle.dump(xgboost_models, file)\n",
    "\n",
    "# Load CatBoost models\n",
    "if load_cat:\n",
    "    with open(\"catboost_models.pkl\", \"rb\") as file:\n",
    "        catboost_models = pickle.load(file)\n",
    "else:\n",
    "    with open(\"catboost_models.pkl\", \"wb\") as file:\n",
    "        pickle.dump(catboost_models, file)\n",
    "\n",
    "# Load lightGBM models\n",
    "if load_lgb:\n",
    "    with open(\"lightgbm_models.pkl\", \"rb\") as file:\n",
    "        lightgbm_models = pickle.load(file)\n",
    "else:\n",
    "    with open(\"lightgbm_models.pkl\", \"wb\") as file:\n",
    "        pickle.dump(lightgbm_models, file)\n",
    "\n",
    "# Load random forest models\n",
    "if load_rf:\n",
    "    with open(\"random_forest_models.pkl\", \"rb\") as file:\n",
    "        random_forest_models = pickle.load(file)\n",
    "else:\n",
    "    with open(\"random_forest_models.pkl\", \"wb\") as file:\n",
    "        pickle.dump(random_forest_models, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_prediction(x_values :pd.DataFrame, models) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function for predicting on multiple models and averaging the results\n",
    "    \"\"\"\n",
    "    results = models[0].predict(x_values)\n",
    "    for model in models[1:]:\n",
    "        model: xgb.XGBRegressor\n",
    "        prediction = model.predict(x_values)\n",
    "        results += prediction\n",
    "    \n",
    "    results = results / len(models)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train meta learner model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset for meta learner model by using models to predict on the meta layer training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the base layer on meta_x_train\n",
    "base_xgboost_predictions  = average_prediction(meta_x_train, xgboost_models)\n",
    "base_catboost_predictions = average_prediction(meta_x_train, catboost_models)\n",
    "base_lightgbm_predictions = average_prediction(meta_x_train, lightgbm_models)\n",
    "base_random_forest_predictions = average_prediction(meta_x_train, random_forest_models)\n",
    "\n",
    "# Add the predictions to the meta_x_train\n",
    "meta_base_x_train = pd.DataFrame()\n",
    "meta_base_x_train[\"xgboost\"] = base_xgboost_predictions\n",
    "meta_base_x_train[\"catboost\"] = base_catboost_predictions\n",
    "meta_base_x_train[\"lightgbm\"] = base_lightgbm_predictions\n",
    "meta_x_train[\"random_forest\"] = base_random_forest_predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train meta learner model on new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold cross validation\n",
    "\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "total_mae = 0\n",
    "\n",
    "meta_models = []\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(meta_base_x_train):\n",
    "\n",
    "    reg = xgb.XGBRegressor(n_estimators=100000,\n",
    "                       early_stopping_rounds=50,\n",
    "                       learning_rate= 0.01,\n",
    "                       objective=\"reg:linear\",\n",
    "                       eval_metric=\"mae\",\n",
    "                       sub_sample = 0.9,\n",
    "                       colsample_bytree = 0.8,\n",
    "                       gamma = 0,\n",
    "                       alpha = 0.001,\n",
    "                       min_child_weight=0,\n",
    "                       max_depth=4)\n",
    "\n",
    "    X_train, X_test = meta_base_x_train.iloc[train_index], meta_base_x_train.iloc[test_index]\n",
    "    y_train, y_test = meta_y_train.iloc[train_index], meta_y_train.iloc[test_index]\n",
    "\n",
    "    reg.fit(X_train, y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            verbose=100)\n",
    "    \n",
    "    meta_models.append(reg)\n",
    "    predictions = reg.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    total_mae += mae\n",
    "    \n",
    "    print(f\"Fold {total_mae}, Mean Absolute Error: {mae}\")\n",
    "\n",
    "average_mse = total_mae / num_folds\n",
    "print(f\"Average Mean Squared Error: {average_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.DataFrame(data=reg.feature_importances_,\n",
    "             index=reg.feature_names_in_,\n",
    "             columns=[\"importance\"])\n",
    "\n",
    "plt.figure(figsize=(100,100))\n",
    "plt.tight_layout()\n",
    "fi.sort_values(\"importance\").plot(kind=\"barh\", title=\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val_obs_combined = average_prediction(X_val_obs_combined, meta_models)\n",
    "y_pred_val_est_combined = average_prediction(X_val_est_combined, meta_models)\n",
    "\n",
    "# Evaluate the model's performance using Mean Absolute Error (MAE) on the combined validation observed data\n",
    "mae_obs_combined = mean_absolute_error(y_val_obs_combined, y_pred_val_obs_combined)\n",
    "mae_est_combined = mean_absolute_error(y_val_est_combined, y_pred_val_est_combined)\n",
    "print('MAE on validation observed data: ', mae_obs_combined)\n",
    "print('MAE on validation estimated data: ', mae_est_combined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions for meta learner model test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the cleaned validation set\n",
    "from src.features.preprocess_data import get_final_prediction\n",
    "\n",
    "y_predictions = average_prediction(x_test_whole, meta_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.saving import save_predictions\n",
    "\n",
    "\n",
    "save_predictions(y_predictions, 'stacking with possible data leakage')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
