{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking of models into two layers\n",
    "1. First layer: train models on the whole training set\n",
    "2. Second layer: train a model on the first layer's predictions and the rest of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "\n",
    "from src.features.preprocess_data import get_preprocessed_test_data, fetch_preprocessed_data\n",
    "pd.set_option('display.max_columns', 200)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_xgb: bool = False\n",
    "load_lgb: bool = False\n",
    "load_cat: bool = False\n",
    "load_rf:  bool = False\n",
    "load_svr: bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_obs_combined, X_val_obs_combined, y_train_obs_combined, y_val_obs_combined, X_train_est_combined, X_val_est_combined, y_train_est_combined, y_val_est_combined = fetch_preprocessed_data()\n",
    "x_test_whole = get_preprocessed_test_data()\n",
    "\n",
    "x_whole = pd.concat([X_train_obs_combined, X_val_obs_combined, X_train_est_combined, X_val_est_combined])\n",
    "y_whole = pd.concat([y_train_obs_combined, y_val_obs_combined, y_train_est_combined, y_val_est_combined])\n",
    "x_whole.reset_index(drop=True, inplace=True)\n",
    "y_whole.reset_index(drop=True, inplace=True)\n",
    "\n",
    "x_whole.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split for the base layer and meta layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The base layer gets 80% of the data\n",
    "# The meta layer gets 20% of the data\n",
    "base_to_meta_layer_split = 0.8\n",
    "\n",
    "base_x_train = x_whole.sample(frac=base_to_meta_layer_split)\n",
    "meta_x_train = x_whole.sample(frac=1-base_to_meta_layer_split)\n",
    "\n",
    "# Get the corresponding y values\n",
    "base_y_train = y_whole[base_x_train.index]\n",
    "meta_y_train = y_whole[meta_x_train.index]\n",
    "base_x_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Base level models\n",
    "It is important to use a variety of models to get a diverse set of predictions.\n",
    "\n",
    "I want a model to check if there is a linear relationship between the location features and the target. I will use a linear regression model for this.\n",
    "I want to check if the different irradiation values are correlated with the target. I will use xgboost for this.\n",
    "I want to check if the different temperature values are correlated with the target. I will use \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost\n",
    "The XGBoost model is a gradient boosting model. It is a tree based model that uses the gradient descent algorithm to minimize the loss function. The loss function is a combination of the training loss and a regularization term. The regularization term is used to prevent overfitting. The model is trained on the training set and the validation set is used to check if the model is overfitting. The model is then trained on the whole training set and the test set is used to check the model's performance.\n",
    "\n",
    "Utilizing k-fold cross validation to train the model on different subsets of the training set and validate on the rest of the training set. This is done to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold cross validation\n",
    "if not load_xgb:\n",
    "    num_folds = 5\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    total_mae = 0\n",
    "\n",
    "    xgboost_models = []\n",
    "\n",
    "\n",
    "    for train_index, test_index in kf.split(base_x_train):\n",
    "        reg = xgb.XGBRegressor(n_estimators=10000000,\n",
    "                        early_stopping_rounds=50,\n",
    "                        learning_rate= 0.001,\n",
    "                        objective=\"reg:linear\",\n",
    "                        eval_metric=\"mae\",\n",
    "                        sub_sample = 0.9,\n",
    "                        colsample_bytree = 1.0,\n",
    "                        gamma = 0,\n",
    "                        min_child_weight=0,\n",
    "                        max_depth=9)\n",
    "\n",
    "        X_train, X_test = base_x_train.iloc[train_index], base_x_train.iloc[test_index]\n",
    "        y_train, y_test = base_y_train.iloc[train_index], base_y_train.iloc[test_index]\n",
    "        \n",
    "        # Create sample weights for training data\n",
    "        sample_weight_train = np.where(X_train['time_since_prediction'] == 0, 1, 2)\n",
    "        # Create sample weights for testing data\n",
    "        sample_weight_test = np.where(X_test['time_since_prediction'] == 0, 1, 2)\n",
    "        \n",
    "        reg.fit(X_train, y_train,\n",
    "                eval_set=[(X_test, y_test)],\n",
    "                sample_weight=sample_weight_train,\n",
    "                sample_weight_eval_set=[sample_weight_test],  # Here's how you pass the eval weights\n",
    "                verbose=100)\n",
    "        \n",
    "        xgboost_models.append(reg)\n",
    "        predictions = reg.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, predictions, sample_weight=sample_weight_test)\n",
    "        total_mae += mae\n",
    "        \n",
    "        print(f\"Fold {total_mae}, Mean Absolute Error: {mae}\")\n",
    "\n",
    "    average_mse = total_mae / num_folds\n",
    "    print(f\"Average Mean Squared Error: {average_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_cat:\n",
    "    num_folds = 5\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    total_mae = 0\n",
    "    catboost_models = []\n",
    "\n",
    "    def compute_sample_weight(data):\n",
    "        # Assign weight of 2 for estimated data and 1 for observed data\n",
    "        return np.where(data['time_since_prediction'] > 0, 2, 1)\n",
    "\n",
    "    for train_index, test_index in kf.split(base_x_train):\n",
    "        reg = CatBoostRegressor(\n",
    "            iterations=10000000,\n",
    "            depth=8,\n",
    "            learning_rate=0.001,\n",
    "            loss_function='MAE',\n",
    "            verbose=200\n",
    "        )\n",
    "        \n",
    "        X_train, X_test = base_x_train.iloc[train_index], base_x_train.iloc[test_index]\n",
    "        y_train, y_test = base_y_train.iloc[train_index], base_y_train.iloc[test_index]\n",
    "        \n",
    "        # Compute sample weights for training and testing data\n",
    "        train_weight = compute_sample_weight(X_train)\n",
    "        test_weight = compute_sample_weight(X_test)\n",
    "\n",
    "        # Create Pool for training and testing\n",
    "        train_pool = Pool(data=X_train, label=y_train, weight=train_weight)\n",
    "        test_pool = Pool(data=X_test, label=y_test, weight=test_weight)\n",
    "\n",
    "        # Fit the model using the sample weights\n",
    "        reg.fit(train_pool, eval_set=test_pool, early_stopping_rounds=100)\n",
    "\n",
    "        catboost_models.append(reg)\n",
    "        predictions = reg.predict(test_pool)\n",
    "        \n",
    "        # Compute weighted MAE manually\n",
    "        weighted_mae = np.sum(test_weight * np.abs(y_test - predictions)) / np.sum(test_weight)\n",
    "        total_mae += weighted_mae\n",
    "        \n",
    "        print(f\"Fold {len(catboost_models)}, Weighted Mean Absolute Error: {weighted_mae}\")\n",
    "\n",
    "    average_mae = total_mae / num_folds\n",
    "    print(f\"Average Weighted Mean Absolute Error: {average_mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_lgb:\n",
    "    num_folds = 5\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "    total_mae = 0\n",
    "\n",
    "    lightgbm_models = []\n",
    "\n",
    "    params = {\n",
    "        'objective': 'regression_l1',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'max_depth': -1,\n",
    "        'metric': 'mae',\n",
    "        'num_leaves': 128,\n",
    "        'learning_rate': 0.001,\n",
    "        'feature_fraction': 1.0,\n",
    "        'bagging_fraction': 0.9,\n",
    "        'bagging_freq': 5,\n",
    "        'min_data_in_leaf': 20,\n",
    "        'early_stopping_rounds': 100,\n",
    "        'verbosity': -1,  # 0 for verbose, -1 for silent\n",
    "    }\n",
    "\n",
    "    num_round = 10000000 # number of training iterations\n",
    "\n",
    "    # Ensure column names are compatible with LightGBM\n",
    "    base_x_train.columns = base_x_train.columns.str.replace('[^A-Za-z0-9_]', '_', regex=True)\n",
    "\n",
    "    for train_index, test_index in kf.split(base_x_train):\n",
    "        X_train, X_test = base_x_train.iloc[train_index], base_x_train.iloc[test_index]\n",
    "        y_train, y_test = base_y_train.iloc[train_index], base_y_train.iloc[test_index]\n",
    "\n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        valid_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "        reg = lgb.train(params, train_data, num_round, valid_sets=[valid_data])\n",
    "        lightgbm_models.append(reg)\n",
    "        predictions = reg.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        total_mae += mae\n",
    "        \n",
    "        print(f\"Fold {total_mae}, Mean Absolute Error: {mae}\")\n",
    "\n",
    "    average_mse = total_mae / num_folds\n",
    "    print(f\"Average Mean Squared Error: {average_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_rf:\n",
    "    # K-fold cross validation\n",
    "    num_folds = 5\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    total_mae = 0\n",
    "\n",
    "    random_forest_models = []\n",
    "\n",
    "\n",
    "    for train_index, test_index in kf.split(base_x_train):\n",
    "\n",
    "        rf_model = RandomForestRegressor(n_estimators=200, max_depth=25, random_state=42)\n",
    "\n",
    "        X_train, X_test = base_x_train.iloc[train_index], base_x_train.iloc[test_index]\n",
    "        y_train, y_test = base_y_train.iloc[train_index], base_y_train.iloc[test_index]\n",
    "\n",
    "        # Train the Random Forest model on the cleaned training data\n",
    "        \n",
    "        rf_model.fit(X_train, y_train)\n",
    "        \n",
    "        random_forest_models.append(rf_model)\n",
    "        predictions = rf_model.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        total_mae += mae\n",
    "        \n",
    "        print(f\"Fold {total_mae}, Mean Absolute Error: {mae}\")\n",
    "\n",
    "    average_mse = total_mae / num_folds\n",
    "    print(f\"Average Mean Squared Error: {average_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swarm Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_svr:\n",
    "    # K-fold cross validation\n",
    "    num_folds = 5\n",
    "    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    total_mae = 0\n",
    "    svr_models = []\n",
    "\n",
    "    for train_index, test_index in kf.split(base_x_train):\n",
    "        # Train an SVR model\n",
    "        svr = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n",
    "\n",
    "        X_train, X_test = base_x_train.iloc[train_index], base_x_train.iloc[test_index]\n",
    "        y_train, y_test = base_y_train.iloc[train_index], base_y_train.iloc[test_index]\n",
    "\n",
    "        # Train the Swarm Vector Regression model on the cleaned training data\n",
    "        svr.fit(X_train, y_train)\n",
    "\n",
    "        svr_models.append(svr)\n",
    "        predictions = rf_model.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, predictions)\n",
    "        total_mae += mae\n",
    "        \n",
    "        print(f\"Fold {total_mae}, Mean Absolute Error: {mae}\")\n",
    "\n",
    "    average_mse = total_mae / num_folds\n",
    "    print(f\"Average Mean Squared Error: {average_mse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the models and save the newly trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what models should be loaded\n",
    "# Load XGBoost models\n",
    "if load_xgb:\n",
    "    with open(\"xgboost_models_stack.pkl\", \"rb\") as file:\n",
    "        xgboost_models = pickle.load(file)\n",
    "        print(f\"[LOADED] xgboost_models {len(xgboost_models)} has successfully been loaded\")\n",
    "else:\n",
    "    with open(\"xgboost_models_stack.pkl\", \"wb\") as file:\n",
    "        pickle.dump(xgboost_models, file)\n",
    "        print(f\"[SAVED] xgboost_models has successfully been saved.\")\n",
    "\n",
    "# Load CatBoost models\n",
    "if load_cat:\n",
    "    with open(\"catboost_models_stack.pkl\", \"rb\") as file:\n",
    "        catboost_models = pickle.load(file)\n",
    "        print(f\"[LOADED] catboost_models {len(catboost_models)} has successfully been loaded\")\n",
    "else:\n",
    "    with open(\"catboost_models_stack.pkl\", \"wb\") as file:\n",
    "        pickle.dump(catboost_models, file)\n",
    "        print(f\"[SAVED] catboost_models has successfully been saved.\")\n",
    "\n",
    "# Load lightGBM models\n",
    "if load_lgb:\n",
    "    with open(\"lightgbm_models_stack.pkl\", \"rb\") as file:\n",
    "        lightgbm_models = pickle.load(file)\n",
    "        print(f\"[LOADED] lightgbm_models {len(lightgbm_models)} has successfully been loaded\")\n",
    "else:\n",
    "    with open(\"lightgbm_models_stack.pkl\", \"wb\") as file:\n",
    "        pickle.dump(lightgbm_models, file)\n",
    "        print(f\"[SAVED] lightgbm_models has successfully been saved.\")\n",
    "\n",
    "# Load random forest models\n",
    "if load_rf:\n",
    "    with open(\"random_forest_models_stack.pkl\", \"rb\") as file:\n",
    "        random_forest_models = pickle.load(file)\n",
    "        print(f\"[LOADED] random_forest_models {len(random_forest_models)} has successfully been loaded\")\n",
    "else:\n",
    "    with open(\"random_forest_models_stack.pkl\", \"wb\") as file:\n",
    "        pickle.dump(random_forest_models, file)\n",
    "        print(f\"[SAVED] random_forest_models has successfully been saved.\")\n",
    "\n",
    "# Load SVR models\n",
    "if load_svr:\n",
    "    with open(\"svr_models_stack.pkl\", \"rb\") as file:\n",
    "        svr_models = pickle.load(file)\n",
    "        print(f\"[LOADED] svr_models {len(svr_models)} has successfully been loaded\")\n",
    "else:\n",
    "    with open(\"svr_models_stack.pkl\", \"wb\") as file:\n",
    "        pickle.dump(svr_models, file)\n",
    "        print(f\"[SAVED] svr_models has successfully been saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_prediction(x_values :pd.DataFrame, models) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function for predicting on multiple models and averaging the results\n",
    "    \"\"\"\n",
    "    results = models[0].predict(x_values)\n",
    "    for model in models[1:]:\n",
    "        model: xgb.XGBRegressor\n",
    "        prediction = model.predict(x_values)\n",
    "        results += prediction\n",
    "    \n",
    "    results = results / len(models)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train meta learner model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataset for meta learner model by using models to predict on the meta layer training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the base layer on meta_x_train\n",
    "base_xgboost_predictions  = average_prediction(meta_x_train, xgboost_models)\n",
    "base_catboost_predictions = average_prediction(meta_x_train, catboost_models)\n",
    "base_lightgbm_predictions = average_prediction(meta_x_train, lightgbm_models)\n",
    "base_random_forest_predictions = average_prediction(meta_x_train, random_forest_models)\n",
    "base_swarm_vector_regression_predictions = average_prediction(meta_x_train, svr_models)\n",
    "\n",
    "# Add the predictions to the meta_x_train\n",
    "meta_base_x_train = pd.DataFrame()\n",
    "meta_base_x_train[\"xgboost\"] = base_xgboost_predictions\n",
    "meta_base_x_train[\"catboost\"] = base_catboost_predictions\n",
    "meta_base_x_train[\"lightgbm\"] = base_lightgbm_predictions\n",
    "meta_base_x_train[\"random_forest\"] = base_random_forest_predictions\n",
    "meta_base_x_train[\"swarm_vector_regression\"] = base_swarm_vector_regression_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train meta learner model on new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold cross validation\n",
    "\n",
    "num_folds = 5\n",
    "kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "total_mae = 0\n",
    "\n",
    "meta_models = []\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(meta_base_x_train):\n",
    "\n",
    "    reg = xgb.XGBRegressor(n_estimators=100000,\n",
    "                       early_stopping_rounds=50,\n",
    "                       learning_rate= 0.01,\n",
    "                       objective=\"reg:linear\",\n",
    "                       eval_metric=\"mae\",\n",
    "                       sub_sample = 0.9,\n",
    "                       colsample_bytree = 0.8,\n",
    "                       gamma = 0,\n",
    "                       alpha = 0.001,\n",
    "                       min_child_weight=0,\n",
    "                       max_depth=4)\n",
    "\n",
    "    X_train, X_test = meta_base_x_train.iloc[train_index], meta_base_x_train.iloc[test_index]\n",
    "    y_train, y_test = meta_y_train.iloc[train_index], meta_y_train.iloc[test_index]\n",
    "\n",
    "    reg.fit(X_train, y_train,\n",
    "            eval_set=[(X_test, y_test)],\n",
    "            verbose=100)\n",
    "    \n",
    "    meta_models.append(reg)\n",
    "    predictions = reg.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    total_mae += mae\n",
    "    \n",
    "    print(f\"Fold {total_mae}, Mean Absolute Error: {mae}\")\n",
    "\n",
    "average_mse = total_mae / num_folds\n",
    "print(f\"Average Mean Squared Error: {average_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.DataFrame(data=reg.feature_importances_,\n",
    "             index=reg.feature_names_in_,\n",
    "             columns=[\"importance\"])\n",
    "\n",
    "plt.figure(figsize=(100,100))\n",
    "plt.tight_layout()\n",
    "fi.sort_values(\"importance\").plot(kind=\"barh\", title=\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val_obs_combined = average_prediction(X_val_obs_combined, meta_models)\n",
    "y_pred_val_est_combined = average_prediction(X_val_est_combined, meta_models)\n",
    "\n",
    "# Evaluate the model's performance using Mean Absolute Error (MAE) on the combined validation observed data\n",
    "mae_obs_combined = mean_absolute_error(y_val_obs_combined, y_pred_val_obs_combined)\n",
    "mae_est_combined = mean_absolute_error(y_val_est_combined, y_pred_val_est_combined)\n",
    "print('MAE on validation observed data: ', mae_obs_combined)\n",
    "print('MAE on validation estimated data: ', mae_est_combined)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get predictions for meta learner model test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the base layer on meta_x_train\n",
    "base_xgboost_predictions  = average_prediction(x_test_whole, xgboost_models)\n",
    "base_catboost_predictions = average_prediction(x_test_whole, catboost_models)\n",
    "base_lightgbm_predictions = average_prediction(x_test_whole, lightgbm_models)\n",
    "# base_random_forest_predictions = average_prediction(meta_x_train, random_forest_models)\n",
    "\n",
    "# Add the predictions to the meta_x_train\n",
    "meta_base_x_train = pd.DataFrame()\n",
    "meta_base_x_train[\"xgboost\"] = base_xgboost_predictions\n",
    "meta_base_x_train[\"catboost\"] = base_catboost_predictions\n",
    "meta_base_x_train[\"lightgbm\"] = base_lightgbm_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the cleaned validation set\n",
    "y_predictions = average_prediction(meta_base_x_train, meta_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.saving import save_predictions\n",
    "from src.features.postprocess_data import postprocess_data\n",
    "y_predictions = postprocess_data(x_test_whole, pd.Series(y_predictions))\n",
    "submission_name = 'stacking with xgboost catboost lightgbm random_forest svr '\n",
    "save_predictions(y_predictions, submission_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize predictions vs best model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.visualization.plotting import plot_residual_predictions\n",
    "plot_residual_predictions(\"stack + 149\", \"ny_catboost_3_modeller_4\")\n",
    "plot_residual_predictions(\"stacking test1\", \"xgboost est min max\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
